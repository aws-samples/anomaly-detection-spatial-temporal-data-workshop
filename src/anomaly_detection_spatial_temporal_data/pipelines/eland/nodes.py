"""
This is a boilerplate pipeline 'eland'
generated using Kedro 0.18.0
"""
import pandas as pd 
import numpy as np 
import os
import logging
from typing import Dict, Tuple
import torch
from torch.utils.data import DataLoader

from anomaly_detection_spatial_temporal_data.model.data_loader import DynamicGraphWNFDataSet, DynamicGraphWNodeFeatDatasetLoader
from anomaly_detection_spatial_temporal_data.model.dynamic_graph import Eland_e2e
from anomaly_detection_spatial_temporal_data.model.model_config import ElandConfig

 

def load_data(labels, u2index, p2index, edge_list, tvt_nids, user_features, item_features, parameters: Dict) -> Dict:
    """
    Load the processed data for the model 
    Args:
        processed_data: processed graph data 
        parameters: parameters for the data loader
        
    Returns: 
        data dict generated by TADDY data loader 
    """
    data_loader = DynamicGraphWNodeFeatDatasetLoader(
        labels, 
        u2index, 
        p2index, 
        edge_list, 
        tvt_nids, 
        user_features, 
        item_features
    )
    
    #sequential data loader
    dataset = DynamicGraphWNFDataSet(p2index, item_features, edge_list)
    lstm_dataloader = DataLoader(dataset, batch_size=300)
    
    return {
        'graph': data_loader.graph, 
        'lstm_dataloader': lstm_dataloader,
        'user_features': data_loader.user_features,
        'item_features': data_loader.item_features,
        'labels': data_loader.labels,
        'tvt_nids': data_loader.tvt_idx,
        'u2index': data_loader.u2index,
        'p2index': data_loader.p2index
    }

def set_and_train_model(data_dict: Dict, parameters: Dict) -> Tuple:
    """
    Set the model training configurations and train the model 
    Args:
        data_dict: data for the dataloader
        parameters: parameters for model training 
    Returns:
        model training result
    """
    model_config = ElandConfig(parameters)
    model_obj = Eland_e2e(
        data_dict['graph'], 
        data_dict['lstm_dataloader'], 
        data_dict['user_features'],
        data_dict['item_features'], 
        data_dict['labels'], 
        data_dict['tvt_nids'], 
        data_dict['u2index'],
        data_dict['p2index'], 
        data_dict['item_features'], 
        model_config
    )
    auc, ap = model_obj.train()
    training_result = {"auc": auc, "ap":ap}
    save_model_path = ''
    return learned_result,save_model_path


# def run(ds, graph_num=0.1, name='debug', baseline=False, gnnlayer_type='gcn', rnnlayer_type='lstm', device='cpu'):
#     """main function to kick off model training"""

#     eland = Eland_e2e(
#         graph, 
#         lstm_dataloader, 
#         user_features,
#         item_features, 
#         labels, 
#         tvt_nids, 
#         u2index,
#         p2index, 
#         item_features, 
#         lr=0.01, 
#         n_layers=2, 
#         name=name, 
#         pretrain_bm=25,
#         pretrain_nc=300, 
#         gnnlayer_type=gnnlayer_type, 
#         rnn_type=rnnlayer_type, 
#         bmloss_type='mse', 
#         device=device, 
#         base_pred=base_pred)
#     if not baseline:
#         auc, ap = eland.train()
#     else:
#         auc, ap = eland.pretrain_nc_net(n_epochs=300)
#     return auc, ap

# def predict(model_path: str, parameters: Dict) -> np.ndarray:
#     """
#     Run inference on specific snapshot number 
#     Args:
#         model_path: saved model path 
#         parameters: parameters for running the inference 
#     Returns: 
#         model inference result as numpy arrays
#     """
#     logger = logging.getLogger(__name__)
#     logger.info(f"Loading model {model_path}")
#     model_obj = torch.load(model_path)
#     pred = model_obj.predict(parameters['snap_num'])
    
#     return pred
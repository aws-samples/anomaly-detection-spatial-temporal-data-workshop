{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb6bd26",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02268e2",
   "metadata": {},
   "source": [
    "# This notebook uses the pre-processed Reddit user-behavior data to train ELAND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1954f6",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Loading data\n",
    "2. Setting up the model trainer\n",
    "3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f731a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/external-repos/anomaly-detection-spatial-temporal-data-workshop/src/kedro-eland-venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import pickle as pk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, CosineEmbeddingLoss\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f014a",
   "metadata": {},
   "source": [
    "### 1. Loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b558c",
   "metadata": {},
   "source": [
    "### User labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a528e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_label = pd.read_csv(\"../../data/02_intermediate/user_behavior/user_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1109cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultimatt42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jonknee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dons</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jedravent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burtonmkz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sblinn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WebZen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doodahdei</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tack122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author  label\n",
       "0    ultimatt42      0\n",
       "1       jonknee      0\n",
       "2          dons      0\n",
       "3     Jedravent      0\n",
       "4     burtonmkz      0\n",
       "5  pavel_lishin      0\n",
       "6        sblinn      0\n",
       "7        WebZen      0\n",
       "8     doodahdei      0\n",
       "9       Tack122      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_label.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b87c37",
   "metadata": {},
   "source": [
    "#### User and subreddit topic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b329f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/u2index.pkl\",\"rb\") as f:\n",
    "    u2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186209ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/p2index.pkl\",\"rb\") as f:\n",
    "    p2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20e7b7",
   "metadata": {},
   "source": [
    "#### Edge list data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826af68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_df = pd.read_csv(\"../../data/02_intermediate/user_behavior/edge_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a9178c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>retrieved_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultimatt42</td>\n",
       "      <td>science</td>\n",
       "      <td>1425846806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jonknee</td>\n",
       "      <td>programming</td>\n",
       "      <td>1425846807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burtonmkz</td>\n",
       "      <td>science</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sblinn</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dons</td>\n",
       "      <td>programming</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jedravent</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WebZen</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doodahdei</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author    subreddit  retrieved_on\n",
       "0    ultimatt42      science    1425846806\n",
       "1       jonknee  programming    1425846807\n",
       "2     burtonmkz      science    1425846810\n",
       "3  pavel_lishin   reddit.com    1425846810\n",
       "4  pavel_lishin   reddit.com    1425846810\n",
       "5        sblinn     politics    1425846810\n",
       "6          dons  programming    1425846811\n",
       "7     Jedravent     politics    1425846811\n",
       "8        WebZen     politics    1425846811\n",
       "9     doodahdei     politics    1425846812"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgelist_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12435806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "def process_edgelist(edge_list, u2index, p2index):\n",
    "    \"\"\" Load edge list and construct a graph \"\"\"\n",
    "    edges = Counter()\n",
    "\n",
    "    for i, row in edge_list.iterrows():\n",
    "        #u = row[0]\n",
    "        #p = row[1]\n",
    "        #t = row[2]\n",
    "        u = row['author']\n",
    "        p = row['subreddit']\n",
    "        t = row['retrieved_on']\n",
    "\n",
    "        if i<1:\n",
    "            print(u, p, t)\n",
    "        edges[(u2index[u], p2index[p])] += 1\n",
    "    # Construct the graph\n",
    "    row = []\n",
    "    col = []\n",
    "    entry = []\n",
    "    for edge, w in edges.items():\n",
    "        #print(w)\n",
    "        i, j = edge\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "        entry.append(w)\n",
    "    graph = csr_matrix(\n",
    "        (entry, (row, col)), \n",
    "        shape=(len(u2index), len(p2index))\n",
    "    )   \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09bbb377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultimatt42 science 1425846806\n"
     ]
    }
   ],
   "source": [
    "graph = process_edgelist(edgelist_df, u2index, p2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f9dc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5189f5e",
   "metadata": {},
   "source": [
    "#### Train/validation/test id split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0291fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/data_tvt.pkl\",\"rb\") as f:\n",
    "    tvt_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b71f3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = tvt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "516d0174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195,), (198,), (393,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train.shape, idx_val.shape, idx_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a864241",
   "metadata": {},
   "source": [
    "#### Convert label format (to numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbcc8016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(labels: pd.DataFrame) -> np.array:\n",
    "    \"\"\"process label information\"\"\"\n",
    "    u_all = set()\n",
    "    pos_uids = set()\n",
    "    labeled_uids = set()\n",
    "    #convert a dataframe to an numpy array, array index being mapped indexes from u2index\n",
    "    for i,row in labels.iterrows():\n",
    "        author = row['author']\n",
    "        author_label = row['label']\n",
    "        u_all.add(author)\n",
    "        if author_label == 1:\n",
    "            pos_uids.add(author)\n",
    "            labeled_uids.add(author)\n",
    "        elif author_label == 0:\n",
    "            labeled_uids.add(author)\n",
    "    print(f'loaded labels, total of {len(pos_uids)} positive users and {len(labeled_uids)} labeled users')\n",
    "    labels = np.zeros(len(u2index))\n",
    "    for u in u2index:\n",
    "        if u in pos_uids:\n",
    "            labels[u2index[u]] = 1\n",
    "    labels = labels.astype(int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddb628de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 327 positive users and 787 labeled users\n"
     ]
    }
   ],
   "source": [
    "labels = process_label(user_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "123f6409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: total of   195 users with    75 pos users and   120 neg users\n",
      "Val:   total of   198 users with    87 pos users and   111 neg users\n",
      "Test:  total of   393 users with   165 pos users and   228 neg users\n"
     ]
    }
   ],
   "source": [
    "print('Train: total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_train), \n",
    "    np.sum(labels[idx_train]), \n",
    "    len(idx_train)-np.sum(labels[idx_train]))\n",
    "     )\n",
    "print('Val:   total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_val), \n",
    "    np.sum(labels[idx_val]), \n",
    "    len(idx_val)-np.sum(labels[idx_val]))\n",
    "     )\n",
    "print('Test:  total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_test), \n",
    "    np.sum(labels[idx_test]), \n",
    "    len(idx_test)-np.sum(labels[idx_test]))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2508311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787, 300)\n"
     ]
    }
   ],
   "source": [
    "user_features = np.load(\"../../data/02_intermediate/user_behavior/user2vec_npy.npz\")\n",
    "print(user_features['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05d54bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 300)\n"
     ]
    }
   ],
   "source": [
    "item_features = np.load(\"../../data/02_intermediate/user_behavior/prod2vec_npy.npz\")\n",
    "print(item_features['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c1190a",
   "metadata": {},
   "source": [
    "### 2. Setting up the model trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe19134",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66d6f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomaly_detection_spatial_temporal_data.model.data_loader import DynamicGraphWNFDataSet, DynamicGraphWNodeFeatDatasetLoader\n",
    "from anomaly_detection_spatial_temporal_data.model.dynamic_graph import Eland_e2e\n",
    "from anomaly_detection_spatial_temporal_data.model.model_config import ElandConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53510635",
   "metadata": {},
   "source": [
    "#### Set up dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b25cac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 327 positive users and 787 labeled users\n",
      "Train: total of   195 users with    75 pos users and   120 neg users\n",
      "Val:   total of   198 users with    87 pos users and   111 neg users\n",
      "Test:  total of   393 users with   165 pos users and   228 neg users\n"
     ]
    }
   ],
   "source": [
    "data_loader = DynamicGraphWNodeFeatDatasetLoader(\n",
    "    user_label, \n",
    "    u2index, \n",
    "    p2index, \n",
    "    edgelist_df, \n",
    "    tvt_idx, \n",
    "    user_features['data'], \n",
    "    item_features['data']\n",
    ")\n",
    "\n",
    "#sequential data loader\n",
    "dataset = DynamicGraphWNFDataSet(p2index, item_features['data'], edgelist_df)\n",
    "lstm_dataloader = DataLoader(dataset, batch_size=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9c465a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "        'graph': data_loader.graph, \n",
    "        'lstm_dataloader': lstm_dataloader,\n",
    "        'user_features': data_loader.user_features,\n",
    "        'item_features': data_loader.item_features,\n",
    "        'labels': data_loader.labels,\n",
    "        'tvt_nids': data_loader.tvt_idx,\n",
    "        'u2index': data_loader.u2index,\n",
    "        'p2index': data_loader.p2index\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5cd963",
   "metadata": {},
   "source": [
    "#### Load model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cad3248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcd9f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_file = '../../conf/base/parameters/eland.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d48b9c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eland_data_load_options': {'dataset': 'reddit', 'baseline': 'store_true', 'batch_size': 300}, 'eland_model_options': {'dim_feats': 300, 'cuda': 0, 'hidden_size': 128, 'n_layers': 2, 'epochs': 50, 'batch_size': 300, 'seed': -1, 'lr': 0.0001, 'log': True, 'weight_decay': 1e-06, 'dropout': 0.4, 'tensorboard': False, 'name': 'debug', 'gnnlayer_type': 'gcn', 'rnn_type': 'lstm', 'pretrain_bm': 25, 'pretrain_nc': 200, 'alpha': 0.05, 'bmloss_type': 'mse', 'device': 'cpu', 'base_pred': 400, 'save_directory': 'data/07_model_output/user_behavior'}}\n"
     ]
    }
   ],
   "source": [
    "with open(model_config_file, \"r\") as stream:\n",
    "    try:\n",
    "        mode_config=yaml.safe_load(stream)\n",
    "        print(mode_config)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a0e214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a log directory for notebook training session \n",
    "from pathlib import Path\n",
    "log_dir = Path('logs/')\n",
    "log_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44eb5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "eland_config = ElandConfig(mode_config['eland_model_options'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32b022",
   "metadata": {},
   "source": [
    "#### Adjust model directory for notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92320bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/07_model_output/user_behavior'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eland_config.save_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "509650ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "eland_config.save_directory = '../../data/07_model_output/user_behavior/'\n",
    "eland_config.epochs = 10 # reduce to 10 epochs in notebooks for demonstration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b123a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(eland_config.save_directory):\n",
    "    os.makedirs(eland_config.save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1bfde",
   "metadata": {},
   "source": [
    "### 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cba6844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-12 22:26:32,177 - Parameters: {'dim_feats': 300, 'hidden_size': 128, 'n_layers': 2, 'lr': 0.0001, 'weight_decay': 1e-06, 'dropout': 0.4, 'gnnlayer_type': 'gcn', 'rnn_type': 'lstm', 'bmloss_type': 'mse'}\n",
      "2022-08-12 22:26:38,437 - BM Module pretrain, Epoch 1/25: loss 104.97600428\n",
      "2022-08-12 22:26:43,544 - BM Module pretrain, Epoch 2/25: loss 99.27821954\n",
      "2022-08-12 22:26:48,775 - BM Module pretrain, Epoch 3/25: loss 91.91539001\n",
      "2022-08-12 22:26:53,843 - BM Module pretrain, Epoch 4/25: loss 81.15819422\n",
      "2022-08-12 22:26:58,970 - BM Module pretrain, Epoch 5/25: loss 67.96443526\n",
      "2022-08-12 22:27:04,149 - BM Module pretrain, Epoch 6/25: loss 56.07752895\n",
      "2022-08-12 22:27:09,218 - BM Module pretrain, Epoch 7/25: loss 47.38684654\n",
      "2022-08-12 22:27:14,355 - BM Module pretrain, Epoch 8/25: loss 40.11199252\n",
      "2022-08-12 22:27:19,463 - BM Module pretrain, Epoch 9/25: loss 33.8731308\n",
      "2022-08-12 22:27:24,491 - BM Module pretrain, Epoch 10/25: loss 28.78942426\n",
      "2022-08-12 22:27:29,604 - BM Module pretrain, Epoch 11/25: loss 24.92529122\n",
      "2022-08-12 22:27:34,639 - BM Module pretrain, Epoch 12/25: loss 22.24799109\n",
      "2022-08-12 22:27:39,820 - BM Module pretrain, Epoch 13/25: loss 20.31170559\n",
      "2022-08-12 22:27:44,831 - BM Module pretrain, Epoch 14/25: loss 18.88890441\n",
      "2022-08-12 22:27:49,928 - BM Module pretrain, Epoch 15/25: loss 17.9292264\n",
      "2022-08-12 22:27:55,036 - BM Module pretrain, Epoch 16/25: loss 17.2460707\n",
      "2022-08-12 22:28:00,096 - BM Module pretrain, Epoch 17/25: loss 16.71153529\n",
      "2022-08-12 22:28:05,898 - BM Module pretrain, Epoch 18/25: loss 16.41131949\n",
      "2022-08-12 22:28:10,990 - BM Module pretrain, Epoch 19/25: loss 16.29038095\n",
      "2022-08-12 22:28:16,077 - BM Module pretrain, Epoch 20/25: loss 16.24760437\n",
      "2022-08-12 22:28:21,150 - BM Module pretrain, Epoch 21/25: loss 16.23513087\n",
      "2022-08-12 22:28:26,190 - BM Module pretrain, Epoch 22/25: loss 16.16652568\n",
      "2022-08-12 22:28:31,420 - BM Module pretrain, Epoch 23/25: loss 16.11224024\n",
      "2022-08-12 22:28:36,484 - BM Module pretrain, Epoch 24/25: loss 16.05609918\n",
      "2022-08-12 22:28:41,605 - BM Module pretrain, Epoch 25/25: loss 16.03398705\n",
      "2022-08-12 22:28:41,659 - NCNet pretrain, Epoch [1 / 200]: loss 0.6841, training auc: 0.2491, val_auc 0.1033, test auc 0.1821\n",
      "2022-08-12 22:28:41,694 - NCNet pretrain, Epoch [2 / 200]: loss 0.6783, training auc: 0.3662, val_auc 0.1882, test auc 0.2758\n",
      "2022-08-12 22:28:41,729 - NCNet pretrain, Epoch [3 / 200]: loss 0.6743, training auc: 0.4464, val_auc 0.2401, test auc 0.3569\n",
      "2022-08-12 22:28:41,763 - NCNet pretrain, Epoch [4 / 200]: loss 0.6802, training auc: 0.3621, val_auc 0.3151, test auc 0.4401\n",
      "2022-08-12 22:28:41,798 - NCNet pretrain, Epoch [5 / 200]: loss 0.6817, training auc: 0.2673, val_auc 0.3864, test auc 0.5325\n",
      "2022-08-12 22:28:41,833 - NCNet pretrain, Epoch [6 / 200]: loss 0.6580, training auc: 0.6493, val_auc 0.4986, test auc 0.6264\n",
      "2022-08-12 22:28:41,868 - NCNet pretrain, Epoch [7 / 200]: loss 0.6673, training auc: 0.5562, val_auc 0.5945, test auc 0.7002\n",
      "2022-08-12 22:28:41,903 - NCNet pretrain, Epoch [8 / 200]: loss 0.6678, training auc: 0.4708, val_auc 0.6784, test auc 0.7736\n",
      "2022-08-12 22:28:41,937 - NCNet pretrain, Epoch [9 / 200]: loss 0.6649, training auc: 0.5700, val_auc 0.7625, test auc 0.8405\n",
      "2022-08-12 22:28:41,975 - NCNet pretrain, Epoch [10 / 200]: loss 0.6612, training auc: 0.6028, val_auc 0.8266, test auc 0.8852\n",
      "2022-08-12 22:28:42,009 - NCNet pretrain, Epoch [11 / 200]: loss 0.6567, training auc: 0.6814, val_auc 0.8691, test auc 0.9136\n",
      "2022-08-12 22:28:42,044 - NCNet pretrain, Epoch [12 / 200]: loss 0.6566, training auc: 0.6811, val_auc 0.8990, test auc 0.9325\n",
      "2022-08-12 22:28:42,079 - NCNet pretrain, Epoch [13 / 200]: loss 0.6634, training auc: 0.5903, val_auc 0.9184, test auc 0.9447\n",
      "2022-08-12 22:28:42,115 - NCNet pretrain, Epoch [14 / 200]: loss 0.6642, training auc: 0.5259, val_auc 0.9282, test auc 0.9506\n",
      "2022-08-12 22:28:42,150 - NCNet pretrain, Epoch [15 / 200]: loss 0.6514, training auc: 0.7204, val_auc 0.9391, test auc 0.9582\n",
      "2022-08-12 22:28:42,186 - NCNet pretrain, Epoch [16 / 200]: loss 0.6463, training auc: 0.7904, val_auc 0.9454, test auc 0.9620\n",
      "2022-08-12 22:28:42,221 - NCNet pretrain, Epoch [17 / 200]: loss 0.6593, training auc: 0.6047, val_auc 0.9512, test auc 0.9653\n",
      "2022-08-12 22:28:42,255 - NCNet pretrain, Epoch [18 / 200]: loss 0.6495, training auc: 0.7736, val_auc 0.9543, test auc 0.9680\n",
      "2022-08-12 22:28:42,290 - NCNet pretrain, Epoch [19 / 200]: loss 0.6533, training auc: 0.7243, val_auc 0.9564, test auc 0.9696\n",
      "2022-08-12 22:28:42,325 - NCNet pretrain, Epoch [20 / 200]: loss 0.6511, training auc: 0.7527, val_auc 0.9580, test auc 0.9707\n",
      "2022-08-12 22:28:42,361 - NCNet pretrain, Epoch [21 / 200]: loss 0.6382, training auc: 0.8723, val_auc 0.9628, test auc 0.9716\n",
      "2022-08-12 22:28:42,396 - NCNet pretrain, Epoch [22 / 200]: loss 0.6477, training auc: 0.8014, val_auc 0.9639, test auc 0.9726\n",
      "2022-08-12 22:28:42,430 - NCNet pretrain, Epoch [23 / 200]: loss 0.6467, training auc: 0.7619, val_auc 0.9666, test auc 0.9741\n",
      "2022-08-12 22:28:42,465 - NCNet pretrain, Epoch [24 / 200]: loss 0.6401, training auc: 0.8671, val_auc 0.9683, test auc 0.9746\n",
      "2022-08-12 22:28:42,499 - NCNet pretrain, Epoch [25 / 200]: loss 0.6426, training auc: 0.8547, val_auc 0.9693, test auc 0.9751\n",
      "2022-08-12 22:28:42,533 - NCNet pretrain, Epoch [26 / 200]: loss 0.6272, training auc: 0.9250, val_auc 0.9700, test auc 0.9755\n",
      "2022-08-12 22:28:42,568 - NCNet pretrain, Epoch [27 / 200]: loss 0.6451, training auc: 0.8351, val_auc 0.9704, test auc 0.9760\n",
      "2022-08-12 22:28:42,603 - NCNet pretrain, Epoch [28 / 200]: loss 0.6242, training auc: 0.9401, val_auc 0.9709, test auc 0.9766\n",
      "2022-08-12 22:28:42,637 - NCNet pretrain, Epoch [29 / 200]: loss 0.6271, training auc: 0.9243, val_auc 0.9714, test auc 0.9768\n",
      "2022-08-12 22:28:42,671 - NCNet pretrain, Epoch [30 / 200]: loss 0.6306, training auc: 0.9117, val_auc 0.9715, test auc 0.9772\n",
      "2022-08-12 22:28:42,712 - NCNet pretrain, Epoch [31 / 200]: loss 0.6277, training auc: 0.9204, val_auc 0.9720, test auc 0.9775\n",
      "2022-08-12 22:28:42,746 - NCNet pretrain, Epoch [32 / 200]: loss 0.6193, training auc: 0.9256, val_auc 0.9732, test auc 0.9776\n",
      "2022-08-12 22:28:42,781 - NCNet pretrain, Epoch [33 / 200]: loss 0.6212, training auc: 0.9447, val_auc 0.9736, test auc 0.9778\n",
      "2022-08-12 22:28:42,816 - NCNet pretrain, Epoch [34 / 200]: loss 0.6173, training auc: 0.9286, val_auc 0.9750, test auc 0.9780\n",
      "2022-08-12 22:28:42,851 - NCNet pretrain, Epoch [35 / 200]: loss 0.6200, training auc: 0.9321, val_auc 0.9756, test auc 0.9783\n",
      "2022-08-12 22:28:42,885 - NCNet pretrain, Epoch [36 / 200]: loss 0.6250, training auc: 0.9421, val_auc 0.9757, test auc 0.9784\n",
      "2022-08-12 22:28:42,919 - NCNet pretrain, Epoch [37 / 200]: loss 0.6262, training auc: 0.9117, val_auc 0.9762, test auc 0.9785\n",
      "2022-08-12 22:28:42,954 - NCNet pretrain, Epoch [38 / 200]: loss 0.6273, training auc: 0.9236, val_auc 0.9765, test auc 0.9787\n",
      "2022-08-12 22:28:42,990 - NCNet pretrain, Epoch [39 / 200]: loss 0.6227, training auc: 0.9411, val_auc 0.9767, test auc 0.9788\n",
      "2022-08-12 22:28:43,032 - NCNet pretrain, Epoch [40 / 200]: loss 0.6207, training auc: 0.9359, val_auc 0.9768, test auc 0.9790\n",
      "2022-08-12 22:28:43,066 - NCNet pretrain, Epoch [41 / 200]: loss 0.6092, training auc: 0.9560, val_auc 0.9769, test auc 0.9791\n",
      "2022-08-12 22:28:43,101 - NCNet pretrain, Epoch [42 / 200]: loss 0.6135, training auc: 0.9507, val_auc 0.9771, test auc 0.9792\n",
      "2022-08-12 22:28:43,136 - NCNet pretrain, Epoch [43 / 200]: loss 0.6122, training auc: 0.9456, val_auc 0.9773, test auc 0.9793\n",
      "2022-08-12 22:28:43,168 - NCNet pretrain, Epoch [44 / 200]: loss 0.6121, training auc: 0.9583, val_auc 0.9774, test auc 0.9793\n",
      "2022-08-12 22:28:43,204 - NCNet pretrain, Epoch [45 / 200]: loss 0.6170, training auc: 0.9579, val_auc 0.9775, test auc 0.9794\n",
      "2022-08-12 22:28:43,236 - NCNet pretrain, Epoch [46 / 200]: loss 0.6121, training auc: 0.9469, val_auc 0.9776, test auc 0.9794\n",
      "2022-08-12 22:28:43,265 - NCNet pretrain, Epoch [47 / 200]: loss 0.6074, training auc: 0.9622, val_auc 0.9776\n",
      "2022-08-12 22:28:43,295 - NCNet pretrain, Epoch [48 / 200]: loss 0.6174, training auc: 0.9566, val_auc 0.9776\n",
      "2022-08-12 22:28:43,325 - NCNet pretrain, Epoch [49 / 200]: loss 0.5988, training auc: 0.9564, val_auc 0.9776\n",
      "2022-08-12 22:28:43,354 - NCNet pretrain, Epoch [50 / 200]: loss 0.6053, training auc: 0.9460, val_auc 0.9776\n",
      "2022-08-12 22:28:43,384 - NCNet pretrain, Epoch [51 / 200]: loss 0.6071, training auc: 0.9590, val_auc 0.9776\n",
      "2022-08-12 22:28:43,414 - NCNet pretrain, Epoch [52 / 200]: loss 0.6062, training auc: 0.9480, val_auc 0.9776\n",
      "2022-08-12 22:28:43,449 - NCNet pretrain, Epoch [53 / 200]: loss 0.6013, training auc: 0.9630, val_auc 0.9777, test auc 0.9799\n",
      "2022-08-12 22:28:43,483 - NCNet pretrain, Epoch [54 / 200]: loss 0.5938, training auc: 0.9627, val_auc 0.9778, test auc 0.9799\n",
      "2022-08-12 22:28:43,513 - NCNet pretrain, Epoch [55 / 200]: loss 0.5973, training auc: 0.9590, val_auc 0.9778\n",
      "2022-08-12 22:28:43,542 - NCNet pretrain, Epoch [56 / 200]: loss 0.5998, training auc: 0.9523, val_auc 0.9778\n",
      "2022-08-12 22:28:43,571 - NCNet pretrain, Epoch [57 / 200]: loss 0.5965, training auc: 0.9657, val_auc 0.9778\n",
      "2022-08-12 22:28:43,603 - NCNet pretrain, Epoch [58 / 200]: loss 0.6064, training auc: 0.9503, val_auc 0.9778\n",
      "2022-08-12 22:28:43,632 - NCNet pretrain, Epoch [59 / 200]: loss 0.6012, training auc: 0.9574, val_auc 0.9778\n",
      "2022-08-12 22:28:43,662 - NCNet pretrain, Epoch [60 / 200]: loss 0.5916, training auc: 0.9463, val_auc 0.9778\n",
      "2022-08-12 22:28:43,692 - NCNet pretrain, Epoch [61 / 200]: loss 0.6054, training auc: 0.9617, val_auc 0.9778\n",
      "2022-08-12 22:28:43,722 - NCNet pretrain, Epoch [62 / 200]: loss 0.6060, training auc: 0.9563, val_auc 0.9778\n",
      "2022-08-12 22:28:43,751 - NCNet pretrain, Epoch [63 / 200]: loss 0.5985, training auc: 0.9586, val_auc 0.9778\n",
      "2022-08-12 22:28:43,786 - NCNet pretrain, Epoch [64 / 200]: loss 0.5992, training auc: 0.9611, val_auc 0.9779, test auc 0.9801\n",
      "2022-08-12 22:28:43,815 - NCNet pretrain, Epoch [65 / 200]: loss 0.6002, training auc: 0.9570, val_auc 0.9779\n",
      "2022-08-12 22:28:43,844 - NCNet pretrain, Epoch [66 / 200]: loss 0.5912, training auc: 0.9550, val_auc 0.9779\n",
      "2022-08-12 22:28:43,873 - NCNet pretrain, Epoch [67 / 200]: loss 0.5830, training auc: 0.9664, val_auc 0.9779\n",
      "2022-08-12 22:28:43,902 - NCNet pretrain, Epoch [68 / 200]: loss 0.5881, training auc: 0.9590, val_auc 0.9779\n",
      "2022-08-12 22:28:43,931 - NCNet pretrain, Epoch [69 / 200]: loss 0.5908, training auc: 0.9634, val_auc 0.9779\n",
      "2022-08-12 22:28:43,965 - NCNet pretrain, Epoch [70 / 200]: loss 0.5908, training auc: 0.9714, val_auc 0.9782, test auc 0.9802\n",
      "2022-08-12 22:28:44,001 - NCNet pretrain, Epoch [71 / 200]: loss 0.5896, training auc: 0.9688, val_auc 0.9783, test auc 0.9802\n",
      "2022-08-12 22:28:44,034 - NCNet pretrain, Epoch [72 / 200]: loss 0.5969, training auc: 0.9644, val_auc 0.9785, test auc 0.9803\n",
      "2022-08-12 22:28:44,066 - NCNet pretrain, Epoch [73 / 200]: loss 0.5862, training auc: 0.9597, val_auc 0.9788, test auc 0.9803\n",
      "2022-08-12 22:28:44,099 - NCNet pretrain, Epoch [74 / 200]: loss 0.5877, training auc: 0.9678, val_auc 0.9789, test auc 0.9802\n",
      "2022-08-12 22:28:44,134 - NCNet pretrain, Epoch [75 / 200]: loss 0.5859, training auc: 0.9569, val_auc 0.9790, test auc 0.9803\n",
      "2022-08-12 22:28:44,170 - NCNet pretrain, Epoch [76 / 200]: loss 0.5814, training auc: 0.9534, val_auc 0.9791, test auc 0.9803\n",
      "2022-08-12 22:28:44,213 - NCNet pretrain, Epoch [77 / 200]: loss 0.5960, training auc: 0.9649, val_auc 0.9792, test auc 0.9803\n",
      "2022-08-12 22:28:44,243 - NCNet pretrain, Epoch [78 / 200]: loss 0.5867, training auc: 0.9559, val_auc 0.9792\n",
      "2022-08-12 22:28:44,273 - NCNet pretrain, Epoch [79 / 200]: loss 0.5855, training auc: 0.9653, val_auc 0.9792\n",
      "2022-08-12 22:28:44,305 - NCNet pretrain, Epoch [80 / 200]: loss 0.5884, training auc: 0.9604, val_auc 0.9792\n",
      "2022-08-12 22:28:44,339 - NCNet pretrain, Epoch [81 / 200]: loss 0.5825, training auc: 0.9592, val_auc 0.9792\n",
      "2022-08-12 22:28:44,387 - NCNet pretrain, Epoch [82 / 200]: loss 0.5802, training auc: 0.9671, val_auc 0.9793, test auc 0.9804\n",
      "2022-08-12 22:28:44,418 - NCNet pretrain, Epoch [83 / 200]: loss 0.5768, training auc: 0.9572, val_auc 0.9793\n",
      "2022-08-12 22:28:44,449 - NCNet pretrain, Epoch [84 / 200]: loss 0.5768, training auc: 0.9601, val_auc 0.9792\n",
      "2022-08-12 22:28:44,479 - NCNet pretrain, Epoch [85 / 200]: loss 0.5804, training auc: 0.9618, val_auc 0.9792\n",
      "2022-08-12 22:28:44,509 - NCNet pretrain, Epoch [86 / 200]: loss 0.5764, training auc: 0.9587, val_auc 0.9792\n",
      "2022-08-12 22:28:44,540 - NCNet pretrain, Epoch [87 / 200]: loss 0.5795, training auc: 0.9587, val_auc 0.9793\n",
      "2022-08-12 22:28:44,575 - NCNet pretrain, Epoch [88 / 200]: loss 0.5844, training auc: 0.9583, val_auc 0.9794, test auc 0.9805\n",
      "2022-08-12 22:28:44,610 - NCNet pretrain, Epoch [89 / 200]: loss 0.5710, training auc: 0.9617, val_auc 0.9795, test auc 0.9805\n",
      "2022-08-12 22:28:44,640 - NCNet pretrain, Epoch [90 / 200]: loss 0.5640, training auc: 0.9614, val_auc 0.9795\n",
      "2022-08-12 22:28:44,670 - NCNet pretrain, Epoch [91 / 200]: loss 0.5723, training auc: 0.9739, val_auc 0.9795\n",
      "2022-08-12 22:28:44,701 - NCNet pretrain, Epoch [92 / 200]: loss 0.5758, training auc: 0.9538, val_auc 0.9795\n",
      "2022-08-12 22:28:44,730 - NCNet pretrain, Epoch [93 / 200]: loss 0.5700, training auc: 0.9611, val_auc 0.9795\n",
      "2022-08-12 22:28:44,765 - NCNet pretrain, Epoch [94 / 200]: loss 0.5671, training auc: 0.9667, val_auc 0.9796, test auc 0.9806\n",
      "2022-08-12 22:28:44,795 - NCNet pretrain, Epoch [95 / 200]: loss 0.5716, training auc: 0.9679, val_auc 0.9796\n",
      "2022-08-12 22:28:44,825 - NCNet pretrain, Epoch [96 / 200]: loss 0.5679, training auc: 0.9734, val_auc 0.9796\n",
      "2022-08-12 22:28:44,855 - NCNet pretrain, Epoch [97 / 200]: loss 0.5686, training auc: 0.9696, val_auc 0.9796\n",
      "2022-08-12 22:28:44,887 - NCNet pretrain, Epoch [98 / 200]: loss 0.5675, training auc: 0.9692, val_auc 0.9797, test auc 0.9806\n",
      "2022-08-12 22:28:44,917 - NCNet pretrain, Epoch [99 / 200]: loss 0.5724, training auc: 0.9589, val_auc 0.9797\n",
      "2022-08-12 22:28:44,947 - NCNet pretrain, Epoch [100 / 200]: loss 0.5664, training auc: 0.9672, val_auc 0.9797\n",
      "2022-08-12 22:28:44,976 - NCNet pretrain, Epoch [101 / 200]: loss 0.5779, training auc: 0.9392, val_auc 0.9797\n",
      "2022-08-12 22:28:45,007 - NCNet pretrain, Epoch [102 / 200]: loss 0.5644, training auc: 0.9692, val_auc 0.9797\n",
      "2022-08-12 22:28:45,037 - NCNet pretrain, Epoch [103 / 200]: loss 0.5726, training auc: 0.9640, val_auc 0.9797\n",
      "2022-08-12 22:28:45,071 - NCNet pretrain, Epoch [104 / 200]: loss 0.5754, training auc: 0.9531, val_auc 0.9799, test auc 0.9807\n",
      "2022-08-12 22:28:45,101 - NCNet pretrain, Epoch [105 / 200]: loss 0.5675, training auc: 0.9618, val_auc 0.9799\n",
      "2022-08-12 22:28:45,131 - NCNet pretrain, Epoch [106 / 200]: loss 0.5640, training auc: 0.9656, val_auc 0.9799\n",
      "2022-08-12 22:28:45,165 - NCNet pretrain, Epoch [107 / 200]: loss 0.5575, training auc: 0.9646, val_auc 0.9800, test auc 0.9807\n",
      "2022-08-12 22:28:45,197 - NCNet pretrain, Epoch [108 / 200]: loss 0.5630, training auc: 0.9639, val_auc 0.9800\n",
      "2022-08-12 22:28:45,227 - NCNet pretrain, Epoch [109 / 200]: loss 0.5583, training auc: 0.9638, val_auc 0.9800\n",
      "2022-08-12 22:28:45,257 - NCNet pretrain, Epoch [110 / 200]: loss 0.5576, training auc: 0.9674, val_auc 0.9800\n",
      "2022-08-12 22:28:45,287 - NCNet pretrain, Epoch [111 / 200]: loss 0.5607, training auc: 0.9703, val_auc 0.9800\n",
      "2022-08-12 22:28:45,321 - NCNet pretrain, Epoch [112 / 200]: loss 0.5594, training auc: 0.9649, val_auc 0.9801, test auc 0.9808\n",
      "2022-08-12 22:28:45,351 - NCNet pretrain, Epoch [113 / 200]: loss 0.5659, training auc: 0.9581, val_auc 0.9801\n",
      "2022-08-12 22:28:45,381 - NCNet pretrain, Epoch [114 / 200]: loss 0.5602, training auc: 0.9703, val_auc 0.9801\n",
      "2022-08-12 22:28:45,415 - NCNet pretrain, Epoch [115 / 200]: loss 0.5547, training auc: 0.9702, val_auc 0.9802, test auc 0.9809\n",
      "2022-08-12 22:28:45,446 - NCNet pretrain, Epoch [116 / 200]: loss 0.5590, training auc: 0.9658, val_auc 0.9802\n",
      "2022-08-12 22:28:45,475 - NCNet pretrain, Epoch [117 / 200]: loss 0.5606, training auc: 0.9610, val_auc 0.9802\n",
      "2022-08-12 22:28:45,506 - NCNet pretrain, Epoch [118 / 200]: loss 0.5504, training auc: 0.9681, val_auc 0.9802\n",
      "2022-08-12 22:28:45,536 - NCNet pretrain, Epoch [119 / 200]: loss 0.5583, training auc: 0.9597, val_auc 0.9802\n",
      "2022-08-12 22:28:45,566 - NCNet pretrain, Epoch [120 / 200]: loss 0.5579, training auc: 0.9628, val_auc 0.9802\n",
      "2022-08-12 22:28:45,596 - NCNet pretrain, Epoch [121 / 200]: loss 0.5542, training auc: 0.9680, val_auc 0.9802\n",
      "2022-08-12 22:28:45,625 - NCNet pretrain, Epoch [122 / 200]: loss 0.5580, training auc: 0.9616, val_auc 0.9802\n",
      "2022-08-12 22:28:45,655 - NCNet pretrain, Epoch [123 / 200]: loss 0.5589, training auc: 0.9578, val_auc 0.9802\n",
      "2022-08-12 22:28:45,684 - NCNet pretrain, Epoch [124 / 200]: loss 0.5599, training auc: 0.9600, val_auc 0.9802\n",
      "2022-08-12 22:28:45,714 - NCNet pretrain, Epoch [125 / 200]: loss 0.5533, training auc: 0.9604, val_auc 0.9802\n",
      "2022-08-12 22:28:45,744 - NCNet pretrain, Epoch [126 / 200]: loss 0.5539, training auc: 0.9619, val_auc 0.9802\n",
      "2022-08-12 22:28:45,773 - NCNet pretrain, Epoch [127 / 200]: loss 0.5555, training auc: 0.9637, val_auc 0.9802\n",
      "2022-08-12 22:28:45,802 - NCNet pretrain, Epoch [128 / 200]: loss 0.5615, training auc: 0.9676, val_auc 0.9802\n",
      "2022-08-12 22:28:45,832 - NCNet pretrain, Epoch [129 / 200]: loss 0.5539, training auc: 0.9601, val_auc 0.9802\n",
      "2022-08-12 22:28:45,863 - NCNet pretrain, Epoch [130 / 200]: loss 0.5530, training auc: 0.9569, val_auc 0.9802\n",
      "2022-08-12 22:28:45,893 - NCNet pretrain, Epoch [131 / 200]: loss 0.5413, training auc: 0.9686, val_auc 0.9802\n",
      "2022-08-12 22:28:45,922 - NCNet pretrain, Epoch [132 / 200]: loss 0.5510, training auc: 0.9561, val_auc 0.9802\n",
      "2022-08-12 22:28:45,953 - NCNet pretrain, Epoch [133 / 200]: loss 0.5454, training auc: 0.9613, val_auc 0.9802\n",
      "2022-08-12 22:28:45,983 - NCNet pretrain, Epoch [134 / 200]: loss 0.5449, training auc: 0.9717, val_auc 0.9802\n",
      "2022-08-12 22:28:46,015 - NCNet pretrain, Epoch [135 / 200]: loss 0.5468, training auc: 0.9701, val_auc 0.9802\n",
      "2022-08-12 22:28:46,060 - NCNet pretrain, Epoch [136 / 200]: loss 0.5474, training auc: 0.9672, val_auc 0.9803, test auc 0.9809\n",
      "2022-08-12 22:28:46,101 - NCNet pretrain, Epoch [137 / 200]: loss 0.5517, training auc: 0.9664, val_auc 0.9803\n",
      "2022-08-12 22:28:46,132 - NCNet pretrain, Epoch [138 / 200]: loss 0.5445, training auc: 0.9700, val_auc 0.9803\n",
      "2022-08-12 22:28:46,165 - NCNet pretrain, Epoch [139 / 200]: loss 0.5500, training auc: 0.9666, val_auc 0.9804, test auc 0.9809\n",
      "2022-08-12 22:28:46,197 - NCNet pretrain, Epoch [140 / 200]: loss 0.5453, training auc: 0.9651, val_auc 0.9804\n",
      "2022-08-12 22:28:46,226 - NCNet pretrain, Epoch [141 / 200]: loss 0.5502, training auc: 0.9609, val_auc 0.9804\n",
      "2022-08-12 22:28:46,258 - NCNet pretrain, Epoch [142 / 200]: loss 0.5449, training auc: 0.9646, val_auc 0.9805, test auc 0.9809\n",
      "2022-08-12 22:28:46,293 - NCNet pretrain, Epoch [143 / 200]: loss 0.5509, training auc: 0.9679, val_auc 0.9805\n",
      "2022-08-12 22:28:46,325 - NCNet pretrain, Epoch [144 / 200]: loss 0.5452, training auc: 0.9588, val_auc 0.9806, test auc 0.9809\n",
      "2022-08-12 22:28:46,355 - NCNet pretrain, Epoch [145 / 200]: loss 0.5464, training auc: 0.9596, val_auc 0.9806\n",
      "2022-08-12 22:28:46,386 - NCNet pretrain, Epoch [146 / 200]: loss 0.5371, training auc: 0.9738, val_auc 0.9806\n",
      "2022-08-12 22:28:46,416 - NCNet pretrain, Epoch [147 / 200]: loss 0.5445, training auc: 0.9568, val_auc 0.9806\n",
      "2022-08-12 22:28:46,446 - NCNet pretrain, Epoch [148 / 200]: loss 0.5413, training auc: 0.9657, val_auc 0.9806\n",
      "2022-08-12 22:28:46,477 - NCNet pretrain, Epoch [149 / 200]: loss 0.5365, training auc: 0.9670, val_auc 0.9805\n",
      "2022-08-12 22:28:46,507 - NCNet pretrain, Epoch [150 / 200]: loss 0.5389, training auc: 0.9616, val_auc 0.9805\n",
      "2022-08-12 22:28:46,537 - NCNet pretrain, Epoch [151 / 200]: loss 0.5391, training auc: 0.9674, val_auc 0.9805\n",
      "2022-08-12 22:28:46,567 - NCNet pretrain, Epoch [152 / 200]: loss 0.5423, training auc: 0.9626, val_auc 0.9804\n",
      "2022-08-12 22:28:46,597 - NCNet pretrain, Epoch [153 / 200]: loss 0.5387, training auc: 0.9563, val_auc 0.9803\n",
      "2022-08-12 22:28:46,627 - NCNet pretrain, Epoch [154 / 200]: loss 0.5428, training auc: 0.9677, val_auc 0.9803\n",
      "2022-08-12 22:28:46,657 - NCNet pretrain, Epoch [155 / 200]: loss 0.5322, training auc: 0.9630, val_auc 0.9803\n",
      "2022-08-12 22:28:46,687 - NCNet pretrain, Epoch [156 / 200]: loss 0.5412, training auc: 0.9731, val_auc 0.9803\n",
      "2022-08-12 22:28:46,718 - NCNet pretrain, Epoch [157 / 200]: loss 0.5283, training auc: 0.9664, val_auc 0.9802\n",
      "2022-08-12 22:28:46,748 - NCNet pretrain, Epoch [158 / 200]: loss 0.5324, training auc: 0.9626, val_auc 0.9802\n",
      "2022-08-12 22:28:46,779 - NCNet pretrain, Epoch [159 / 200]: loss 0.5324, training auc: 0.9628, val_auc 0.9800\n",
      "2022-08-12 22:28:46,809 - NCNet pretrain, Epoch [160 / 200]: loss 0.5281, training auc: 0.9674, val_auc 0.9800\n",
      "2022-08-12 22:28:46,838 - NCNet pretrain, Epoch [161 / 200]: loss 0.5246, training auc: 0.9701, val_auc 0.9799\n",
      "2022-08-12 22:28:46,868 - NCNet pretrain, Epoch [162 / 200]: loss 0.5362, training auc: 0.9588, val_auc 0.9799\n",
      "2022-08-12 22:28:46,897 - NCNet pretrain, Epoch [163 / 200]: loss 0.5426, training auc: 0.9633, val_auc 0.9799\n",
      "2022-08-12 22:28:46,926 - NCNet pretrain, Epoch [164 / 200]: loss 0.5346, training auc: 0.9577, val_auc 0.9799\n",
      "2022-08-12 22:28:46,956 - NCNet pretrain, Epoch [165 / 200]: loss 0.5295, training auc: 0.9607, val_auc 0.9799\n",
      "2022-08-12 22:28:46,986 - NCNet pretrain, Epoch [166 / 200]: loss 0.5292, training auc: 0.9619, val_auc 0.9799\n",
      "2022-08-12 22:28:47,017 - NCNet pretrain, Epoch [167 / 200]: loss 0.5371, training auc: 0.9681, val_auc 0.9799\n",
      "2022-08-12 22:28:47,047 - NCNet pretrain, Epoch [168 / 200]: loss 0.5300, training auc: 0.9574, val_auc 0.9799\n",
      "2022-08-12 22:28:47,077 - NCNet pretrain, Epoch [169 / 200]: loss 0.5281, training auc: 0.9558, val_auc 0.9800\n",
      "2022-08-12 22:28:47,106 - NCNet pretrain, Epoch [170 / 200]: loss 0.5271, training auc: 0.9656, val_auc 0.9800\n",
      "2022-08-12 22:28:47,135 - NCNet pretrain, Epoch [171 / 200]: loss 0.5280, training auc: 0.9663, val_auc 0.9799\n",
      "2022-08-12 22:28:47,164 - NCNet pretrain, Epoch [172 / 200]: loss 0.5299, training auc: 0.9610, val_auc 0.9799\n",
      "2022-08-12 22:28:47,195 - NCNet pretrain, Epoch [173 / 200]: loss 0.5316, training auc: 0.9680, val_auc 0.9800\n",
      "2022-08-12 22:28:47,224 - NCNet pretrain, Epoch [174 / 200]: loss 0.5289, training auc: 0.9689, val_auc 0.9800\n",
      "2022-08-12 22:28:47,255 - NCNet pretrain, Epoch [175 / 200]: loss 0.5333, training auc: 0.9580, val_auc 0.9800\n",
      "2022-08-12 22:28:47,285 - NCNet pretrain, Epoch [176 / 200]: loss 0.5251, training auc: 0.9712, val_auc 0.9800\n",
      "2022-08-12 22:28:47,314 - NCNet pretrain, Epoch [177 / 200]: loss 0.5336, training auc: 0.9631, val_auc 0.9802\n",
      "2022-08-12 22:28:47,344 - NCNet pretrain, Epoch [178 / 200]: loss 0.5356, training auc: 0.9678, val_auc 0.9802\n",
      "2022-08-12 22:28:47,374 - NCNet pretrain, Epoch [179 / 200]: loss 0.5309, training auc: 0.9680, val_auc 0.9802\n",
      "2022-08-12 22:28:47,404 - NCNet pretrain, Epoch [180 / 200]: loss 0.5294, training auc: 0.9612, val_auc 0.9802\n",
      "2022-08-12 22:28:47,434 - NCNet pretrain, Epoch [181 / 200]: loss 0.5286, training auc: 0.9591, val_auc 0.9802\n",
      "2022-08-12 22:28:47,465 - NCNet pretrain, Epoch [182 / 200]: loss 0.5309, training auc: 0.9689, val_auc 0.9802\n",
      "2022-08-12 22:28:47,495 - NCNet pretrain, Epoch [183 / 200]: loss 0.5237, training auc: 0.9634, val_auc 0.9802\n",
      "2022-08-12 22:28:47,529 - NCNet pretrain, Epoch [184 / 200]: loss 0.5300, training auc: 0.9659, val_auc 0.9802\n",
      "2022-08-12 22:28:47,561 - NCNet pretrain, Epoch [185 / 200]: loss 0.5320, training auc: 0.9568, val_auc 0.9802\n",
      "2022-08-12 22:28:47,591 - NCNet pretrain, Epoch [186 / 200]: loss 0.5259, training auc: 0.9642, val_auc 0.9802\n",
      "2022-08-12 22:28:47,622 - NCNet pretrain, Epoch [187 / 200]: loss 0.5239, training auc: 0.9654, val_auc 0.9802\n",
      "2022-08-12 22:28:47,663 - NCNet pretrain, Epoch [188 / 200]: loss 0.5271, training auc: 0.9569, val_auc 0.9802\n",
      "2022-08-12 22:28:47,704 - NCNet pretrain, Epoch [189 / 200]: loss 0.5213, training auc: 0.9677, val_auc 0.9802\n",
      "2022-08-12 22:28:47,734 - NCNet pretrain, Epoch [190 / 200]: loss 0.5223, training auc: 0.9639, val_auc 0.9802\n",
      "2022-08-12 22:28:47,763 - NCNet pretrain, Epoch [191 / 200]: loss 0.5120, training auc: 0.9714, val_auc 0.9801\n",
      "2022-08-12 22:28:47,792 - NCNet pretrain, Epoch [192 / 200]: loss 0.5262, training auc: 0.9617, val_auc 0.9801\n",
      "2022-08-12 22:28:47,822 - NCNet pretrain, Epoch [193 / 200]: loss 0.5235, training auc: 0.9652, val_auc 0.9801\n",
      "2022-08-12 22:28:47,852 - NCNet pretrain, Epoch [194 / 200]: loss 0.5229, training auc: 0.9602, val_auc 0.9801\n",
      "2022-08-12 22:28:47,853 - Early stop!\n",
      "2022-08-12 22:28:47,854 - Best Test Results: auc 0.9809, ap 0.9841, f1 0.0000\n",
      "2022-08-12 22:29:13,861 - Eland Training, Epoch [1/10]: loss 2.9115, train_auc: 0.9702, val_auc 0.9832, test_auc 0.9829\n",
      "2022-08-12 22:29:40,941 - Eland Training, Epoch [2/10]: loss 2.9021, train_auc: 0.9674, val_auc 0.9854, test_auc 0.9828\n",
      "2022-08-12 22:30:07,565 - Eland Training, Epoch [3/10]: loss 2.9171, train_auc: 0.9654, val_auc 0.9842\n",
      "2022-08-12 22:30:34,984 - Eland Training, Epoch [4/10]: loss 2.9184, train_auc: 0.9638, val_auc 0.9855, test_auc 0.9831\n",
      "2022-08-12 22:31:02,422 - Eland Training, Epoch [5/10]: loss 2.9237, train_auc: 0.9659, val_auc 0.9870, test_auc 0.9837\n",
      "2022-08-12 22:31:29,797 - Eland Training, Epoch [6/10]: loss 2.9110, train_auc: 0.9634, val_auc 0.9873, test_auc 0.9833\n",
      "2022-08-12 22:31:56,902 - Eland Training, Epoch [7/10]: loss 2.9000, train_auc: 0.9687, val_auc 0.9857\n",
      "2022-08-12 22:32:24,070 - Eland Training, Epoch [8/10]: loss 2.9044, train_auc: 0.9658, val_auc 0.9844\n",
      "2022-08-12 22:32:51,029 - Eland Training, Epoch [9/10]: loss 2.9019, train_auc: 0.9751, val_auc 0.9875, test_auc 0.9844\n",
      "2022-08-12 22:33:18,202 - Eland Training, Epoch [10/10]: loss 2.9139, train_auc: 0.9673, val_auc 0.9823\n",
      "2022-08-12 22:33:18,203 - Best Test Results: auc 0.9844, ap 0.9878, f1 0.1143\n"
     ]
    }
   ],
   "source": [
    "model_obj = Eland_e2e(\n",
    "    data_dict['graph'], \n",
    "    data_dict['lstm_dataloader'], \n",
    "    data_dict['user_features'],\n",
    "    data_dict['item_features'], \n",
    "    data_dict['labels'], \n",
    "    data_dict['tvt_nids'], \n",
    "    data_dict['u2index'],\n",
    "    data_dict['p2index'], \n",
    "    data_dict['item_features'], \n",
    "    eland_config\n",
    ")\n",
    "training_result,save_model_path = model_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0998a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'train_auc': 0.9702222222222223, 'val_auc': 0.983224603914259},\n",
       " 2: {'train_auc': 0.9674444444444443, 'val_auc': 0.9853991922957441},\n",
       " 3: {'train_auc': 0.9654444444444445, 'val_auc': 0.9841565703634669},\n",
       " 4: {'train_auc': 0.9637777777777778, 'val_auc': 0.9855027441234339},\n",
       " 5: {'train_auc': 0.9658888888888889, 'val_auc': 0.9869524697110903},\n",
       " 6: {'train_auc': 0.9634444444444444, 'val_auc': 0.9872631251941596},\n",
       " 7: {'train_auc': 0.9686666666666668, 'val_auc': 0.9857098477788133},\n",
       " 8: {'train_auc': 0.9657777777777777, 'val_auc': 0.9843636740188465},\n",
       " 9: {'train_auc': 0.975111111111111, 'val_auc': 0.9874702288495392},\n",
       " 10: {'train_auc': 0.9673333333333334, 'val_auc': 0.9822926374650512}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2451669",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The Pushshift Reddit Dataset.\n",
    "\n",
    "Tong Zhao, Bo Ni, Wenhao Yu, Zhichun Guo, Neil Shah, and Meng Jiang, 2021. Action Sequence Augmentation for Early Graph-based Anomaly Detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-eland-venv",
   "language": "python",
   "name": "kedro-eland-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99daf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6c48f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "     |████████████████████████████████| 748.8 MB 4.3 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch==1.6.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch==1.6.0) (1.19.5)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a684fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../Eland/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c65d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model.Eland_e2e import Eland_e2e\n",
    "from model.Eland_e2e_unsup import Eland_e2e_uns\n",
    "from Datasets import MyDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e258cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258c7d4",
   "metadata": {},
   "source": [
    "# Examine the reddit data from the ELAND repo to understand what data it expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e27ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Eland/data/reddit/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a533e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "def load_data(data_dir, graph_num):\n",
    "    \"\"\" Initialize u2index, labels, train/validation/test indices \"\"\"\n",
    "    u_all = set()\n",
    "    pos_uids = set()\n",
    "    labeled_uids = set()\n",
    "    with open(f'{data_dir}/userlabels', 'r') as f:\n",
    "        for line in f:\n",
    "            arr = line.strip('\\r\\n').split(',')\n",
    "            u_all.add(arr[0])\n",
    "            if arr[1] == 'anomaly':\n",
    "                pos_uids.add(arr[0])\n",
    "                labeled_uids.add(arr[0])\n",
    "            elif arr[1] == 'benign':\n",
    "                labeled_uids.add(arr[0])\n",
    "    print(f'loaded labels, total of {len(pos_uids)} positive users and {len(labeled_uids)} labeled users')\n",
    "\n",
    "    # get users' features\n",
    "    u2index = pk.load(open(f'{data_dir}/u2index.pkl', 'rb'))\n",
    "    user_feats = np.load(open(f'{data_dir}/user2vec.npy', 'rb'), allow_pickle=True)\n",
    "    # Get prod features\n",
    "    p2index = pk.load(open(f'{data_dir}/p2index.pkl', 'rb'))\n",
    "    item_feats = np.load(open(f'{data_dir}/prod2vec.npy', 'rb'), allow_pickle=True)\n",
    "\n",
    "    labels = np.zeros(len(u2index))\n",
    "    for u in u2index:\n",
    "        if u in pos_uids:\n",
    "            labels[u2index[u]] = 1\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    tvt_idx = pk.load(open(f'{data_dir}/tvt_idx.pkl', 'rb'))\n",
    "    idx_train, idx_val, idx_test = tvt_idx\n",
    "    print('Train: total of {:5} users with {:5} pos users and {:5} neg users'.format(len(idx_train), np.sum(labels[idx_train]), len(idx_train)-np.sum(labels[idx_train])))\n",
    "    print('Val:   total of {:5} users with {:5} pos users and {:5} neg users'.format(len(idx_val), np.sum(labels[idx_val]), len(idx_val)-np.sum(labels[idx_val])))\n",
    "    print('Test:  total of {:5} users with {:5} pos users and {:5} neg users'.format(len(idx_test), np.sum(labels[idx_test]), len(idx_test)-np.sum(labels[idx_test])))\n",
    "\n",
    "    \"\"\" Get graph, graph features, and initialize u2index, p2index \"\"\"\n",
    "    edges = Counter()\n",
    "    n = int(graph_num * 10)\n",
    "    edgelist_file = f'{data_dir}/splitted_edgelist_{n}' if n < 10 else f'{data_dir}/edgelist'\n",
    "    with open(edgelist_file, 'r') as f:\n",
    "        for line in f:\n",
    "            arr = line.strip('\\r\\n').split(',')\n",
    "            u = arr[0]\n",
    "            p = arr[1]\n",
    "            t = int(arr[2])\n",
    "            edges[(u2index[u], p2index[p])] += 1\n",
    "    # Construct the graph\n",
    "    row = []\n",
    "    col = []\n",
    "    entry = []\n",
    "    for edge, w in edges.items():\n",
    "        i, j = edge\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "        entry.append(w)\n",
    "    graph = csr_matrix((entry, (row, col)), shape=(len(u2index), len(p2index)))\n",
    "    return u2index, labels, tvt_idx, user_feats, p2index, item_feats, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22a7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_num = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11352517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    }
   ],
   "source": [
    "u2index, labels, tvt_nids, user_features, p2index, item_features, graph = load_data(data_dir, graph_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f01768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57975it [00:00, 309627.18it/s]\n"
     ]
    }
   ],
   "source": [
    "base_pred = 30\n",
    "\n",
    "# DataLoader\n",
    "\n",
    "n = int(graph_num * 10)\n",
    "\n",
    "edgelist_file = f'{data_dir}/splitted_edgelist_{n}' if n < 10 else f'{data_dir}/edgelist'\n",
    "\n",
    "dataset = MyDataSet(p2index, item_features, edgelist_file)\n",
    "lstm_dataloader = DataLoader(dataset, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de9de8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='debug'\n",
    "baseline=False\n",
    "gnnlayer_type='gcn'\n",
    "rnnlayer_type='lstm'\n",
    "device='cpu'\n",
    "\n",
    "method='gcn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be40aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 22:57:01,834 - Parameters: {'base_pred': 30, 'device': 'cpu', 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'lstm', 'gnnlayer_type': 'gcn', 'name': 'debug', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 10, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n"
     ]
    }
   ],
   "source": [
    "if method in ('dominant', 'deepae'):\n",
    "    eland = Eland_e2e_uns(graph, lstm_dataloader, user_features,\n",
    "            item_features, labels, tvt_nids, u2index,\n",
    "            p2index, item_features, lr=0.01, n_layers=2, name=name, pretrain_bm=25,\n",
    "            pretrain_nc=25, epochs=10, method=args.method, rnn_type=rnnlayer_type, bmloss_type='mse', device=device, base_pred=base_pred)\n",
    "else:\n",
    "    eland = Eland_e2e(graph, lstm_dataloader, user_features,\n",
    "            item_features, labels, tvt_nids, u2index,\n",
    "            p2index, item_features, lr=0.01, n_layers=2, name=name, pretrain_bm=25, epochs=10,\n",
    "            pretrain_nc=300, gnnlayer_type=gnnlayer_type, rnn_type=rnnlayer_type, bmloss_type='mse', device=device, base_pred=base_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "479a4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb2ccb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71523a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 22:57:54,187 - BM Module pretrain, Epoch 1/25: loss 324.97962386\n",
      "2022-05-24 22:58:41,713 - BM Module pretrain, Epoch 2/25: loss 0.87825827\n",
      "2022-05-24 22:59:31,019 - BM Module pretrain, Epoch 3/25: loss 0.6821715\n",
      "2022-05-24 23:00:19,954 - BM Module pretrain, Epoch 4/25: loss 0.63952341\n",
      "2022-05-24 23:01:08,680 - BM Module pretrain, Epoch 5/25: loss 0.6182542\n",
      "2022-05-24 23:01:57,446 - BM Module pretrain, Epoch 6/25: loss 0.59951662\n",
      "2022-05-24 23:02:46,338 - BM Module pretrain, Epoch 7/25: loss 0.59130993\n",
      "2022-05-24 23:03:34,953 - BM Module pretrain, Epoch 8/25: loss 0.58739793\n",
      "2022-05-24 23:04:24,491 - BM Module pretrain, Epoch 9/25: loss 0.59353044\n",
      "2022-05-24 23:05:13,173 - BM Module pretrain, Epoch 10/25: loss 0.58320346\n",
      "2022-05-24 23:06:01,591 - BM Module pretrain, Epoch 11/25: loss 0.57342202\n",
      "2022-05-24 23:06:49,564 - BM Module pretrain, Epoch 12/25: loss 0.56354089\n",
      "2022-05-24 23:07:38,563 - BM Module pretrain, Epoch 13/25: loss 0.54526532\n",
      "2022-05-24 23:08:26,313 - BM Module pretrain, Epoch 14/25: loss 0.54115676\n",
      "2022-05-24 23:09:15,196 - BM Module pretrain, Epoch 15/25: loss 0.53506325\n",
      "2022-05-24 23:10:03,576 - BM Module pretrain, Epoch 16/25: loss 0.53593205\n",
      "2022-05-24 23:10:53,082 - BM Module pretrain, Epoch 17/25: loss 0.52939435\n",
      "2022-05-24 23:11:40,879 - BM Module pretrain, Epoch 18/25: loss 0.52145663\n",
      "2022-05-24 23:12:28,407 - BM Module pretrain, Epoch 19/25: loss 0.52609148\n",
      "2022-05-24 23:13:17,587 - BM Module pretrain, Epoch 20/25: loss 0.52528963\n",
      "2022-05-24 23:14:06,469 - BM Module pretrain, Epoch 21/25: loss 0.52137502\n",
      "2022-05-24 23:14:54,605 - BM Module pretrain, Epoch 22/25: loss 0.52113385\n",
      "2022-05-24 23:15:42,204 - BM Module pretrain, Epoch 23/25: loss 0.51711497\n",
      "2022-05-24 23:16:30,829 - BM Module pretrain, Epoch 24/25: loss 0.51536493\n",
      "2022-05-24 23:17:18,355 - BM Module pretrain, Epoch 25/25: loss 0.51187916\n",
      "2022-05-24 23:17:19,089 - NCNet pretrain, Epoch [1 / 300]: loss 0.6672, training auc: 0.5990, val_auc 0.4919, test auc 0.4795\n",
      "2022-05-24 23:17:19,780 - NCNet pretrain, Epoch [2 / 300]: loss 0.4685, training auc: 0.5041, val_auc 0.4854\n",
      "2022-05-24 23:17:20,472 - NCNet pretrain, Epoch [3 / 300]: loss 0.4515, training auc: 0.4215, val_auc 0.4843\n",
      "2022-05-24 23:17:21,162 - NCNet pretrain, Epoch [4 / 300]: loss 0.4666, training auc: 0.4499, val_auc 0.4865\n",
      "2022-05-24 23:17:21,855 - NCNet pretrain, Epoch [5 / 300]: loss 0.4778, training auc: 0.4316, val_auc 0.4914\n",
      "2022-05-24 23:17:22,570 - NCNet pretrain, Epoch [6 / 300]: loss 0.4402, training auc: 0.5221, val_auc 0.4984, test auc 0.4901\n",
      "2022-05-24 23:17:23,270 - NCNet pretrain, Epoch [7 / 300]: loss 0.4399, training auc: 0.4857, val_auc 0.5059, test auc 0.5034\n",
      "2022-05-24 23:17:23,972 - NCNet pretrain, Epoch [8 / 300]: loss 0.4455, training auc: 0.4338, val_auc 0.5139, test auc 0.5167\n",
      "2022-05-24 23:17:24,673 - NCNet pretrain, Epoch [9 / 300]: loss 0.4440, training auc: 0.4975, val_auc 0.5165, test auc 0.5213\n",
      "2022-05-24 23:17:25,382 - NCNet pretrain, Epoch [10 / 300]: loss 0.4458, training auc: 0.4777, val_auc 0.5171, test auc 0.5217\n",
      "2022-05-24 23:17:26,079 - NCNet pretrain, Epoch [11 / 300]: loss 0.4358, training auc: 0.5433, val_auc 0.5166\n",
      "2022-05-24 23:17:26,766 - NCNet pretrain, Epoch [12 / 300]: loss 0.4349, training auc: 0.5284, val_auc 0.5150\n",
      "2022-05-24 23:17:27,457 - NCNet pretrain, Epoch [13 / 300]: loss 0.4373, training auc: 0.4865, val_auc 0.5147\n",
      "2022-05-24 23:17:28,151 - NCNet pretrain, Epoch [14 / 300]: loss 0.4390, training auc: 0.4737, val_auc 0.5158\n",
      "2022-05-24 23:17:28,870 - NCNet pretrain, Epoch [15 / 300]: loss 0.4340, training auc: 0.5177, val_auc 0.5174, test auc 0.5187\n",
      "2022-05-24 23:17:29,567 - NCNet pretrain, Epoch [16 / 300]: loss 0.4370, training auc: 0.4964, val_auc 0.5198, test auc 0.5220\n",
      "2022-05-24 23:17:30,268 - NCNet pretrain, Epoch [17 / 300]: loss 0.4324, training auc: 0.5309, val_auc 0.5220, test auc 0.5255\n",
      "2022-05-24 23:17:30,973 - NCNet pretrain, Epoch [18 / 300]: loss 0.4278, training auc: 0.5428, val_auc 0.5238, test auc 0.5286\n",
      "2022-05-24 23:17:31,669 - NCNet pretrain, Epoch [19 / 300]: loss 0.4319, training auc: 0.5202, val_auc 0.5256, test auc 0.5317\n",
      "2022-05-24 23:17:32,386 - NCNet pretrain, Epoch [20 / 300]: loss 0.4253, training auc: 0.5480, val_auc 0.5275, test auc 0.5349\n",
      "2022-05-24 23:17:33,097 - NCNet pretrain, Epoch [21 / 300]: loss 0.4301, training auc: 0.5232, val_auc 0.5292, test auc 0.5384\n",
      "2022-05-24 23:17:33,799 - NCNet pretrain, Epoch [22 / 300]: loss 0.4212, training auc: 0.5669, val_auc 0.5314, test auc 0.5418\n",
      "2022-05-24 23:17:34,496 - NCNet pretrain, Epoch [23 / 300]: loss 0.4312, training auc: 0.5176, val_auc 0.5339, test auc 0.5454\n",
      "2022-05-24 23:17:35,215 - NCNet pretrain, Epoch [24 / 300]: loss 0.4275, training auc: 0.5402, val_auc 0.5367, test auc 0.5489\n",
      "2022-05-24 23:17:35,919 - NCNet pretrain, Epoch [25 / 300]: loss 0.4274, training auc: 0.5418, val_auc 0.5395, test auc 0.5520\n",
      "2022-05-24 23:17:36,647 - NCNet pretrain, Epoch [26 / 300]: loss 0.4198, training auc: 0.5602, val_auc 0.5423, test auc 0.5545\n",
      "2022-05-24 23:17:37,352 - NCNet pretrain, Epoch [27 / 300]: loss 0.4188, training auc: 0.5766, val_auc 0.5461, test auc 0.5584\n",
      "2022-05-24 23:17:38,051 - NCNet pretrain, Epoch [28 / 300]: loss 0.4196, training auc: 0.5662, val_auc 0.5505, test auc 0.5648\n",
      "2022-05-24 23:17:38,766 - NCNet pretrain, Epoch [29 / 300]: loss 0.4217, training auc: 0.5711, val_auc 0.5563, test auc 0.5738\n",
      "2022-05-24 23:17:39,470 - NCNet pretrain, Epoch [30 / 300]: loss 0.4198, training auc: 0.5738, val_auc 0.5645, test auc 0.5851\n",
      "2022-05-24 23:17:40,175 - NCNet pretrain, Epoch [31 / 300]: loss 0.4177, training auc: 0.6021, val_auc 0.5734, test auc 0.5942\n",
      "2022-05-24 23:17:40,877 - NCNet pretrain, Epoch [32 / 300]: loss 0.4152, training auc: 0.6063, val_auc 0.5814, test auc 0.6007\n",
      "2022-05-24 23:17:41,581 - NCNet pretrain, Epoch [33 / 300]: loss 0.4053, training auc: 0.6282, val_auc 0.5851, test auc 0.6028\n",
      "2022-05-24 23:17:42,296 - NCNet pretrain, Epoch [34 / 300]: loss 0.4068, training auc: 0.6309, val_auc 0.5919, test auc 0.6085\n",
      "2022-05-24 23:17:42,990 - NCNet pretrain, Epoch [35 / 300]: loss 0.4068, training auc: 0.6263, val_auc 0.6051, test auc 0.6218\n",
      "2022-05-24 23:17:43,692 - NCNet pretrain, Epoch [36 / 300]: loss 0.4032, training auc: 0.6430, val_auc 0.6205, test auc 0.6399\n",
      "2022-05-24 23:17:44,396 - NCNet pretrain, Epoch [37 / 300]: loss 0.4025, training auc: 0.6572, val_auc 0.6348, test auc 0.6553\n",
      "2022-05-24 23:17:45,114 - NCNet pretrain, Epoch [38 / 300]: loss 0.3979, training auc: 0.6719, val_auc 0.6410, test auc 0.6598\n",
      "2022-05-24 23:17:45,815 - NCNet pretrain, Epoch [39 / 300]: loss 0.3945, training auc: 0.6737, val_auc 0.6444, test auc 0.6622\n",
      "2022-05-24 23:17:46,514 - NCNet pretrain, Epoch [40 / 300]: loss 0.3891, training auc: 0.6839, val_auc 0.6539, test auc 0.6735\n",
      "2022-05-24 23:17:47,220 - NCNet pretrain, Epoch [41 / 300]: loss 0.3902, training auc: 0.6768, val_auc 0.6675, test auc 0.6889\n",
      "2022-05-24 23:17:47,917 - NCNet pretrain, Epoch [42 / 300]: loss 0.3742, training auc: 0.7271, val_auc 0.6745, test auc 0.6929\n",
      "2022-05-24 23:17:48,638 - NCNet pretrain, Epoch [43 / 300]: loss 0.3778, training auc: 0.7073, val_auc 0.6818, test auc 0.6979\n",
      "2022-05-24 23:17:49,341 - NCNet pretrain, Epoch [44 / 300]: loss 0.3750, training auc: 0.7075, val_auc 0.6960, test auc 0.7138\n",
      "2022-05-24 23:17:50,036 - NCNet pretrain, Epoch [45 / 300]: loss 0.3763, training auc: 0.7142, val_auc 0.7056, test auc 0.7235\n",
      "2022-05-24 23:17:50,736 - NCNet pretrain, Epoch [46 / 300]: loss 0.3615, training auc: 0.7504, val_auc 0.7052\n",
      "2022-05-24 23:17:51,446 - NCNet pretrain, Epoch [47 / 300]: loss 0.3727, training auc: 0.7240, val_auc 0.7144, test auc 0.7294\n",
      "2022-05-24 23:17:52,155 - NCNet pretrain, Epoch [48 / 300]: loss 0.3659, training auc: 0.7343, val_auc 0.7200, test auc 0.7338\n",
      "2022-05-24 23:17:52,850 - NCNet pretrain, Epoch [49 / 300]: loss 0.3636, training auc: 0.7461, val_auc 0.7274, test auc 0.7422\n",
      "2022-05-24 23:17:53,542 - NCNet pretrain, Epoch [50 / 300]: loss 0.3607, training auc: 0.7524, val_auc 0.7269\n",
      "2022-05-24 23:17:54,248 - NCNet pretrain, Epoch [51 / 300]: loss 0.3576, training auc: 0.7576, val_auc 0.7379, test auc 0.7544\n",
      "2022-05-24 23:17:54,952 - NCNet pretrain, Epoch [52 / 300]: loss 0.3614, training auc: 0.7560, val_auc 0.7344\n",
      "2022-05-24 23:17:55,650 - NCNet pretrain, Epoch [53 / 300]: loss 0.3519, training auc: 0.7612, val_auc 0.7375\n",
      "2022-05-24 23:17:56,352 - NCNet pretrain, Epoch [54 / 300]: loss 0.3475, training auc: 0.7695, val_auc 0.7447, test auc 0.7607\n",
      "2022-05-24 23:17:57,052 - NCNet pretrain, Epoch [55 / 300]: loss 0.3599, training auc: 0.7492, val_auc 0.7477, test auc 0.7650\n",
      "2022-05-24 23:17:57,745 - NCNet pretrain, Epoch [56 / 300]: loss 0.3561, training auc: 0.7627, val_auc 0.7406\n",
      "2022-05-24 23:17:58,456 - NCNet pretrain, Epoch [57 / 300]: loss 0.3467, training auc: 0.7752, val_auc 0.7468\n",
      "2022-05-24 23:17:59,158 - NCNet pretrain, Epoch [58 / 300]: loss 0.3446, training auc: 0.7768, val_auc 0.7541, test auc 0.7764\n",
      "2022-05-24 23:17:59,853 - NCNet pretrain, Epoch [59 / 300]: loss 0.3549, training auc: 0.7778, val_auc 0.7497\n",
      "2022-05-24 23:18:00,551 - NCNet pretrain, Epoch [60 / 300]: loss 0.3461, training auc: 0.7756, val_auc 0.7447\n",
      "2022-05-24 23:18:01,255 - NCNet pretrain, Epoch [61 / 300]: loss 0.3462, training auc: 0.7765, val_auc 0.7527\n",
      "2022-05-24 23:18:01,971 - NCNet pretrain, Epoch [62 / 300]: loss 0.3447, training auc: 0.7854, val_auc 0.7575, test auc 0.7817\n",
      "2022-05-24 23:18:02,734 - NCNet pretrain, Epoch [63 / 300]: loss 0.3481, training auc: 0.7905, val_auc 0.7523\n",
      "2022-05-24 23:18:03,439 - NCNet pretrain, Epoch [64 / 300]: loss 0.3374, training auc: 0.7847, val_auc 0.7494\n",
      "2022-05-24 23:18:04,134 - NCNet pretrain, Epoch [65 / 300]: loss 0.3344, training auc: 0.7908, val_auc 0.7542\n",
      "2022-05-24 23:18:04,850 - NCNet pretrain, Epoch [66 / 300]: loss 0.3336, training auc: 0.7966, val_auc 0.7593, test auc 0.7827\n",
      "2022-05-24 23:18:05,548 - NCNet pretrain, Epoch [67 / 300]: loss 0.3464, training auc: 0.7927, val_auc 0.7579\n",
      "2022-05-24 23:18:06,237 - NCNet pretrain, Epoch [68 / 300]: loss 0.3337, training auc: 0.7956, val_auc 0.7569\n",
      "2022-05-24 23:18:06,928 - NCNet pretrain, Epoch [69 / 300]: loss 0.3363, training auc: 0.7922, val_auc 0.7582\n",
      "2022-05-24 23:18:07,635 - NCNet pretrain, Epoch [70 / 300]: loss 0.3336, training auc: 0.7996, val_auc 0.7615, test auc 0.7837\n",
      "2022-05-24 23:18:08,349 - NCNet pretrain, Epoch [71 / 300]: loss 0.3292, training auc: 0.8050, val_auc 0.7634, test auc 0.7860\n",
      "2022-05-24 23:18:09,069 - NCNet pretrain, Epoch [72 / 300]: loss 0.3403, training auc: 0.7982, val_auc 0.7616\n",
      "2022-05-24 23:18:09,762 - NCNet pretrain, Epoch [73 / 300]: loss 0.3300, training auc: 0.7994, val_auc 0.7607\n",
      "2022-05-24 23:18:10,457 - NCNet pretrain, Epoch [74 / 300]: loss 0.3318, training auc: 0.8051, val_auc 0.7639, test auc 0.7849\n",
      "2022-05-24 23:18:11,157 - NCNet pretrain, Epoch [75 / 300]: loss 0.3314, training auc: 0.8014, val_auc 0.7682, test auc 0.7902\n",
      "2022-05-24 23:18:11,862 - NCNet pretrain, Epoch [76 / 300]: loss 0.3358, training auc: 0.7940, val_auc 0.7667\n",
      "2022-05-24 23:18:12,557 - NCNet pretrain, Epoch [77 / 300]: loss 0.3285, training auc: 0.8055, val_auc 0.7654\n",
      "2022-05-24 23:18:13,256 - NCNet pretrain, Epoch [78 / 300]: loss 0.3252, training auc: 0.8117, val_auc 0.7692, test auc 0.7895\n",
      "2022-05-24 23:18:13,963 - NCNet pretrain, Epoch [79 / 300]: loss 0.3223, training auc: 0.8227, val_auc 0.7713, test auc 0.7928\n",
      "2022-05-24 23:18:14,666 - NCNet pretrain, Epoch [80 / 300]: loss 0.3300, training auc: 0.8100, val_auc 0.7698\n",
      "2022-05-24 23:18:15,460 - NCNet pretrain, Epoch [81 / 300]: loss 0.3309, training auc: 0.7962, val_auc 0.7661\n",
      "2022-05-24 23:18:16,156 - NCNet pretrain, Epoch [82 / 300]: loss 0.3303, training auc: 0.8026, val_auc 0.7718, test auc 0.7915\n",
      "2022-05-24 23:18:16,857 - NCNet pretrain, Epoch [83 / 300]: loss 0.3234, training auc: 0.8126, val_auc 0.7748, test auc 0.7963\n",
      "2022-05-24 23:18:17,552 - NCNet pretrain, Epoch [84 / 300]: loss 0.3299, training auc: 0.8174, val_auc 0.7724\n",
      "2022-05-24 23:18:18,253 - NCNet pretrain, Epoch [85 / 300]: loss 0.3189, training auc: 0.8227, val_auc 0.7681\n",
      "2022-05-24 23:18:18,956 - NCNet pretrain, Epoch [86 / 300]: loss 0.3271, training auc: 0.8218, val_auc 0.7733\n",
      "2022-05-24 23:18:19,653 - NCNet pretrain, Epoch [87 / 300]: loss 0.3225, training auc: 0.8184, val_auc 0.7744\n",
      "2022-05-24 23:18:20,340 - NCNet pretrain, Epoch [88 / 300]: loss 0.3255, training auc: 0.8290, val_auc 0.7723\n",
      "2022-05-24 23:18:21,042 - NCNet pretrain, Epoch [89 / 300]: loss 0.3181, training auc: 0.8358, val_auc 0.7714\n",
      "2022-05-24 23:18:21,751 - NCNet pretrain, Epoch [90 / 300]: loss 0.3197, training auc: 0.8272, val_auc 0.7743\n",
      "2022-05-24 23:18:22,442 - NCNet pretrain, Epoch [91 / 300]: loss 0.3291, training auc: 0.8152, val_auc 0.7748\n",
      "2022-05-24 23:18:23,131 - NCNet pretrain, Epoch [92 / 300]: loss 0.3218, training auc: 0.8300, val_auc 0.7720\n",
      "2022-05-24 23:18:23,823 - NCNet pretrain, Epoch [93 / 300]: loss 0.3273, training auc: 0.8190, val_auc 0.7727\n",
      "2022-05-24 23:18:24,532 - NCNet pretrain, Epoch [94 / 300]: loss 0.3195, training auc: 0.8215, val_auc 0.7761, test auc 0.7957\n",
      "2022-05-24 23:18:25,225 - NCNet pretrain, Epoch [95 / 300]: loss 0.3146, training auc: 0.8394, val_auc 0.7761\n",
      "2022-05-24 23:18:25,918 - NCNet pretrain, Epoch [96 / 300]: loss 0.3154, training auc: 0.8330, val_auc 0.7731\n",
      "2022-05-24 23:18:26,605 - NCNet pretrain, Epoch [97 / 300]: loss 0.3213, training auc: 0.8212, val_auc 0.7736\n",
      "2022-05-24 23:18:27,305 - NCNet pretrain, Epoch [98 / 300]: loss 0.3190, training auc: 0.8221, val_auc 0.7775, test auc 0.7964\n",
      "2022-05-24 23:18:28,022 - NCNet pretrain, Epoch [99 / 300]: loss 0.3199, training auc: 0.8378, val_auc 0.7777, test auc 0.7964\n",
      "2022-05-24 23:18:28,720 - NCNet pretrain, Epoch [100 / 300]: loss 0.3229, training auc: 0.8282, val_auc 0.7756\n",
      "2022-05-24 23:18:29,411 - NCNet pretrain, Epoch [101 / 300]: loss 0.3147, training auc: 0.8360, val_auc 0.7752\n",
      "2022-05-24 23:18:30,101 - NCNet pretrain, Epoch [102 / 300]: loss 0.3073, training auc: 0.8432, val_auc 0.7771\n",
      "2022-05-24 23:18:30,802 - NCNet pretrain, Epoch [103 / 300]: loss 0.3058, training auc: 0.8497, val_auc 0.7765\n",
      "2022-05-24 23:18:31,499 - NCNet pretrain, Epoch [104 / 300]: loss 0.3109, training auc: 0.8296, val_auc 0.7759\n",
      "2022-05-24 23:18:32,192 - NCNet pretrain, Epoch [105 / 300]: loss 0.3006, training auc: 0.8496, val_auc 0.7765\n",
      "2022-05-24 23:18:32,883 - NCNet pretrain, Epoch [106 / 300]: loss 0.3092, training auc: 0.8398, val_auc 0.7763\n",
      "2022-05-24 23:18:33,574 - NCNet pretrain, Epoch [107 / 300]: loss 0.3071, training auc: 0.8420, val_auc 0.7764\n",
      "2022-05-24 23:18:34,287 - NCNet pretrain, Epoch [108 / 300]: loss 0.3087, training auc: 0.8446, val_auc 0.7768\n",
      "2022-05-24 23:18:35,012 - NCNet pretrain, Epoch [109 / 300]: loss 0.3056, training auc: 0.8431, val_auc 0.7761\n",
      "2022-05-24 23:18:35,702 - NCNet pretrain, Epoch [110 / 300]: loss 0.3093, training auc: 0.8421, val_auc 0.7765\n",
      "2022-05-24 23:18:36,389 - NCNet pretrain, Epoch [111 / 300]: loss 0.3069, training auc: 0.8386, val_auc 0.7771\n",
      "2022-05-24 23:18:37,086 - NCNet pretrain, Epoch [112 / 300]: loss 0.3082, training auc: 0.8408, val_auc 0.7770\n",
      "2022-05-24 23:18:37,815 - NCNet pretrain, Epoch [113 / 300]: loss 0.3070, training auc: 0.8455, val_auc 0.7778, test auc 0.7966\n",
      "2022-05-24 23:18:38,518 - NCNet pretrain, Epoch [114 / 300]: loss 0.3031, training auc: 0.8539, val_auc 0.7778\n",
      "2022-05-24 23:18:39,240 - NCNet pretrain, Epoch [115 / 300]: loss 0.3012, training auc: 0.8534, val_auc 0.7763\n",
      "2022-05-24 23:18:39,948 - NCNet pretrain, Epoch [116 / 300]: loss 0.3049, training auc: 0.8428, val_auc 0.7786, test auc 0.7967\n",
      "2022-05-24 23:18:40,667 - NCNet pretrain, Epoch [117 / 300]: loss 0.2990, training auc: 0.8598, val_auc 0.7788, test auc 0.7969\n",
      "2022-05-24 23:18:41,366 - NCNet pretrain, Epoch [118 / 300]: loss 0.2928, training auc: 0.8585, val_auc 0.7759\n",
      "2022-05-24 23:18:42,064 - NCNet pretrain, Epoch [119 / 300]: loss 0.2987, training auc: 0.8533, val_auc 0.7782\n",
      "2022-05-24 23:18:42,765 - NCNet pretrain, Epoch [120 / 300]: loss 0.2998, training auc: 0.8533, val_auc 0.7791, test auc 0.7980\n",
      "2022-05-24 23:18:43,455 - NCNet pretrain, Epoch [121 / 300]: loss 0.2996, training auc: 0.8572, val_auc 0.7756\n",
      "2022-05-24 23:18:44,168 - NCNet pretrain, Epoch [122 / 300]: loss 0.3052, training auc: 0.8484, val_auc 0.7781\n",
      "2022-05-24 23:18:44,857 - NCNet pretrain, Epoch [123 / 300]: loss 0.3075, training auc: 0.8512, val_auc 0.7783\n",
      "2022-05-24 23:18:45,548 - NCNet pretrain, Epoch [124 / 300]: loss 0.3007, training auc: 0.8618, val_auc 0.7751\n",
      "2022-05-24 23:18:46,241 - NCNet pretrain, Epoch [125 / 300]: loss 0.2981, training auc: 0.8580, val_auc 0.7764\n",
      "2022-05-24 23:18:46,935 - NCNet pretrain, Epoch [126 / 300]: loss 0.3034, training auc: 0.8469, val_auc 0.7770\n",
      "2022-05-24 23:18:47,642 - NCNet pretrain, Epoch [127 / 300]: loss 0.3019, training auc: 0.8586, val_auc 0.7753\n",
      "2022-05-24 23:18:48,331 - NCNet pretrain, Epoch [128 / 300]: loss 0.3002, training auc: 0.8519, val_auc 0.7754\n",
      "2022-05-24 23:18:49,028 - NCNet pretrain, Epoch [129 / 300]: loss 0.3057, training auc: 0.8464, val_auc 0.7765\n",
      "2022-05-24 23:18:49,715 - NCNet pretrain, Epoch [130 / 300]: loss 0.2959, training auc: 0.8682, val_auc 0.7753\n",
      "2022-05-24 23:18:50,424 - NCNet pretrain, Epoch [131 / 300]: loss 0.2975, training auc: 0.8532, val_auc 0.7755\n",
      "2022-05-24 23:18:51,122 - NCNet pretrain, Epoch [132 / 300]: loss 0.3027, training auc: 0.8593, val_auc 0.7772\n",
      "2022-05-24 23:18:51,820 - NCNet pretrain, Epoch [133 / 300]: loss 0.2937, training auc: 0.8692, val_auc 0.7772\n",
      "2022-05-24 23:18:52,517 - NCNet pretrain, Epoch [134 / 300]: loss 0.2909, training auc: 0.8604, val_auc 0.7770\n",
      "2022-05-24 23:18:53,207 - NCNet pretrain, Epoch [135 / 300]: loss 0.2993, training auc: 0.8562, val_auc 0.7785\n",
      "2022-05-24 23:18:53,911 - NCNet pretrain, Epoch [136 / 300]: loss 0.2946, training auc: 0.8616, val_auc 0.7779\n",
      "2022-05-24 23:18:54,607 - NCNet pretrain, Epoch [137 / 300]: loss 0.2951, training auc: 0.8620, val_auc 0.7780\n",
      "2022-05-24 23:18:55,298 - NCNet pretrain, Epoch [138 / 300]: loss 0.2940, training auc: 0.8637, val_auc 0.7784\n",
      "2022-05-24 23:18:55,993 - NCNet pretrain, Epoch [139 / 300]: loss 0.2973, training auc: 0.8577, val_auc 0.7775\n",
      "2022-05-24 23:18:56,679 - NCNet pretrain, Epoch [140 / 300]: loss 0.2932, training auc: 0.8640, val_auc 0.7780\n",
      "2022-05-24 23:18:57,393 - NCNet pretrain, Epoch [141 / 300]: loss 0.2920, training auc: 0.8639, val_auc 0.7775\n",
      "2022-05-24 23:18:58,119 - NCNet pretrain, Epoch [142 / 300]: loss 0.2904, training auc: 0.8666, val_auc 0.7765\n",
      "2022-05-24 23:18:58,810 - NCNet pretrain, Epoch [143 / 300]: loss 0.2864, training auc: 0.8676, val_auc 0.7767\n",
      "2022-05-24 23:18:59,506 - NCNet pretrain, Epoch [144 / 300]: loss 0.2797, training auc: 0.8791, val_auc 0.7757\n",
      "2022-05-24 23:19:00,211 - NCNet pretrain, Epoch [145 / 300]: loss 0.2879, training auc: 0.8676, val_auc 0.7751\n",
      "2022-05-24 23:19:00,906 - NCNet pretrain, Epoch [146 / 300]: loss 0.2960, training auc: 0.8603, val_auc 0.7757\n",
      "2022-05-24 23:19:01,614 - NCNet pretrain, Epoch [147 / 300]: loss 0.2953, training auc: 0.8601, val_auc 0.7750\n",
      "2022-05-24 23:19:02,323 - NCNet pretrain, Epoch [148 / 300]: loss 0.2839, training auc: 0.8697, val_auc 0.7749\n",
      "2022-05-24 23:19:03,022 - NCNet pretrain, Epoch [149 / 300]: loss 0.2897, training auc: 0.8655, val_auc 0.7754\n",
      "2022-05-24 23:19:03,740 - NCNet pretrain, Epoch [150 / 300]: loss 0.2930, training auc: 0.8620, val_auc 0.7736\n",
      "2022-05-24 23:19:04,439 - NCNet pretrain, Epoch [151 / 300]: loss 0.2855, training auc: 0.8735, val_auc 0.7749\n",
      "2022-05-24 23:19:05,131 - NCNet pretrain, Epoch [152 / 300]: loss 0.2883, training auc: 0.8697, val_auc 0.7751\n",
      "2022-05-24 23:19:05,854 - NCNet pretrain, Epoch [153 / 300]: loss 0.2799, training auc: 0.8839, val_auc 0.7749\n",
      "2022-05-24 23:19:06,547 - NCNet pretrain, Epoch [154 / 300]: loss 0.2840, training auc: 0.8696, val_auc 0.7752\n",
      "2022-05-24 23:19:07,252 - NCNet pretrain, Epoch [155 / 300]: loss 0.2819, training auc: 0.8797, val_auc 0.7753\n",
      "2022-05-24 23:19:07,944 - NCNet pretrain, Epoch [156 / 300]: loss 0.2825, training auc: 0.8745, val_auc 0.7753\n",
      "2022-05-24 23:19:08,631 - NCNet pretrain, Epoch [157 / 300]: loss 0.2768, training auc: 0.8837, val_auc 0.7755\n",
      "2022-05-24 23:19:09,321 - NCNet pretrain, Epoch [158 / 300]: loss 0.2881, training auc: 0.8677, val_auc 0.7756\n",
      "2022-05-24 23:19:10,022 - NCNet pretrain, Epoch [159 / 300]: loss 0.2848, training auc: 0.8648, val_auc 0.7759\n",
      "2022-05-24 23:19:10,710 - NCNet pretrain, Epoch [160 / 300]: loss 0.2860, training auc: 0.8737, val_auc 0.7761\n",
      "2022-05-24 23:19:11,403 - NCNet pretrain, Epoch [161 / 300]: loss 0.2803, training auc: 0.8824, val_auc 0.7755\n",
      "2022-05-24 23:19:12,091 - NCNet pretrain, Epoch [162 / 300]: loss 0.2961, training auc: 0.8611, val_auc 0.7757\n",
      "2022-05-24 23:19:12,785 - NCNet pretrain, Epoch [163 / 300]: loss 0.2855, training auc: 0.8869, val_auc 0.7741\n",
      "2022-05-24 23:19:13,492 - NCNet pretrain, Epoch [164 / 300]: loss 0.2960, training auc: 0.8610, val_auc 0.7760\n",
      "2022-05-24 23:19:14,244 - NCNet pretrain, Epoch [165 / 300]: loss 0.2870, training auc: 0.8774, val_auc 0.7755\n",
      "2022-05-24 23:19:15,445 - NCNet pretrain, Epoch [166 / 300]: loss 0.2895, training auc: 0.8780, val_auc 0.7716\n",
      "2022-05-24 23:19:16,729 - NCNet pretrain, Epoch [167 / 300]: loss 0.3048, training auc: 0.8750, val_auc 0.7743\n",
      "2022-05-24 23:19:17,502 - NCNet pretrain, Epoch [168 / 300]: loss 0.2827, training auc: 0.8849, val_auc 0.7736\n",
      "2022-05-24 23:19:18,202 - NCNet pretrain, Epoch [169 / 300]: loss 0.2821, training auc: 0.8754, val_auc 0.7708\n",
      "2022-05-24 23:19:18,890 - NCNet pretrain, Epoch [170 / 300]: loss 0.2876, training auc: 0.8710, val_auc 0.7722\n",
      "2022-05-24 23:19:18,891 - Early stop!\n",
      "2022-05-24 23:19:18,892 - Best Test Results: auc 0.7980, ap 0.4272, f1 0.3768\n",
      "2022-05-24 23:20:11,368 - Eland Training, Epoch [1/10]: loss 1.5036, train_auc: 0.7993, val_auc 0.7445, test_auc 0.7824\n",
      "2022-05-24 23:21:03,993 - Eland Training, Epoch [2/10]: loss 1.1977, train_auc: 0.8160, val_auc 0.7486, test_auc 0.7870\n",
      "2022-05-24 23:21:56,363 - Eland Training, Epoch [3/10]: loss 1.3330, train_auc: 0.7985, val_auc 0.7449\n",
      "2022-05-24 23:22:49,596 - Eland Training, Epoch [4/10]: loss 1.0495, train_auc: 0.8292, val_auc 0.7409\n",
      "2022-05-24 23:23:42,174 - Eland Training, Epoch [5/10]: loss 0.9933, train_auc: 0.8221, val_auc 0.7361\n",
      "2022-05-24 23:24:34,556 - Eland Training, Epoch [6/10]: loss 1.0091, train_auc: 0.8083, val_auc 0.7336\n",
      "2022-05-24 23:25:28,896 - Eland Training, Epoch [7/10]: loss 0.9636, train_auc: 0.8129, val_auc 0.7337\n",
      "2022-05-24 23:26:21,683 - Eland Training, Epoch [8/10]: loss 0.9332, train_auc: 0.8016, val_auc 0.7364\n",
      "2022-05-24 23:27:14,856 - Eland Training, Epoch [9/10]: loss 0.8558, train_auc: 0.8131, val_auc 0.7420\n",
      "2022-05-24 23:28:07,340 - Eland Training, Epoch [10/10]: loss 0.8186, train_auc: 0.8172, val_auc 0.7427\n",
      "2022-05-24 23:28:07,341 - Best Test Results: auc 0.7870, ap 0.4339, f1 0.4540\n"
     ]
    }
   ],
   "source": [
    "if not baseline:\n",
    "    auc, ap = eland.train()\n",
    "else:\n",
    "    auc, ap = eland.pretrain_nc_net(n_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3979a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

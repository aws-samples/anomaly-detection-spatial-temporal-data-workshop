{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14736dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../../src/anomaly_detection_spatial_temporal_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16113b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import List, Union, Optional, Tuple, Dict\n",
    "\n",
    "from pathlib import Path, PosixPath\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import ncad\n",
    "from ncad.ts import TimeSeries, TimeSeriesDataset\n",
    "from ncad.ts import transforms as tr\n",
    "from ncad.model import NCAD, NCADDataModule\n",
    "\n",
    "import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fce303",
   "metadata": {},
   "source": [
    "# Load model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638d0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_file = \"../../conf/base/parameters/ncad.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d8ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_dir': 'data/07_model_output/ncad/', 'log_dir': 'data/07_model_output/ncad/logs/', 'evaluation_result_path': 'data/07_model_output/ncad/evaluation', 'exp_name': 'batadal-ncad', 'epochs': 3, 'limit_val_batches': 1.0, 'num_sanity_val_steps': 1, 'injection_method': 'local_outliers', 'ratio_injected_spikes': 0.1, 'window_length': 500, 'suspect_window_length': 10, 'num_series_in_train_batch': 1, 'num_crops_per_series': 32, 'num_workers_loader': 0, 'tcn_kernel_size': 7, 'tcn_layers': 1, 'tcn_out_channels': 16, 'tcn_maxpool_out_channels': 32, 'embedding_rep_dim': 64, 'normalize_embedding': True, 'distance': 'cosine', 'classifier_threshold': 0.5, 'threshold_grid_length_val': 0.1, 'threshold_grid_length_test': 0.05, 'coe_rate': 0.5, 'mixup_rate': 2.0, 'learning_rate': 0.0001, 'check_val_every_n_epoch': 25, 'stride_roll_pred_val_test': 5, 'val_labels_adj': True, 'test_labels_adj': True, 'max_windows_unfold_batch': 5000}\n",
      "{'model_dir': 'data/07_model_output/ncad/', 'log_dir': 'data/07_model_output/ncad/logs/', 'evaluation_result_path': 'data/07_model_output/ncad/evaluation', 'exp_name': 'batadal-ncad', 'epochs': 3, 'limit_val_batches': 1.0, 'num_sanity_val_steps': 1, 'injection_method': 'local_outliers', 'ratio_injected_spikes': 0.1, 'window_length': 500, 'suspect_window_length': 10, 'num_series_in_train_batch': 1, 'num_crops_per_series': 32, 'num_workers_loader': 0, 'tcn_kernel_size': 7, 'tcn_layers': 1, 'tcn_out_channels': 16, 'tcn_maxpool_out_channels': 32, 'embedding_rep_dim': 64, 'normalize_embedding': True, 'distance': 'cosine', 'classifier_threshold': 0.5, 'threshold_grid_length_val': 0.1, 'threshold_grid_length_test': 0.05, 'coe_rate': 0.5, 'mixup_rate': 2.0, 'learning_rate': 0.0001, 'check_val_every_n_epoch': 25, 'stride_roll_pred_val_test': 5, 'val_labels_adj': True, 'test_labels_adj': True, 'max_windows_unfold_batch': 5000}\n"
     ]
    }
   ],
   "source": [
    "with open(model_config_file, \"r\") as stream:\n",
    "    try:\n",
    "        model_config = yaml.safe_load(stream)\n",
    "        print(model_config)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ea155",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load processed CSVs into `TimeSeries` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7837c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_clean: pd.DataFrame, train_anom: pd.DataFrame, test: pd.DataFrame, sensor_cols: List[str], label_col: str) -> Tuple:\n",
    "    if type(sensor_cols) == str:\n",
    "        sensor_cols = sensor_cols.split(\"\\n\")\n",
    "    train_dataset = TimeSeriesDataset()\n",
    "    test_dataset = TimeSeriesDataset()\n",
    "    \n",
    "    \n",
    "    for sensor in sensor_cols:\n",
    "        train_dataset.append(\n",
    "            TimeSeries(\n",
    "                values=train_clean[sensor].values,\n",
    "                labels=None,\n",
    "                item_id=f\"{sensor}_train_clean\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        train_dataset.append(\n",
    "            TimeSeries(\n",
    "                values=train_anom[sensor].values,\n",
    "                labels=train_anom[label_col].values,\n",
    "                item_id=f\"{sensor}_train_anom\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        test_dataset.append(\n",
    "            TimeSeries(\n",
    "                values=test[sensor].values,\n",
    "                labels=test[label_col].values,\n",
    "                item_id=f\"{sensor}_test\"\n",
    "            )\n",
    "        )\n",
    "           \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91db871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = pd.read_csv(\"../../data/03_primary/iot/iot_ncad_train.csv\")\n",
    "train_anom = pd.read_csv(\"../../data/03_primary/iot/iot_ncad_train_anom.csv\")\n",
    "test = pd.read_csv(\"../../data/03_primary/iot/iot_ncad_test.csv\")\n",
    "\n",
    "with open(f\"../../data/03_primary/iot_sensor_list_batadal.txt\", \"r\") as f:\n",
    "    sensors = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594aabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_dataset(train_clean, train_anom, test, sensors, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e58e9",
   "metadata": {},
   "source": [
    "# Standardize data\n",
    "\n",
    "substract median, divide by interquartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801dbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(train_set: TimeSeriesDataset, test_set: TimeSeriesDataset) -> Tuple:\n",
    "    # Standardize TimeSeries values (substract median, divide by interquartile range)\n",
    "    scaler = tr.TimeSeriesScaler(type=\"robust\")\n",
    "    train_set = TimeSeriesDataset(ncad.utils.take_n_cycle(scaler(train_set), len(train_set)))\n",
    "    test_set = TimeSeriesDataset(ncad.utils.take_n_cycle(scaler(test_set), len(test_set)))\n",
    "    ts_channels = train_set[0].shape[1]\n",
    "    return train_set, test_set, ts_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067e31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_standard, test_standard, ts_channels = standardize_dataset(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75370ab3",
   "metadata": {},
   "source": [
    "# Inject anomalies\n",
    "\n",
    "NCAD is a semi-supervised algorithm, and includes functionalities to inject anomalies into the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed3b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batadal_inject_anomalies(\n",
    "    dataset: TimeSeriesDataset,\n",
    "    injection_method: str,\n",
    "    ratio_injected_spikes: float,\n",
    ") -> TimeSeriesDataset:\n",
    "\n",
    "    # without this, kedro treats the output as a list of multiple elements rather than\n",
    "    scratch = \"\"\n",
    "    # dataset is transformed using a TimeSeriesTransform depending on the type of injection\n",
    "    if injection_method == \"None\":\n",
    "        return dataset\n",
    "    elif injection_method == \"local_outliers\":\n",
    "        if ratio_injected_spikes is None:\n",
    "            # Inject synthetic anomalies: LocalOutlier\n",
    "            ts_transform = tr.LocalOutlier(area_radius=500, num_spikes=360)\n",
    "        else:\n",
    "            ts_transform = tr.LocalOutlier(\n",
    "                area_radius=700,\n",
    "                num_spikes=ratio_injected_spikes,\n",
    "                spike_multiplier_range=(1.0, 3.0),\n",
    "                # direction_options = ['increase'],\n",
    "            )\n",
    "\n",
    "        # Generate many examples of injected time series\n",
    "        multiplier = 20\n",
    "        ts_transform_iterator = ts_transform(itertools.cycle(dataset))\n",
    "        dataset_transformed = ncad.utils.take_n_cycle(\n",
    "            ts_transform_iterator, multiplier * len(dataset)\n",
    "        )\n",
    "        dataset_transformed = TimeSeriesDataset(dataset_transformed)\n",
    "    else:\n",
    "        raise ValueError(f\"injection_method = {injection_method} not supported!\")\n",
    "\n",
    "    return dataset_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac49747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 206 ms, total: 2min 45s\n",
      "Wall time: 2min 45s\n",
      "CPU times: user 2min 44s, sys: 206 ms, total: 2min 45s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_injected = batadal_inject_anomalies(\n",
    "    train_standard, \n",
    "    model_config[\"injection_method\"], \n",
    "    model_config[\"ratio_injected_spikes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c50871",
   "metadata": {},
   "source": [
    "# Split Test into val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "654bfa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_into_val(test_set: TimeSeriesDataset, window_length: int, suspect_window_length: int) -> Tuple:\n",
    "    # Split test dataset in two, half for validation and half for test\n",
    "    _, validation_set, test_set = ncad.ts.split_train_val_test(\n",
    "        data=test_set,\n",
    "        val_portion=0.5,  # No labels in training data that could serve for validation\n",
    "        test_portion=0.5,  # This dataset provides test set, so we don't need to take from training data\n",
    "        split_method=\"past_future_with_warmup\",\n",
    "        split_warmup_length=window_length - suspect_window_length,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    return validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03643c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split, test_split = split_test_into_val(\n",
    "    test_standard, \n",
    "    model_config[\"window_length\"],\n",
    "    model_config[\"suspect_window_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6074794",
   "metadata": {},
   "source": [
    "# Construct data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0382a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_data_module(\n",
    "    train_set_transformed: TimeSeriesDataset, validation_set: TimeSeriesDataset, test_set: TimeSeriesDataset,\n",
    "    window_length: int, suspect_window_length: int, \n",
    "    num_series_in_train_batch: int,\n",
    "    num_crops_per_series: int,\n",
    "    stride_roll_pred_val_test: int,\n",
    "    num_workers_loader: int\n",
    "    \n",
    ") -> NCADDataModule:\n",
    "    # Define DataModule for training with lighting (window cropping + window injection)#\n",
    "    data_module = NCADDataModule(\n",
    "        train_ts_dataset=train_set_transformed,\n",
    "        validation_ts_dataset=validation_set,\n",
    "        test_ts_dataset=test_set,\n",
    "        window_length=window_length,\n",
    "        suspect_window_length=suspect_window_length,\n",
    "        num_series_in_train_batch=num_series_in_train_batch,\n",
    "        num_crops_per_series=num_crops_per_series,\n",
    "        label_reduction_method=\"any\",\n",
    "        stride_val_and_test=stride_roll_pred_val_test,\n",
    "        num_workers=num_workers_loader,\n",
    "    )\n",
    "    \n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbc02342",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncad_data_module = construct_data_module(\n",
    "    train_injected, validation_split, test_split,\n",
    "    model_config[\"window_length\"], model_config[\"suspect_window_length\"],\n",
    "    model_config[\"num_series_in_train_batch\"],\n",
    "    model_config[\"num_crops_per_series\"],\n",
    "    model_config[\"stride_roll_pred_val_test\"],\n",
    "    model_config[\"num_workers_loader\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6c07b0",
   "metadata": {},
   "source": [
    "# Set up call backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66bef687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_callbacks(\n",
    "    ## General\n",
    "    model_dir: Union[str, PosixPath],\n",
    "    log_dir: Union[str, PosixPath],\n",
    ") -> ModelCheckpoint:\n",
    "    # Experiment name #\n",
    "    time_now = time.strftime(\"%Y-%m-%d-%H%M%S\", time.localtime())\n",
    "    exp_name = f\"batadal-{time_now}\"\n",
    "\n",
    "    ### Training the model ###\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir=log_dir, name=exp_name)\n",
    "\n",
    "    # Checkpoint callback, monitoring 'val_f1'\n",
    "    checkpoint_cb = ModelCheckpoint(\n",
    "        monitor=\"val_f1\",\n",
    "        dirpath=model_dir,\n",
    "        filename=\"ncad-model-\" + exp_name + \"-{epoch:02d}-{val_f1:.4f}\",\n",
    "        save_top_k=1,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "    \n",
    "    return checkpoint_cb, logger, exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "996f493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory data/07_model_output/ncad/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:360: UserWarning: Checkpoint directory data/07_model_output/ncad/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    }
   ],
   "source": [
    "checkpointer, tb_logger, exp_name = set_up_callbacks(\n",
    "    model_config[\"model_dir\"],\n",
    "    model_config[\"log_dir\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39f688",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8ef77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_and_train_model(\n",
    "    data_module: NCADDataModule, \n",
    "    model_dir: Union[str, PosixPath],\n",
    "    logger: TensorBoardLogger,\n",
    "    epochs: int,\n",
    "    limit_val_batches: float,\n",
    "    num_sanity_val_steps: int,\n",
    "    check_val_every_n_epoch: int,\n",
    "    checkpoint_cb: ModelCheckpoint,\n",
    "    ts_channels: int,\n",
    "    window_length: int,\n",
    "    suspect_window_length: int, \n",
    "    tcn_kernel_size: int,\n",
    "    tcn_layers: int,\n",
    "    tcn_out_channels: int,\n",
    "    tcn_maxpool_out_channels: int,\n",
    "    embedding_rep_dim: int,\n",
    "    normalize_embedding: bool,\n",
    "    # hpars for classifier\n",
    "    distance: str,\n",
    "    classifier_threshold: float,\n",
    "    threshold_grid_length_test: float,\n",
    "    # hpars for optimizer\n",
    "    learning_rate: float,\n",
    "    # hpars for anomalizers\n",
    "    coe_rate: float,\n",
    "    mixup_rate: float,\n",
    "    # hpars for validation and test\n",
    "    stride_roll_pred_val_test: int,\n",
    "    val_labels_adj: bool,\n",
    "    test_labels_adj: bool,\n",
    "    max_windows_unfold_batch: Optional[int] = 5000,\n",
    "    \n",
    "    evaluation_result_path: Optional[Union[str, PosixPath]] = None,\n",
    ") -> Tuple:\n",
    "    if distance == \"cosine\":\n",
    "        # For the contrastive approach, the cosine distance is used\n",
    "        distance = ncad.model.distances.CosineDistance()\n",
    "    elif distance == \"L2\":\n",
    "        # For the contrastive approach, the L2 distance is used\n",
    "        distance = ncad.model.distances.LpDistance(p=2)\n",
    "    elif distance == \"non-contrastive\":\n",
    "        # For the non-contrastive approach, the classifier is\n",
    "        # a neural-net based on the embedding of the whole window\n",
    "        distance = ncad.model.distances.BinaryOnX1(rep_dim=embedding_rep_dim, layers=1)\n",
    "\n",
    "    # Instantiate model #\n",
    "    model = NCAD(\n",
    "        ts_channels=ts_channels,\n",
    "        window_length=window_length,\n",
    "        suspect_window_length=suspect_window_length,\n",
    "        # hpars for encoder\n",
    "        tcn_kernel_size=tcn_kernel_size,\n",
    "        tcn_layers=tcn_layers,\n",
    "        tcn_out_channels=tcn_out_channels,\n",
    "        tcn_maxpool_out_channels=tcn_maxpool_out_channels,\n",
    "        embedding_rep_dim=embedding_rep_dim,\n",
    "        normalize_embedding=normalize_embedding,\n",
    "        # hpars for classifier\n",
    "        distance=distance,\n",
    "        classification_loss=nn.BCELoss(),\n",
    "        classifier_threshold=classifier_threshold,\n",
    "        threshold_grid_length_test=threshold_grid_length_test,\n",
    "        # hpars for anomalizers\n",
    "        coe_rate=coe_rate,\n",
    "        mixup_rate=mixup_rate,\n",
    "        # hpars for validation and test\n",
    "        stride_rolling_val_test=stride_roll_pred_val_test,\n",
    "        val_labels_adj=val_labels_adj,\n",
    "        test_labels_adj=test_labels_adj,\n",
    "        max_windows_unfold_batch=max_windows_unfold_batch,\n",
    "        # hpars for optimizer\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        default_root_dir=model_dir,\n",
    "        logger=logger,\n",
    "        min_epochs=epochs,\n",
    "        max_epochs=epochs,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        num_sanity_val_steps=1,\n",
    "        check_val_every_n_epoch=check_val_every_n_epoch,\n",
    "        callbacks=[checkpoint_cb],\n",
    "        # callbacks=[checkpoint_cb, earlystop_cb, lr_logger],\n",
    "        auto_lr_find=False,\n",
    "    )\n",
    "    \n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        datamodule=data_module,\n",
    "    )\n",
    "    \n",
    "    # Metrics on validation and test data #\n",
    "    evaluation_result = trainer.test()\n",
    "    evaluation_result = evaluation_result[0]\n",
    "    classifier_threshold = evaluation_result[\"classifier_threshold\"]\n",
    "    \n",
    "    # Save evaluation results\n",
    "    if evaluation_result_path is not None:\n",
    "        path = evaluation_result_path\n",
    "        path = PosixPath(path).expanduser() if str(path).startswith(\"~\") else Path(path)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(evaluation_result, f, cls=ncad.utils.NpEncoder)\n",
    "\n",
    "    for key, value in evaluation_result.items():\n",
    "        # if key.startswith('test_'):\n",
    "        print(f\"{key}={value}\")\n",
    "        \n",
    "    return model_dir, classifier_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd6eef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Metric` was deprecated since v1.3.0 in favor of `torchmetrics.metric.Metric`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/src/anomaly_detection_spatial_temporal_data/model/ncad/src/ncad/utils/pl_metrics.py:29: UserWarning: Metric `CachePredictions` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  \"Metric `CachePredictions` will save all targets and predictions in buffer.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Metric` was deprecated since v1.3.0 in favor of `torchmetrics.metric.Metric`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/src/anomaly_detection_spatial_temporal_data/model/ncad/src/ncad/utils/pl_metrics.py:29: UserWarning: Metric `CachePredictions` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  \"Metric `CachePredictions` will save all targets and predictions in buffer.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                 | Params\n",
      "-------------------------------------------------------------\n",
      "0 | encoder             | TCNEncoder           | 38.5 K\n",
      "1 | classifier          | ContrastiveClasifier | 0     \n",
      "2 | classification_loss | BCELoss              | 0     \n",
      "3 | val_metrics         | ModuleDict           | 0     \n",
      "4 | test_metrics        | ModuleDict           | 0     \n",
      "-------------------------------------------------------------\n",
      "38.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "38.5 K    Total params\n",
      "0.154     Total estimated model params size (MB)\n",
      "\n",
      "  | Name                | Type                 | Params\n",
      "-------------------------------------------------------------\n",
      "0 | encoder             | TCNEncoder           | 38.5 K\n",
      "1 | classifier          | ContrastiveClasifier | 0     \n",
      "2 | classification_loss | BCELoss              | 0     \n",
      "3 | val_metrics         | ModuleDict           | 0     \n",
      "4 | test_metrics        | ModuleDict           | 0     \n",
      "-------------------------------------------------------------\n",
      "38.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "38.5 K    Total params\n",
      "0.154     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 100%|██████████| 1/1 [00:00<00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1720/1720 [01:01<00:00, 27.96it/s, loss=2.39, v_num=0, classifier_threshold=0.000, val_f1=0.375, train_loss_step=0.654] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  23%|██▎       | 10/43 [00:00<00:00, 94.36it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/kedro-ncad-venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:103: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f'The dataloader, {name}, does not have many workers which may be a bottleneck.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 43/43 [00:04<00:00,  9.03it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'classifier_threshold': 0.6,\n",
      " 'test_FN': 5260,\n",
      " 'test_FP': 5455,\n",
      " 'test_TN': 30450,\n",
      " 'test_TP': 3770,\n",
      " 'test_f0.5': 0.4104071413019813,\n",
      " 'test_f1': 0.4130375239660367,\n",
      " 'test_f2': 0.4157018414378652,\n",
      " 'test_precision': 0.4086720867208672,\n",
      " 'test_recall': 0.4174972314507198}\n",
      "--------------------------------------------------------------------------------\n",
      "classifier_threshold=0.6\n",
      "test_f1=0.4130375239660367\n",
      "test_f2=0.4157018414378652\n",
      "test_f0.5=0.4104071413019813\n",
      "test_precision=0.4086720867208672\n",
      "test_recall=0.4174972314507198\n",
      "test_TN=30450\n",
      "test_FN=5260\n",
      "test_TP=3770\n",
      "test_FP=5455\n",
      "Testing: 100%|██████████| 43/43 [00:04<00:00,  9.03it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'classifier_threshold': 0.6,\n",
      " 'test_FN': 5260,\n",
      " 'test_FP': 5455,\n",
      " 'test_TN': 30450,\n",
      " 'test_TP': 3770,\n",
      " 'test_f0.5': 0.4104071413019813,\n",
      " 'test_f1': 0.4130375239660367,\n",
      " 'test_f2': 0.4157018414378652,\n",
      " 'test_precision': 0.4086720867208672,\n",
      " 'test_recall': 0.4174972314507198}\n",
      "--------------------------------------------------------------------------------\n",
      "classifier_threshold=0.6\n",
      "test_f1=0.4130375239660367\n",
      "test_f2=0.4157018414378652\n",
      "test_f0.5=0.4104071413019813\n",
      "test_precision=0.4086720867208672\n",
      "test_recall=0.4174972314507198\n",
      "test_TN=30450\n",
      "test_FN=5260\n",
      "test_TP=3770\n",
      "test_FP=5455\n"
     ]
    }
   ],
   "source": [
    "_, classifier_threshold = set_and_train_model(\n",
    "    ncad_data_module, \n",
    "    model_config[\"model_dir\"],\n",
    "    tb_logger,\n",
    "    model_config[\"epochs\"],\n",
    "    model_config[\"limit_val_batches\"],\n",
    "    model_config[\"num_sanity_val_steps\"],\n",
    "    model_config[\"check_val_every_n_epoch\"],\n",
    "    checkpointer,\n",
    "    ts_channels,\n",
    "    model_config[\"window_length\"],\n",
    "    model_config[\"suspect_window_length\"], \n",
    "    model_config[\"tcn_kernel_size\"],\n",
    "    model_config[\"tcn_layers\"],\n",
    "    model_config[\"tcn_out_channels\"],\n",
    "    model_config[\"tcn_maxpool_out_channels\"],\n",
    "    model_config[\"embedding_rep_dim\"],\n",
    "    model_config[\"normalize_embedding\"],\n",
    "    # hpars for classifier\n",
    "    model_config[\"distance\"],\n",
    "    model_config[\"classifier_threshold\"],\n",
    "    model_config[\"threshold_grid_length_test\"],\n",
    "    # hpars for optimizer\n",
    "    model_config[\"learning_rate\"],\n",
    "    # hpars for anomalizers\n",
    "    model_config[\"coe_rate\"],\n",
    "    model_config[\"mixup_rate\"],\n",
    "    # hpars for validation and test\n",
    "    model_config[\"stride_roll_pred_val_test\"],\n",
    "    model_config[\"val_labels_adj\"],\n",
    "    model_config[\"test_labels_adj\"],\n",
    "    model_config[\"max_windows_unfold_batch\"],\n",
    "    \n",
    "    model_config[\"evaluation_result_path\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5486b6",
   "metadata": {},
   "source": [
    "# Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "043efd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate(\n",
    "    model_dir: Union[str, PosixPath],\n",
    "    exp_name: str,\n",
    "    data_set: TimeSeriesDataset,\n",
    "    stride: int,\n",
    "    classifier_threshold: float\n",
    "):\n",
    "    model_dir = PosixPath(model_dir).expanduser() if str(model_dir).startswith(\"~\") else Path(model_dir)\n",
    "    \n",
    "    # Load top performing checkpoint\n",
    "    # ckpt_path = [x for x in model_dir.glob('*.ckpt')][-1]\n",
    "    ckpt_file = [\n",
    "        file\n",
    "        for file in os.listdir(model_dir)\n",
    "        if (file.endswith(\".ckpt\") and file.startswith(\"ncad-model-\" + exp_name))\n",
    "    ]\n",
    "\n",
    "    ckpt_file = ckpt_file[-1]\n",
    "    ckpt_path = model_dir / ckpt_file\n",
    "    model = NCAD.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    anomaly_probs_avg, anomaly_vote = model.tsdetect(data_set, stride, classifier_threshold)\n",
    "    \n",
    "    print(f\"NCAD on batadal dataset finished successfully!\")    \n",
    "    \n",
    "    return anomaly_probs_avg, anomaly_vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "184a73e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/07_model_output/ncad 1 ['ncad-model-batadal-2022-07-13-214807-epoch=02-val_f1=0.3752.ckpt']\n",
      "data/07_model_output/ncad 1 ['ncad-model-batadal-2022-07-13-214807-epoch=02-val_f1=0.3752.ckpt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/src/anomaly_detection_spatial_temporal_data/model/ncad/src/ncad/utils/pl_metrics.py:29: UserWarning: Metric `CachePredictions` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  \"Metric `CachePredictions` will save all targets and predictions in buffer.\"\n",
      "/home/ec2-user/SageMaker/repos/anomaly-detection-spatial-temporal-data/src/anomaly_detection_spatial_temporal_data/model/ncad/src/ncad/utils/pl_metrics.py:29: UserWarning: Metric `CachePredictions` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  \"Metric `CachePredictions` will save all targets and predictions in buffer.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 2089) (43, 2089)\n",
      "NCAD on batadal dataset finished successfully!\n",
      "(43, 2089) (43, 2089)\n",
      "NCAD on batadal dataset finished successfully!\n"
     ]
    }
   ],
   "source": [
    "anomaly_probs_avg, anomaly_vote = load_and_evaluate(\n",
    "    model_config[\"model_dir\"],\n",
    "    exp_name,\n",
    "    test_standard,\n",
    "    model_config[\"stride_roll_pred_val_test\"],\n",
    "    classifier_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f7edbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43, 2089) (43, 2089)\n",
      "(43, 2089) (43, 2089)\n"
     ]
    }
   ],
   "source": [
    "print(anomaly_probs_avg.shape, anomaly_vote.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908a43c",
   "metadata": {},
   "source": [
    "# References\n",
    "Riccardo Taormina and Stefano Galelli and Nils Ole Tippenhauer and Elad Salomons and Avi Ostfeld and Demetrios G. Eliades and Mohsen Aghashahi and Raanju Sundararajan and Mohsen Pourahmadi and M. Katherine Banks and B. M. Brentan and Enrique Campbell and G. Lima and D. Manzi and D. Ayala-Cabrera and M. Herrera and I. Montalvo and J. Izquierdo and E. Luvizotto and Sarin E. Chandy and Amin Rasekh and Zachary A. Barker and Bruce Campbell and M. Ehsan Shafiee and Marcio Giacomoni and Nikolaos Gatsis and Ahmad Taha and Ahmed A. Abokifa and Kelsey Haddad and Cynthia S. Lo and Pratim Biswas and M. Fayzul K. Pasha and Bijay Kc and Saravanakumar Lakshmanan Somasundaram and Mashor Housh and Ziv Ohar; \"The Battle Of The Attack Detection Algorithms: Disclosing Cyber Attacks On Water Distribution Networks.\" Journal of Water Resources Planning and Management, 144 (8), August 2018\n",
    "\n",
    "Chris U. Carmona, François-Xavier Aubet, Valentin Flunkert, and Jan Gasthaus. 2021. Neural Contextual Anomaly Detection for Time Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0ad1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-ncad-venv",
   "language": "python",
   "name": "kedro-ncad-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fda3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5462b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_anom = pd.read_csv(\"../../data/01_raw/BATADAL_dataset03_train_no_anomaly.csv\")\n",
    "train_some_anom = pd.read_csv(\"../../data/01_raw/BATADAL_dataset04_train_some_anomaly.csv\")\n",
    "test_with_anom = pd.read_csv(\"../../data/01_raw/BATADAL_test_dataset_some_anomaly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d150875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# has leading white space\n",
    "train_some_anom.columns = train_some_anom.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cb92d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8761, 45), (4177, 45), (2089, 44))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_anom.shape, train_some_anom.shape, test_with_anom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0c00ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "0 1 0\n",
      "0 2 1\n",
      "\t {'ATT_FLAG'}\n",
      "1 0 0\n",
      "1 1 0\n",
      "1 2 1\n",
      "\t {'ATT_FLAG'}\n",
      "2 0 0\n",
      "2 1 0\n",
      "2 2 0\n"
     ]
    }
   ],
   "source": [
    "for i, _df in enumerate([train_no_anom, train_some_anom, test_with_anom]):\n",
    "    for j, _df2 in enumerate([train_no_anom, train_some_anom, test_with_anom]):\n",
    "        set1 = set(_df.columns.tolist())\n",
    "        set2 = set(_df2.columns.tolist())\n",
    "        \n",
    "        print(i, j, len(set1 - set2))\n",
    "        if len(set1 - set2) != 0:\n",
    "            print(\"\\t\", set1-set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5541fb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DATETIME', 'L_T1', 'L_T2', 'L_T3', 'L_T4', 'L_T5', 'L_T6', 'L_T7',\n",
       "       'F_PU1', 'S_PU1', 'F_PU2', 'S_PU2', 'F_PU3', 'S_PU3', 'F_PU4', 'S_PU4',\n",
       "       'F_PU5', 'S_PU5', 'F_PU6', 'S_PU6', 'F_PU7', 'S_PU7', 'F_PU8', 'S_PU8',\n",
       "       'F_PU9', 'S_PU9', 'F_PU10', 'S_PU10', 'F_PU11', 'S_PU11', 'F_V2',\n",
       "       'S_V2', 'P_J280', 'P_J269', 'P_J300', 'P_J256', 'P_J289', 'P_J415',\n",
       "       'P_J302', 'P_J306', 'P_J307', 'P_J317', 'P_J14', 'P_J422', 'ATT_FLAG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_anom.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d6cf1",
   "metadata": {},
   "source": [
    "# NAB\n",
    "NAB expects a directly containing CSVs with `timestamp, value` as the columns. Therefore we will split each CSV into multiple CSVs as required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc66f47",
   "metadata": {},
   "source": [
    "# Train no anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6fd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bae0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tna_dataset_name = \"train_no_anomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69aadda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../data/03_primary/{tna_dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d33c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../results\n",
    "# !mkdir ../results/NAB\n",
    "# !mkdir ../results/NAB/{tna_dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86bf7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"../../data/03_primary/{tna_dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39951d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_anom[\"timestamp\"] = pd.to_datetime(train_no_anom[\"DATETIME\"], format=\"%d/%m/%y %H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3f7f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSOR_COLS = [c for c in train_no_anom.columns if c not in [\"DATETIME\", \"ATT_FLAG\", \"timestamp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b322ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in SENSOR_COLS:\n",
    "    train_no_anom[[\"timestamp\", c]].rename(columns={c:\"value\"}).to_csv(f\"{output_dir}/{c}.csv\", index=False)\n",
    "    anomaly_dict[f\"{tna_dataset_name}/{c}.csv\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608d9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cae18adb",
   "metadata": {},
   "source": [
    "# Train with anomaly to use as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a397a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "twa_dataset_name = \"train_with_anomaly\"\n",
    "twa_output_dir = f\"../../data/03_primary/{twa_dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e3639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../data/03_primary/{twa_dataset_name}\n",
    "# !mkdir ../results/NAB/{twa_dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26834fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://www.batadal.net/images/Attacks_TrainingDataset2.png\n",
    "fmt =\"%d/%m/%Y %H\"\n",
    "train_anomalies = [\n",
    "    (\"13/09/2016 23\", \"16/09/2016 00\"),\n",
    "    (\"26/09/2016 11\", \"27/09/2016 10\"),\n",
    "    (\"09/10/2016 09\", \"11/10/2016 20\"),\n",
    "    (\"29/10/2016 19\", \"02/11/2016 16\"),\n",
    "    (\"26/11/2016 17\", \"29/11/2016 04\"),\n",
    "    (\"06/12/2016 07\", \"10/12/2016 04\"),\n",
    "    (\"14/12/2016 15\", \"19/12/2016 04\")\n",
    "]\n",
    "\n",
    "train_anomalies_dt = [\n",
    "    (pd.to_datetime(s, format=fmt), pd.to_datetime(e, format=fmt)) for s, e in train_anomalies\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dd9bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_anomalies_dt[0][0].strftime('%Y-%m-%d %H:%M:%S.%f')#[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def8ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_some_anom[\"timestamp\"] = pd.to_datetime(train_some_anom[\"DATETIME\"], format=\"%d/%m/%y %H\")\n",
    "train_some_anom = train_some_anom.set_index([\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1f456ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_some_anom[\"attack\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a661ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for start, end in train_anomalies_dt:\n",
    "    train_some_anom.loc[start:end, \"attack\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e7db7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3685\n",
       "1     492\n",
       "Name: attack, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_some_anom[\"attack\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d5253da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in SENSOR_COLS:\n",
    "    train_some_anom.reset_index()[[\"timestamp\", c]].rename(\n",
    "        columns={c:\"value\"}\n",
    "    ).to_csv(f\"{twa_output_dir}/{c}.csv\", index=False)\n",
    "    \n",
    "    for s_anom, e_anom in train_anomalies_dt:\n",
    "        anomaly_dict[f\"{twa_dataset_name}/{c}.csv\"].append([\n",
    "            s_anom.strftime('%Y-%m-%d %H:%M:%S.%f'), e_anom.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629dcf8",
   "metadata": {},
   "source": [
    "# Actual test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d301080",
   "metadata": {},
   "outputs": [],
   "source": [
    "testwa_dataset_name = \"test_with_anomaly\"\n",
    "testwa_output_dir = f\"../../data/03_primary/{testwa_dataset_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "383dce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir ../data/03_primary/{testwa_dataset_name}\n",
    "# !mkdir ../results/NAB/{testwa_dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "211aea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.batadal.net/images/Attacks_TestDataset.png\n",
    "test_anomalies = [\n",
    "    (\"16/01/2017 09\", \"19/01/2017 06\"),\n",
    "    (\"30/01/2017 08\", \"02/02/2017 00\"),\n",
    "    (\"09/02/2017 03\", \"10/02/2017 09\"),\n",
    "    (\"12/02/2017 01\", \"13/02/2017 07\"),\n",
    "    (\"24/02/2017 05\", \"28/02/2017 08\"),\n",
    "    (\"10/03/2017 14\", \"13/03/2017 21\"),\n",
    "    (\"25/03/2017 20\", \"27/03/2017 01\")\n",
    "]\n",
    "\n",
    "test_anomalies_dt = [\n",
    "    (pd.to_datetime(s, format=fmt), pd.to_datetime(e, format=fmt)) for s, e in test_anomalies\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33eaa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_anom[\"timestamp\"] = pd.to_datetime(test_with_anom[\"DATETIME\"], format=\"%d/%m/%y %H\")\n",
    "test_with_anom = test_with_anom.set_index([\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "635ff37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_anom[\"attack\"] = 0\n",
    "for start, end in test_anomalies_dt:\n",
    "    test_with_anom.loc[start:end, \"attack\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be57979e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1682\n",
       "1     407\n",
       "Name: attack, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_with_anom[\"attack\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6783d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in SENSOR_COLS:\n",
    "    test_with_anom.reset_index()[[\"timestamp\", c]].rename(\n",
    "        columns={c:\"value\"}\n",
    "    ).to_csv(f\"{testwa_output_dir}/{c}.csv\", index=False)\n",
    "    \n",
    "    for s_anom, e_anom in test_anomalies_dt:\n",
    "        anomaly_dict[f\"{testwa_dataset_name}/{c}.csv\"].append([\n",
    "            s_anom.strftime('%Y-%m-%d %H:%M:%S.%f'), e_anom.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d7155",
   "metadata": {},
   "source": [
    "### Save labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "814e7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4c4d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dumps(anomaly_dict)\n",
    "with open(\"../../data/03_primary/labels-window.json\", 'w') as fp:\n",
    "    json.dump(anomaly_dict, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7551591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_keys = [k for k in anomaly_dict if \"train_with_anomaly/F_\" in k]\n",
    "debug_labels = {key: anomaly_dict[key] for key in debug_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e23b627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/03_primary/labels-window-debug.json\", 'w') as fp:\n",
    "    json.dump(debug_labels, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9649d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ../../data/03_primary/labels-window-debug.json /home/ec2-user/SageMaker/NAB/labels/batadal-debug-labels.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cbc13b",
   "metadata": {},
   "source": [
    "# Version 2: combined\n",
    "\n",
    "As NAB finds anomalies at individual time series level by using previous time steps to predict future time steps, it is also worth experimenting with concatenating the three sets into one large set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "636433ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_anom[\"timestamp\"] = pd.to_datetime(train_no_anom[\"DATETIME\"], format=\"%d/%m/%y %H\")\n",
    "train_no_anom = train_no_anom.set_index([\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31650ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([\n",
    "    train_no_anom.drop(\"ATT_FLAG\", axis=1),\n",
    "    train_some_anom.drop(\"ATT_FLAG\", axis=1),\n",
    "    test_with_anom\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb367462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    14128\n",
       "1.0      899\n",
       "Name: attack, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[\"attack\"] = combined[\"attack\"].fillna(0)\n",
    "combined[\"attack\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66db4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_anomalies = train_anomalies_dt + test_anomalies_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93e33468",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_output_dir = \"/home/ec2-user/SageMaker/NAB/data-batadal/combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cea2fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in SENSOR_COLS:\n",
    "    combined.reset_index()[[\"timestamp\", c]].rename(\n",
    "        columns={c:\"value\"}\n",
    "    ).to_csv(f\"{combined_output_dir}/{c}.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    for s_anom, e_anom in combined_anomalies:\n",
    "        anomaly_dict[f\"combined/{c}.csv\"].append([\n",
    "            s_anom.strftime('%Y-%m-%d %H:%M:%S.%f'), e_anom.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24209600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_no_anomaly/L_T1.csv', 'train_no_anomaly/L_T2.csv', 'train_no_anomaly/L_T3.csv', 'train_no_anomaly/L_T4.csv', 'train_no_anomaly/L_T5.csv', 'train_no_anomaly/L_T6.csv', 'train_no_anomaly/L_T7.csv', 'train_no_anomaly/F_PU1.csv', 'train_no_anomaly/S_PU1.csv', 'train_no_anomaly/F_PU2.csv', 'train_no_anomaly/S_PU2.csv', 'train_no_anomaly/F_PU3.csv', 'train_no_anomaly/S_PU3.csv', 'train_no_anomaly/F_PU4.csv', 'train_no_anomaly/S_PU4.csv', 'train_no_anomaly/F_PU5.csv', 'train_no_anomaly/S_PU5.csv', 'train_no_anomaly/F_PU6.csv', 'train_no_anomaly/S_PU6.csv', 'train_no_anomaly/F_PU7.csv', 'train_no_anomaly/S_PU7.csv', 'train_no_anomaly/F_PU8.csv', 'train_no_anomaly/S_PU8.csv', 'train_no_anomaly/F_PU9.csv', 'train_no_anomaly/S_PU9.csv', 'train_no_anomaly/F_PU10.csv', 'train_no_anomaly/S_PU10.csv', 'train_no_anomaly/F_PU11.csv', 'train_no_anomaly/S_PU11.csv', 'train_no_anomaly/F_V2.csv', 'train_no_anomaly/S_V2.csv', 'train_no_anomaly/P_J280.csv', 'train_no_anomaly/P_J269.csv', 'train_no_anomaly/P_J300.csv', 'train_no_anomaly/P_J256.csv', 'train_no_anomaly/P_J289.csv', 'train_no_anomaly/P_J415.csv', 'train_no_anomaly/P_J302.csv', 'train_no_anomaly/P_J306.csv', 'train_no_anomaly/P_J307.csv', 'train_no_anomaly/P_J317.csv', 'train_no_anomaly/P_J14.csv', 'train_no_anomaly/P_J422.csv', 'train_with_anomaly/L_T1.csv', 'train_with_anomaly/L_T2.csv', 'train_with_anomaly/L_T3.csv', 'train_with_anomaly/L_T4.csv', 'train_with_anomaly/L_T5.csv', 'train_with_anomaly/L_T6.csv', 'train_with_anomaly/L_T7.csv', 'train_with_anomaly/F_PU1.csv', 'train_with_anomaly/S_PU1.csv', 'train_with_anomaly/F_PU2.csv', 'train_with_anomaly/S_PU2.csv', 'train_with_anomaly/F_PU3.csv', 'train_with_anomaly/S_PU3.csv', 'train_with_anomaly/F_PU4.csv', 'train_with_anomaly/S_PU4.csv', 'train_with_anomaly/F_PU5.csv', 'train_with_anomaly/S_PU5.csv', 'train_with_anomaly/F_PU6.csv', 'train_with_anomaly/S_PU6.csv', 'train_with_anomaly/F_PU7.csv', 'train_with_anomaly/S_PU7.csv', 'train_with_anomaly/F_PU8.csv', 'train_with_anomaly/S_PU8.csv', 'train_with_anomaly/F_PU9.csv', 'train_with_anomaly/S_PU9.csv', 'train_with_anomaly/F_PU10.csv', 'train_with_anomaly/S_PU10.csv', 'train_with_anomaly/F_PU11.csv', 'train_with_anomaly/S_PU11.csv', 'train_with_anomaly/F_V2.csv', 'train_with_anomaly/S_V2.csv', 'train_with_anomaly/P_J280.csv', 'train_with_anomaly/P_J269.csv', 'train_with_anomaly/P_J300.csv', 'train_with_anomaly/P_J256.csv', 'train_with_anomaly/P_J289.csv', 'train_with_anomaly/P_J415.csv', 'train_with_anomaly/P_J302.csv', 'train_with_anomaly/P_J306.csv', 'train_with_anomaly/P_J307.csv', 'train_with_anomaly/P_J317.csv', 'train_with_anomaly/P_J14.csv', 'train_with_anomaly/P_J422.csv', 'test_with_anomaly/L_T1.csv', 'test_with_anomaly/L_T2.csv', 'test_with_anomaly/L_T3.csv', 'test_with_anomaly/L_T4.csv', 'test_with_anomaly/L_T5.csv', 'test_with_anomaly/L_T6.csv', 'test_with_anomaly/L_T7.csv', 'test_with_anomaly/F_PU1.csv', 'test_with_anomaly/S_PU1.csv', 'test_with_anomaly/F_PU2.csv', 'test_with_anomaly/S_PU2.csv', 'test_with_anomaly/F_PU3.csv', 'test_with_anomaly/S_PU3.csv', 'test_with_anomaly/F_PU4.csv', 'test_with_anomaly/S_PU4.csv', 'test_with_anomaly/F_PU5.csv', 'test_with_anomaly/S_PU5.csv', 'test_with_anomaly/F_PU6.csv', 'test_with_anomaly/S_PU6.csv', 'test_with_anomaly/F_PU7.csv', 'test_with_anomaly/S_PU7.csv', 'test_with_anomaly/F_PU8.csv', 'test_with_anomaly/S_PU8.csv', 'test_with_anomaly/F_PU9.csv', 'test_with_anomaly/S_PU9.csv', 'test_with_anomaly/F_PU10.csv', 'test_with_anomaly/S_PU10.csv', 'test_with_anomaly/F_PU11.csv', 'test_with_anomaly/S_PU11.csv', 'test_with_anomaly/F_V2.csv', 'test_with_anomaly/S_V2.csv', 'test_with_anomaly/P_J280.csv', 'test_with_anomaly/P_J269.csv', 'test_with_anomaly/P_J300.csv', 'test_with_anomaly/P_J256.csv', 'test_with_anomaly/P_J289.csv', 'test_with_anomaly/P_J415.csv', 'test_with_anomaly/P_J302.csv', 'test_with_anomaly/P_J306.csv', 'test_with_anomaly/P_J307.csv', 'test_with_anomaly/P_J317.csv', 'test_with_anomaly/P_J14.csv', 'test_with_anomaly/P_J422.csv', 'combined/L_T1.csv', 'combined/L_T2.csv', 'combined/L_T3.csv', 'combined/L_T4.csv', 'combined/L_T5.csv', 'combined/L_T6.csv', 'combined/L_T7.csv', 'combined/F_PU1.csv', 'combined/S_PU1.csv', 'combined/F_PU2.csv', 'combined/S_PU2.csv', 'combined/F_PU3.csv', 'combined/S_PU3.csv', 'combined/F_PU4.csv', 'combined/S_PU4.csv', 'combined/F_PU5.csv', 'combined/S_PU5.csv', 'combined/F_PU6.csv', 'combined/S_PU6.csv', 'combined/F_PU7.csv', 'combined/S_PU7.csv', 'combined/F_PU8.csv', 'combined/S_PU8.csv', 'combined/F_PU9.csv', 'combined/S_PU9.csv', 'combined/F_PU10.csv', 'combined/S_PU10.csv', 'combined/F_PU11.csv', 'combined/S_PU11.csv', 'combined/F_V2.csv', 'combined/S_V2.csv', 'combined/P_J280.csv', 'combined/P_J269.csv', 'combined/P_J300.csv', 'combined/P_J256.csv', 'combined/P_J289.csv', 'combined/P_J415.csv', 'combined/P_J302.csv', 'combined/P_J306.csv', 'combined/P_J307.csv', 'combined/P_J317.csv', 'combined/P_J14.csv', 'combined/P_J422.csv'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7a0a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ec2-user/SageMaker/NAB/labels/batadal-labels-combined.json\", 'w') as fp:\n",
    "    json.dump(anomaly_dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd1ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

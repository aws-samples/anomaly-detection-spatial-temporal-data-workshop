{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f3b488",
   "metadata": {},
   "source": [
    "# This notebook is prepared to show eland model training results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e3fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import pickle as pk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, CosineEmbeddingLoss\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fc732a",
   "metadata": {},
   "source": [
    "## loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62667a20",
   "metadata": {},
   "source": [
    "### user label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2aab1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_label = pd.read_csv(\"../../data/02_intermediate/user_behavior/user_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a095fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultimatt42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jonknee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dons</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jedravent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burtonmkz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sblinn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WebZen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doodahdei</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tack122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author  label\n",
       "0    ultimatt42      0\n",
       "1       jonknee      0\n",
       "2          dons      0\n",
       "3     Jedravent      0\n",
       "4     burtonmkz      0\n",
       "5  pavel_lishin      0\n",
       "6        sblinn      0\n",
       "7        WebZen      0\n",
       "8     doodahdei      0\n",
       "9       Tack122      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_label.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a2e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "533b7264",
   "metadata": {},
   "source": [
    "## user and subreddit topic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b524cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/u2index.pkl\",\"rb\") as f:\n",
    "    u2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb549187",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_o': 0,\n",
       " '138': 1,\n",
       " '13ren': 2,\n",
       " '1812overture': 3,\n",
       " '1esproc': 4,\n",
       " '315was_an_inside_job': 5,\n",
       " '43P04T34': 6,\n",
       " '7oby': 7,\n",
       " 'AAjax': 8,\n",
       " 'ABabyAteMyDingo': 9,\n",
       " 'ANSICL': 10,\n",
       " 'AbouBenAdhem': 11,\n",
       " 'Aerik': 12,\n",
       " 'Ajenthavoc': 13,\n",
       " 'AliasHandler': 14,\n",
       " 'AmericanGoyBlog': 15,\n",
       " 'AngelaMotorman': 16,\n",
       " 'AngledLuffa': 17,\n",
       " 'Anonymous7777': 18,\n",
       " 'AnteChronos': 19,\n",
       " 'ApostrophePosse': 20,\n",
       " 'ArcticCelt': 21,\n",
       " 'Bagel': 22,\n",
       " 'Battleloser': 23,\n",
       " 'BedtimeForSheeple': 24,\n",
       " 'BeetleB': 25,\n",
       " 'Benny_Lava': 26,\n",
       " 'Bensch': 27,\n",
       " 'Bixie': 28,\n",
       " 'Bloodlustt': 29,\n",
       " 'Bloody_Eye': 30,\n",
       " 'BlueBeard': 31,\n",
       " 'BobGaffney': 32,\n",
       " 'BraveSirRobin': 33,\n",
       " 'BrianBoyko': 34,\n",
       " 'Browzer': 35,\n",
       " 'Burlapin': 36,\n",
       " 'Busybyeski': 37,\n",
       " 'CampusTour': 38,\n",
       " 'CannedMango': 39,\n",
       " 'Captain-Obliviouss': 40,\n",
       " 'Chirp08': 41,\n",
       " 'ChunkyLaFunga': 42,\n",
       " 'Ciserus': 43,\n",
       " 'Clothos': 44,\n",
       " 'CodeMonkey1': 45,\n",
       " 'Codebender': 46,\n",
       " 'ColdSnickersBar': 47,\n",
       " 'Cookie': 48,\n",
       " 'CrackIsGoodForYou': 49,\n",
       " 'CrimsonSun99': 50,\n",
       " 'D-Style': 51,\n",
       " 'DCGaymer': 52,\n",
       " 'DOGA': 53,\n",
       " 'DaDibbel': 54,\n",
       " 'Dafuzz': 55,\n",
       " 'Dallas442': 56,\n",
       " 'Dark-Dx': 57,\n",
       " 'DarkSideofOZ': 58,\n",
       " 'Darkmeerkat': 59,\n",
       " 'Dax420': 60,\n",
       " 'Deacon': 61,\n",
       " 'Deestan': 62,\n",
       " 'Dildozer': 63,\n",
       " 'Doeke': 64,\n",
       " 'Doomdoomkittydoom': 65,\n",
       " 'Doomed': 66,\n",
       " 'DoorFrame': 67,\n",
       " 'Dr-No': 68,\n",
       " 'Drevor': 69,\n",
       " 'DudeAsInCool': 70,\n",
       " 'Dzazter': 71,\n",
       " 'EFG': 72,\n",
       " 'Eijin': 73,\n",
       " 'El_Guapo': 74,\n",
       " 'EndymionAwake': 75,\n",
       " 'Entropy': 76,\n",
       " 'Erudecorp': 77,\n",
       " 'Etab': 78,\n",
       " 'EvilPigeon': 79,\n",
       " 'ExplodingBob': 80,\n",
       " 'FANGO': 81,\n",
       " 'FMERCURY': 82,\n",
       " 'Farsay': 83,\n",
       " 'FeedMePlease': 84,\n",
       " 'FenPhen': 85,\n",
       " 'Fidodo': 86,\n",
       " 'Filmore': 87,\n",
       " 'FionaSarah': 88,\n",
       " 'FlySwat': 89,\n",
       " 'Flyen': 90,\n",
       " 'FrancisC': 91,\n",
       " 'Fulltangviper': 92,\n",
       " 'G_Morgan': 93,\n",
       " 'Gargilius': 94,\n",
       " 'GeorgeWBush': 95,\n",
       " 'GetToTheKarateChoppa': 96,\n",
       " 'Grimalkin': 97,\n",
       " 'Grue': 98,\n",
       " 'GrumpySimon': 99,\n",
       " 'GunnerMcGrath': 100,\n",
       " 'Guybrush_Threepwood': 101,\n",
       " 'HardwareLust': 102,\n",
       " 'Haroshia': 103,\n",
       " 'Haven': 104,\n",
       " 'HerbertMcSherbert': 105,\n",
       " 'Hubso': 106,\n",
       " 'HumanSockPuppet': 107,\n",
       " 'HunterTV': 108,\n",
       " 'IConrad': 109,\n",
       " 'I_AM_A_NEOCON': 110,\n",
       " 'Icanhazreddit': 111,\n",
       " 'Indyhouse': 112,\n",
       " 'InkyChan': 113,\n",
       " 'Ioewe': 114,\n",
       " 'James_Johnson': 115,\n",
       " 'JarvisCocker': 116,\n",
       " 'JasonDJ': 117,\n",
       " 'Jedravent': 118,\n",
       " 'JimDabell': 119,\n",
       " 'Jimmy': 120,\n",
       " 'Jivlain': 121,\n",
       " 'Johny_Cash': 122,\n",
       " 'JulianMorrison': 123,\n",
       " 'Kardlonoc': 124,\n",
       " 'KazamaSmokers': 125,\n",
       " 'Kestral': 126,\n",
       " 'Klowner': 127,\n",
       " 'LeviDon': 128,\n",
       " 'Lizard': 129,\n",
       " 'Lukifer': 130,\n",
       " 'Lystrodom': 131,\n",
       " 'MadScientist420': 132,\n",
       " 'MarkByers': 133,\n",
       " 'MarlonBain': 134,\n",
       " 'MarshallBanana': 135,\n",
       " 'MaximumBob': 136,\n",
       " 'McGuirk': 137,\n",
       " 'Midwest_Product': 138,\n",
       " 'MisterEggs': 139,\n",
       " 'MrFlesh': 140,\n",
       " 'MrKlaatu': 141,\n",
       " 'Mr_Smartypants': 142,\n",
       " 'MyaloMark': 143,\n",
       " 'Mythrilfan': 144,\n",
       " 'NSMike': 145,\n",
       " 'NadsatBrat': 146,\n",
       " 'NancyGracesTesticles': 147,\n",
       " 'NastyConde': 148,\n",
       " 'Nate_W': 149,\n",
       " 'Nefelia': 150,\n",
       " 'NoControl': 151,\n",
       " 'NoMoreNicksLeft': 152,\n",
       " 'NoSalt': 153,\n",
       " 'Notmyrealname': 154,\n",
       " 'OMouse': 155,\n",
       " 'Oak': 156,\n",
       " 'OlympicPirate': 157,\n",
       " 'Orangutan': 158,\n",
       " 'OriginalSyn': 159,\n",
       " 'Oryx': 160,\n",
       " 'Osmanthus': 161,\n",
       " 'Othello': 162,\n",
       " 'Papper': 163,\n",
       " 'Petrarch1603': 164,\n",
       " 'Philluminati': 165,\n",
       " 'Pikajabroni': 166,\n",
       " 'Pilebsa': 167,\n",
       " 'PlasmaWhore': 168,\n",
       " 'Poromenos': 169,\n",
       " 'Poultry_In_Motion': 170,\n",
       " 'ProximaC': 171,\n",
       " 'Prysorra': 172,\n",
       " 'Purp': 173,\n",
       " 'Pxorp': 174,\n",
       " 'Qubed': 175,\n",
       " 'Quel': 176,\n",
       " 'QuinnFazigu': 177,\n",
       " 'RKBA': 178,\n",
       " 'RainmadeMan': 179,\n",
       " 'Recoil42': 180,\n",
       " 'RedDyeNumber4': 181,\n",
       " 'ReiToei': 182,\n",
       " 'ReligionOfPeace': 183,\n",
       " 'Resilience': 184,\n",
       " 'RevLoveJoy': 185,\n",
       " 'RexManningDay': 186,\n",
       " 'Richeh': 187,\n",
       " 'RonObvious': 188,\n",
       " 'RonPaulTouchedMe': 189,\n",
       " 'RonaldFuckingPaul': 190,\n",
       " 'RugerRedhawk': 191,\n",
       " 'S7evyn': 192,\n",
       " 'Saiing': 193,\n",
       " 'SamHealer': 194,\n",
       " 'Sangermaine': 195,\n",
       " 'Sarm': 196,\n",
       " 'Satanscock': 197,\n",
       " 'Saydrah': 198,\n",
       " 'Scarker': 199,\n",
       " 'ScornForSega': 200,\n",
       " 'Shaper_pmp': 201,\n",
       " 'SirEdmund': 202,\n",
       " 'SkyMarshal': 203,\n",
       " 'Sle': 204,\n",
       " 'Slipgrid': 205,\n",
       " 'SlvrEagle23': 206,\n",
       " 'Smight': 207,\n",
       " 'SodiumKPump': 208,\n",
       " 'Spacksack': 209,\n",
       " 'Spazsquatch': 210,\n",
       " 'SpikeWolfwood': 211,\n",
       " 'Spudders': 212,\n",
       " 'Squarsh': 213,\n",
       " 'Sqwirl': 214,\n",
       " 'Stingray88': 215,\n",
       " 'StoneMe': 216,\n",
       " 'Stormflux': 217,\n",
       " 'SuperKing': 218,\n",
       " 'Tack122': 219,\n",
       " 'Taladar': 220,\n",
       " 'TheColonel': 221,\n",
       " 'TheKorn': 222,\n",
       " 'TheSOB88': 223,\n",
       " 'The_Ultimate_Reality': 224,\n",
       " 'Thimble': 225,\n",
       " 'ThisIsDave': 226,\n",
       " 'ThrasherC': 227,\n",
       " 'Tommah': 228,\n",
       " 'TripMaster_Monkey': 229,\n",
       " 'Twisted': 230,\n",
       " 'TwoToke': 231,\n",
       " 'UncleOxidant': 232,\n",
       " 'Unfair': 233,\n",
       " 'UntakenUsername': 234,\n",
       " 'Vash265': 235,\n",
       " 'VincentVega12': 236,\n",
       " 'VnlaThndr775': 237,\n",
       " 'VoodooIdol': 238,\n",
       " 'Vreep-eep': 239,\n",
       " 'WebZen': 240,\n",
       " 'Whisper': 241,\n",
       " 'Winoria': 242,\n",
       " 'WipeHandsOnPants': 243,\n",
       " 'Wo1ke': 244,\n",
       " 'XS4Me': 245,\n",
       " 'Xiphorian': 246,\n",
       " 'Yst': 247,\n",
       " 'Zweben': 248,\n",
       " 'a_little_perspective': 249,\n",
       " 'abrahamsen': 250,\n",
       " 'adaminc': 251,\n",
       " 'adleym': 252,\n",
       " 'admiralteal': 253,\n",
       " 'adremeaux': 254,\n",
       " 'adrianmonk': 255,\n",
       " 'aedes': 256,\n",
       " 'ajrw': 257,\n",
       " 'akatherder': 258,\n",
       " 'akatsukix': 259,\n",
       " 'akdas': 260,\n",
       " 'aktufe': 261,\n",
       " 'alaskamiller': 262,\n",
       " 'aletoledo': 263,\n",
       " 'allhands': 264,\n",
       " 'alllie': 265,\n",
       " 'alphabeat': 266,\n",
       " 'amstrdamordeath': 267,\n",
       " 'anachronic': 268,\n",
       " 'andrewd': 269,\n",
       " 'andrewnorris': 270,\n",
       " 'anescient': 271,\n",
       " 'anions': 272,\n",
       " 'anonymous-coward': 273,\n",
       " 'anthropology_nerd': 274,\n",
       " 'antifolkhero': 275,\n",
       " 'apathy': 276,\n",
       " 'api': 277,\n",
       " 'aradil': 278,\n",
       " 'argeaux': 279,\n",
       " 'argoff': 280,\n",
       " 'artman': 281,\n",
       " 'asaturn': 282,\n",
       " 'aschapm': 283,\n",
       " 'assteroid': 284,\n",
       " 'atomicthumbs': 285,\n",
       " 'aussie_bob': 286,\n",
       " 'automatedresponse': 287,\n",
       " 'axord': 288,\n",
       " 'ayrnieu': 289,\n",
       " 'b34nz': 290,\n",
       " 'b3mus3d': 291,\n",
       " 'bad_llama': 292,\n",
       " 'bamobrien': 293,\n",
       " 'bananahead': 294,\n",
       " 'barryicide': 295,\n",
       " 'bart2019': 296,\n",
       " 'baxyjr': 297,\n",
       " 'bazoople': 298,\n",
       " 'bbqribs': 299,\n",
       " 'bcash': 300,\n",
       " 'bebnet': 301,\n",
       " 'benihana': 302,\n",
       " 'blaze4metal': 303,\n",
       " 'bobcat': 304,\n",
       " 'bobpaul': 305,\n",
       " 'boredzo': 306,\n",
       " 'brad-walker': 307,\n",
       " 'brainburger': 308,\n",
       " 'break99': 309,\n",
       " 'breakneckridge': 310,\n",
       " 'brennen': 311,\n",
       " 'broohaha': 312,\n",
       " 'brosephius': 313,\n",
       " 'brufleth': 314,\n",
       " 'btl': 315,\n",
       " 'buffi': 316,\n",
       " 'burtonmkz': 317,\n",
       " 'bushwakko': 318,\n",
       " 'busytigger': 319,\n",
       " 'bw1870': 320,\n",
       " 'cabbit': 321,\n",
       " 'cajolingwilhelm': 322,\n",
       " 'calantorntain': 323,\n",
       " 'captainAwesomePants': 324,\n",
       " 'casicatracha': 325,\n",
       " 'casual_observer': 326,\n",
       " 'cbroberts': 327,\n",
       " 'cefm': 328,\n",
       " 'ch00f': 329,\n",
       " 'chall85': 330,\n",
       " 'christianjb': 331,\n",
       " 'chunky_bacon': 332,\n",
       " 'cocorobot': 333,\n",
       " 'codepoet': 334,\n",
       " 'commonslip': 335,\n",
       " 'contrarian': 336,\n",
       " 'corentin': 337,\n",
       " 'cov': 338,\n",
       " 'cracell': 339,\n",
       " 'crawfishsoul': 340,\n",
       " 'crazybones': 341,\n",
       " 'crusoe': 342,\n",
       " 'cryptoz': 343,\n",
       " 'cuddles666': 344,\n",
       " 'cum_pussy_tits_ass': 345,\n",
       " 'cup': 346,\n",
       " 'cweaver': 347,\n",
       " 'cwzwarich': 348,\n",
       " 'cyber_rigger': 349,\n",
       " 'cyks': 350,\n",
       " 'cypherx': 351,\n",
       " 'd07c0m': 352,\n",
       " 'd42': 353,\n",
       " 'daisy0808': 354,\n",
       " 'danweber': 355,\n",
       " 'darkgatherer': 356,\n",
       " 'davidreiss666': 357,\n",
       " 'db2': 358,\n",
       " 'dead_ed': 359,\n",
       " 'deadsoon': 360,\n",
       " 'defproc': 361,\n",
       " 'dellis': 362,\n",
       " 'derefr': 363,\n",
       " 'desrosiers': 364,\n",
       " 'deuteros': 365,\n",
       " 'dfranke': 366,\n",
       " 'dghughes': 367,\n",
       " 'dhusk': 368,\n",
       " 'diamond': 369,\n",
       " 'dicey': 370,\n",
       " 'digg_suxx_bigg': 371,\n",
       " 'digital': 372,\n",
       " 'diogames': 373,\n",
       " 'diversionmary': 374,\n",
       " 'dlds': 375,\n",
       " 'dmd': 376,\n",
       " 'dmiff': 377,\n",
       " 'dodus': 378,\n",
       " 'dons': 379,\n",
       " 'doodahdei': 380,\n",
       " 'doody': 381,\n",
       " 'dora_explorer': 382,\n",
       " 'dotrob': 383,\n",
       " 'downdiagonal': 384,\n",
       " 'doxiegrl1': 385,\n",
       " 'doyoulikeworms': 386,\n",
       " 'dpzdpz': 387,\n",
       " 'dsk': 388,\n",
       " 'duhblow7': 389,\n",
       " 'duus': 390,\n",
       " 'eOgas': 391,\n",
       " 'earthboundkid': 392,\n",
       " 'eaturbrainz': 393,\n",
       " 'eddie964': 394,\n",
       " 'edguy': 395,\n",
       " 'einexile': 396,\n",
       " 'ejp1082': 397,\n",
       " 'el_pinata': 398,\n",
       " 'elasticsoul': 399,\n",
       " 'elissa1959': 400,\n",
       " 'elquesogrande': 401,\n",
       " 'epicRelic': 402,\n",
       " 'epsilona01': 403,\n",
       " 'eroverton': 404,\n",
       " 'erulabs': 405,\n",
       " 'eusephus': 406,\n",
       " 'executivemonkey': 407,\n",
       " 'f0nd004u': 408,\n",
       " 'fangolo': 409,\n",
       " 'fapman': 410,\n",
       " 'ffualo': 411,\n",
       " 'fingers': 412,\n",
       " 'finix': 413,\n",
       " 'fjhqjv': 414,\n",
       " 'flydog2': 415,\n",
       " 'foonly': 416,\n",
       " 'formido': 417,\n",
       " 'free_man': 418,\n",
       " 'frickindeal': 419,\n",
       " 'frogking': 420,\n",
       " 'frukt': 421,\n",
       " 'frutiger': 422,\n",
       " 'fstorino': 423,\n",
       " 'fuckbuddy': 424,\n",
       " 'fujimitsu': 425,\n",
       " 'fun1ne': 426,\n",
       " 'furry8': 427,\n",
       " 'fwork': 428,\n",
       " 'g0taclue': 429,\n",
       " 'garg': 430,\n",
       " 'garyp714': 431,\n",
       " 'garyr_h': 432,\n",
       " 'geeeeoffff': 433,\n",
       " 'generic_handle': 434,\n",
       " 'geoff422': 435,\n",
       " 'gfixler': 436,\n",
       " 'gid13': 437,\n",
       " 'gigaquack': 438,\n",
       " 'gimeit': 439,\n",
       " 'glmory': 440,\n",
       " 'gmick': 441,\n",
       " 'godlesspinko': 442,\n",
       " 'gonorrhea': 443,\n",
       " 'goodbyeworld': 444,\n",
       " 'grauenwolf': 445,\n",
       " 'gregK': 446,\n",
       " 'greginnj': 447,\n",
       " 'grignr': 448,\n",
       " 'growinglotus': 449,\n",
       " 'guntotingliberal': 450,\n",
       " 'guriboysf': 451,\n",
       " 'gvsteve': 452,\n",
       " 'h0dg3s': 453,\n",
       " 'habbadash': 454,\n",
       " 'happysinger': 455,\n",
       " 'harryballsagna': 456,\n",
       " 'haywire': 457,\n",
       " 'haywire9000': 458,\n",
       " 'hellsbelles': 459,\n",
       " 'hiS_oWn': 460,\n",
       " 'hibryd': 461,\n",
       " 'hiredgoon': 462,\n",
       " 'hitler_rape_abortion': 463,\n",
       " 'hitmonval': 464,\n",
       " 'homeworld': 465,\n",
       " 'hongnanhai': 466,\n",
       " 'honus': 467,\n",
       " 'hotwingbias': 468,\n",
       " 'hpymondays': 469,\n",
       " 'hs4x': 470,\n",
       " 'hypo11': 471,\n",
       " 'illuminatedwax': 472,\n",
       " 'indycysive': 473,\n",
       " 'inferno0000': 474,\n",
       " 'infinite': 475,\n",
       " 'infinityvortex': 476,\n",
       " 'innocentbystander': 477,\n",
       " 'intellectual': 478,\n",
       " 'interstate': 479,\n",
       " 'j2d2': 480,\n",
       " 'jaalin': 481,\n",
       " 'jaggederest': 482,\n",
       " 'jamesallen74': 483,\n",
       " 'jamierc': 484,\n",
       " 'jax9999': 485,\n",
       " 'jayssite': 486,\n",
       " 'je255j': 487,\n",
       " 'jerf': 488,\n",
       " 'jesus4u': 489,\n",
       " 'jfowler27': 490,\n",
       " 'jingo04': 491,\n",
       " 'jjrs': 492,\n",
       " 'jodv': 493,\n",
       " 'joe90210': 494,\n",
       " 'johnfn': 495,\n",
       " 'jon_k': 496,\n",
       " 'jon_titor': 497,\n",
       " 'jones77': 498,\n",
       " 'jonknee': 499,\n",
       " 'jordanlund': 500,\n",
       " 'joshfern': 501,\n",
       " 'jotaroh': 502,\n",
       " 'joyork': 503,\n",
       " 'jozzas': 504,\n",
       " 'judgej2': 505,\n",
       " 'jumpyg1258': 506,\n",
       " 'junkeee999': 507,\n",
       " 'justinhj': 508,\n",
       " 'kalazar': 509,\n",
       " 'kanuk876': 510,\n",
       " 'kareems': 511,\n",
       " 'katsi': 512,\n",
       " 'kdraper': 513,\n",
       " 'kelmr2003': 514,\n",
       " 'kermityfrog': 515,\n",
       " 'ketralnis': 516,\n",
       " 'khafra': 517,\n",
       " 'khoury': 518,\n",
       " 'killerstorm': 519,\n",
       " 'kingkilr': 520,\n",
       " 'kjartanelli': 521,\n",
       " 'knylok': 522,\n",
       " 'kolm': 523,\n",
       " 'kousi': 524,\n",
       " 'kraemahz': 525,\n",
       " 'kraftmatic': 526,\n",
       " 'krizo': 527,\n",
       " 'krugerlive': 528,\n",
       " 'leemy': 529,\n",
       " 'liber8US': 530,\n",
       " 'linkedlist': 531,\n",
       " 'llanor': 532,\n",
       " 'locke2002': 533,\n",
       " 'lockhart000': 534,\n",
       " 'lugfish': 535,\n",
       " 'lylia': 536,\n",
       " 'm1ss1ontomars2k4': 537,\n",
       " 'machrider': 538,\n",
       " 'mackprime': 539,\n",
       " 'madmax_br5': 540,\n",
       " 'malcontent': 541,\n",
       " 'manthrax': 542,\n",
       " 'maqr': 543,\n",
       " 'marthirial': 544,\n",
       " 'martoo': 545,\n",
       " 'masklinn': 546,\n",
       " 'mcantelon': 547,\n",
       " 'mccoyn': 548,\n",
       " 'mchrisneglia': 549,\n",
       " 'me_so_porny': 550,\n",
       " 'megablahblah': 551,\n",
       " 'mexicodoug': 552,\n",
       " 'mindbleach': 553,\n",
       " 'mipadi': 554,\n",
       " 'miparasito': 555,\n",
       " 'mistermoxy': 556,\n",
       " 'miyakohouou': 557,\n",
       " 'mkrfctr': 558,\n",
       " 'mmazing': 559,\n",
       " 'moddestmouse': 560,\n",
       " 'moneyprinter': 561,\n",
       " 'moogle516': 562,\n",
       " 'mooglor': 563,\n",
       " 'moonman': 564,\n",
       " 'moonzilla': 565,\n",
       " 'moriya': 566,\n",
       " 'mothereffingtheresa': 567,\n",
       " 'movzx': 568,\n",
       " 'moxy': 569,\n",
       " 'mrbroom': 570,\n",
       " 'mschaef': 571,\n",
       " 'msdesireeg': 572,\n",
       " 'mturk': 573,\n",
       " 'muhfuhkuh': 574,\n",
       " 'mutatron': 575,\n",
       " 'mynameisdave': 576,\n",
       " 'mynameishere': 577,\n",
       " 'myotheralt': 578,\n",
       " 'mystery_guest': 579,\n",
       " 'natrius': 580,\n",
       " 'neat_stuff': 581,\n",
       " 'neoabraxas': 582,\n",
       " 'neoform3': 583,\n",
       " 'neonic': 584,\n",
       " 'neuquino': 585,\n",
       " 'nevesis': 586,\n",
       " 'nextofpumpkin': 587,\n",
       " 'nfulton': 588,\n",
       " 'ngngboone': 589,\n",
       " 'ngroot': 590,\n",
       " 'nickatnite101': 591,\n",
       " 'nicolaslloyd': 592,\n",
       " 'nihilite': 593,\n",
       " 'nixonrichard': 594,\n",
       " 'njharman': 595,\n",
       " 'nmcyall': 596,\n",
       " 'nooneelse': 597,\n",
       " 'noseeme': 598,\n",
       " 'notor': 599,\n",
       " 'ntr0p3': 600,\n",
       " 'numb3rb0y': 601,\n",
       " 'number6': 602,\n",
       " 'numlok': 603,\n",
       " 'oblivious_human': 604,\n",
       " 'oddmanout': 605,\n",
       " 'officemonkey': 606,\n",
       " 'old_gill': 607,\n",
       " 'ondal': 608,\n",
       " 'onebit': 609,\n",
       " 'oniony': 610,\n",
       " 'orthogonality': 611,\n",
       " 'otakucode': 612,\n",
       " 'otatop': 613,\n",
       " 'otterdam': 614,\n",
       " 'ouroborosity': 615,\n",
       " 'p3do': 616,\n",
       " 'pandemic': 617,\n",
       " 'paro': 618,\n",
       " 'pastanoose': 619,\n",
       " 'pavel_lishin': 620,\n",
       " 'pelirrojo': 621,\n",
       " 'pillage': 622,\n",
       " 'pilto': 623,\n",
       " 'pingish': 624,\n",
       " 'pozorvlak': 625,\n",
       " 'pradador': 626,\n",
       " 'protoopus': 627,\n",
       " 'psdtocode': 628,\n",
       " 'psychometry': 629,\n",
       " 'psyne': 630,\n",
       " 'qarl': 631,\n",
       " 'qgyh2': 632,\n",
       " 'quack': 633,\n",
       " 'quiller': 634,\n",
       " 'r2002': 635,\n",
       " 'raedix': 636,\n",
       " 'rainman_104': 637,\n",
       " 'ralphwiggum': 638,\n",
       " 'randomb0y': 639,\n",
       " 'raouldukeesq': 640,\n",
       " 'rats99ass': 641,\n",
       " 'readdit7': 642,\n",
       " 'rebop': 643,\n",
       " 'reddit_user13': 644,\n",
       " 'redditcensoredme': 645,\n",
       " 'redditrasberry': 646,\n",
       " 'redninja2000': 647,\n",
       " 'rek': 648,\n",
       " 'relic2279': 649,\n",
       " 'reverendfrag4': 650,\n",
       " 'ride': 651,\n",
       " 'rieux': 652,\n",
       " 'ristin': 653,\n",
       " 'rjcarr': 654,\n",
       " 'rmuser': 655,\n",
       " 'robdag2': 656,\n",
       " 'robodale': 657,\n",
       " 'robotevil': 658,\n",
       " 'robotfuel': 659,\n",
       " 'rockicon82': 660,\n",
       " 'rogerssucks': 661,\n",
       " 'roland19d': 662,\n",
       " 'ropers': 663,\n",
       " 'rory096': 664,\n",
       " 'rosshettel': 665,\n",
       " 'rthomas6': 666,\n",
       " 'ryanissuper': 667,\n",
       " 'ryanknapper': 668,\n",
       " 'ryanx27': 669,\n",
       " 'sakebomb69': 670,\n",
       " 'sam512': 671,\n",
       " 'san1ty': 672,\n",
       " 'sarahfrancesca': 673,\n",
       " 'satx': 674,\n",
       " 'sblinn': 675,\n",
       " 'scared1': 676,\n",
       " 'schizobullet': 677,\n",
       " 'sense': 678,\n",
       " 'sfultong': 679,\n",
       " 'shadowsurge': 680,\n",
       " 'shaurz': 681,\n",
       " 'shazbaz': 682,\n",
       " 'sheeprevolution': 683,\n",
       " 'shiner_man': 684,\n",
       " 'shitcovereddick': 685,\n",
       " 'shmi': 686,\n",
       " 'silence_hr': 687,\n",
       " 'sinsyder': 688,\n",
       " 'sketerpot': 689,\n",
       " 'skibybadoowap': 690,\n",
       " 'skippy17': 691,\n",
       " 'skitzh0': 692,\n",
       " 'slenderdog': 693,\n",
       " 'slidinglight': 694,\n",
       " 'slydee': 695,\n",
       " 'smitisme': 696,\n",
       " 'sn0re': 697,\n",
       " 'snair': 698,\n",
       " 'snifty': 699,\n",
       " 'snookums': 700,\n",
       " 'socks': 701,\n",
       " 'somn': 702,\n",
       " 'spaceghoti': 703,\n",
       " 'spez': 704,\n",
       " 'spif': 705,\n",
       " 'spookyvision': 706,\n",
       " 'squidboots': 707,\n",
       " 'squigs': 708,\n",
       " 'srika': 709,\n",
       " 'st_gulik': 710,\n",
       " 'stacecom': 711,\n",
       " 'state_of_alert': 712,\n",
       " 'stesch': 713,\n",
       " 'stomicron': 714,\n",
       " 'stopit': 715,\n",
       " 'stopmotionsunrise': 716,\n",
       " 'stunt_penguin': 717,\n",
       " 'sunshine-x': 718,\n",
       " 'supaphly42': 719,\n",
       " 'swagohome': 720,\n",
       " 'swede': 721,\n",
       " 'sylvan': 722,\n",
       " 'synthpop': 723,\n",
       " 'syroncoda': 724,\n",
       " 'tbotcotw': 725,\n",
       " 'tdehnel': 726,\n",
       " 'thatguydr': 727,\n",
       " 'thax': 728,\n",
       " 'theDrWho': 729,\n",
       " 'the_big_wedding': 730,\n",
       " 'thecompletegeek': 731,\n",
       " 'theeth': 732,\n",
       " 'thekrone': 733,\n",
       " 'thepensivepoet': 734,\n",
       " 'thrakhath': 735,\n",
       " 'throwaway': 736,\n",
       " 'tjdziuba': 737,\n",
       " 'tomjen': 738,\n",
       " 'trackerbishop': 739,\n",
       " 'treebright': 740,\n",
       " 'trenchfever': 741,\n",
       " 'tridentgum': 742,\n",
       " 'trivial': 743,\n",
       " 'troglodyte': 744,\n",
       " 'tryk48s': 745,\n",
       " 'tsteele93': 746,\n",
       " 'tuoder': 747,\n",
       " 'turkourjurbs': 748,\n",
       " 'tvshopceo': 749,\n",
       " 'ubernostrum': 750,\n",
       " 'ubitendo': 751,\n",
       " 'ultimatt42': 752,\n",
       " 'username223': 753,\n",
       " 'ustgblerkvusrd': 754,\n",
       " 'uteunawaytay': 755,\n",
       " 'utexaspunk': 756,\n",
       " 'uwjames': 757,\n",
       " 'vajav': 758,\n",
       " 'valeriepieris': 759,\n",
       " 'vanostran': 760,\n",
       " 'vapblack': 761,\n",
       " 'veracity024': 762,\n",
       " 'veritaze': 763,\n",
       " 'wbeavis': 764,\n",
       " 'wejash': 765,\n",
       " 'whatwedo': 766,\n",
       " 'willis77': 767,\n",
       " 'windmilltheory': 768,\n",
       " 'windynights': 769,\n",
       " 'wolfzero': 770,\n",
       " 'woodsier': 771,\n",
       " 'wurtis16': 772,\n",
       " 'wwabc': 773,\n",
       " 'xMadxScientistx': 774,\n",
       " 'xenmate': 775,\n",
       " 'xinhoj': 776,\n",
       " 'xutopia': 777,\n",
       " 'xyphus': 778,\n",
       " 'yellowking': 779,\n",
       " 'zem': 780,\n",
       " 'zepolen': 781,\n",
       " 'zombieaynrand': 782,\n",
       " 'zombiewat': 783,\n",
       " 'zoomzoom83': 784,\n",
       " 'zorno': 785,\n",
       " 'zyzzogeton': 786}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e252bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/p2index.pkl\",\"rb\") as f:\n",
    "    p2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e7e658",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AskReddit': 0,\n",
       " 'Drugs': 1,\n",
       " 'Economics': 2,\n",
       " 'Music': 3,\n",
       " 'WTF': 4,\n",
       " 'apple': 5,\n",
       " 'area51': 6,\n",
       " 'atheism': 7,\n",
       " 'bestof': 8,\n",
       " 'business': 9,\n",
       " 'canada': 10,\n",
       " 'cogsci': 11,\n",
       " 'comics': 12,\n",
       " 'entertainment': 13,\n",
       " 'environment': 14,\n",
       " 'funny': 15,\n",
       " 'gadgets': 16,\n",
       " 'gaming': 17,\n",
       " 'geek': 18,\n",
       " 'happy': 19,\n",
       " 'lgbt': 20,\n",
       " 'linux': 21,\n",
       " 'lolcats': 22,\n",
       " 'math': 23,\n",
       " 'netsec': 24,\n",
       " 'nsfw': 25,\n",
       " 'obama': 26,\n",
       " 'offbeat': 27,\n",
       " 'philosophy': 28,\n",
       " 'photography': 29,\n",
       " 'pics': 30,\n",
       " 'politics': 31,\n",
       " 'programming': 32,\n",
       " 'psychology': 33,\n",
       " 'reddit.com': 34,\n",
       " 'science': 35,\n",
       " 'scifi': 36,\n",
       " 'self': 37,\n",
       " 'sex': 38,\n",
       " 'software': 39,\n",
       " 'sports': 40,\n",
       " 'technology': 41,\n",
       " 'videos': 42,\n",
       " 'web_design': 43,\n",
       " 'worldnews': 44,\n",
       " 'xkcd': 45,\n",
       " 'yourweek': 46}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886eb54",
   "metadata": {},
   "source": [
    "## edge list data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7874fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_df = pd.read_csv(\"../../data/02_intermediate/user_behavior/edge_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d850c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>retrieved_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultimatt42</td>\n",
       "      <td>science</td>\n",
       "      <td>1425846806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jonknee</td>\n",
       "      <td>programming</td>\n",
       "      <td>1425846807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burtonmkz</td>\n",
       "      <td>science</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sblinn</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dons</td>\n",
       "      <td>programming</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jedravent</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WebZen</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doodahdei</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author    subreddit  retrieved_on\n",
       "0    ultimatt42      science    1425846806\n",
       "1       jonknee  programming    1425846807\n",
       "2     burtonmkz      science    1425846810\n",
       "3  pavel_lishin   reddit.com    1425846810\n",
       "4  pavel_lishin   reddit.com    1425846810\n",
       "5        sblinn     politics    1425846810\n",
       "6          dons  programming    1425846811\n",
       "7     Jedravent     politics    1425846811\n",
       "8        WebZen     politics    1425846811\n",
       "9     doodahdei     politics    1425846812"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgelist_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18f7d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "def process_edgelist(edge_list, u2index, p2index):\n",
    "    \"\"\" Load edge list and construct a graph \"\"\"\n",
    "    edges = Counter()\n",
    "\n",
    "    for i, row in edge_list.iterrows():\n",
    "        #u = row[0]\n",
    "        #p = row[1]\n",
    "        #t = row[2]\n",
    "        u = row['author']\n",
    "        p = row['subreddit']\n",
    "        t = row['retrieved_on']\n",
    "\n",
    "        if i<1:\n",
    "            print(u, p, t)\n",
    "        edges[(u2index[u], p2index[p])] += 1\n",
    "    # Construct the graph\n",
    "    row = []\n",
    "    col = []\n",
    "    entry = []\n",
    "    for edge, w in edges.items():\n",
    "        #print(w)\n",
    "        i, j = edge\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "        entry.append(w)\n",
    "    graph = csr_matrix(\n",
    "        (entry, (row, col)), \n",
    "        shape=(len(u2index), len(p2index))\n",
    "    )   \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b390c3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultimatt42 science 1425846806\n"
     ]
    }
   ],
   "source": [
    "graph = process_edgelist(edgelist_df, u2index, p2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c212b68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24071655",
   "metadata": {},
   "source": [
    "## train/validation/test id split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c823216",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/data_tvt.pkl\",\"rb\") as f:\n",
    "    tvt_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7846b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = tvt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e5f77b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((314,), (79,), (393,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train.shape, idx_val.shape, idx_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c624cb",
   "metadata": {},
   "source": [
    "### convert label format (to numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd541fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(labels: pd.DataFrame) -> np.array:\n",
    "    \"\"\"process label information\"\"\"\n",
    "    u_all = set()\n",
    "    pos_uids = set()\n",
    "    labeled_uids = set()\n",
    "    #convert a dataframe to an numpy array, array index being mapped indexes from u2index\n",
    "    for i,row in labels.iterrows():\n",
    "        author = row['author']\n",
    "        author_label = row['label']\n",
    "        u_all.add(author)\n",
    "        if author_label == 1:\n",
    "            pos_uids.add(author)\n",
    "            labeled_uids.add(author)\n",
    "        elif author_label == 0:\n",
    "            labeled_uids.add(author)\n",
    "    print(f'loaded labels, total of {len(pos_uids)} positive users and {len(labeled_uids)} labeled users')\n",
    "    labels = np.zeros(len(u2index))\n",
    "    for u in u2index:\n",
    "        if u in pos_uids:\n",
    "            labels[u2index[u]] = 1\n",
    "    labels = labels.astype(int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7dbf9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 327 positive users and 787 labeled users\n"
     ]
    }
   ],
   "source": [
    "labels = process_label(user_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "627e1571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: total of   314 users with   131 pos users and   183 neg users\n",
      "Val:   total of    79 users with    38 pos users and    41 neg users\n",
      "Test:  total of   393 users with   157 pos users and   236 neg users\n"
     ]
    }
   ],
   "source": [
    "print('Train: total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_train), \n",
    "    np.sum(labels[idx_train]), \n",
    "    len(idx_train)-np.sum(labels[idx_train]))\n",
    "     )\n",
    "print('Val:   total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_val), \n",
    "    np.sum(labels[idx_val]), \n",
    "    len(idx_val)-np.sum(labels[idx_val]))\n",
    "     )\n",
    "print('Test:  total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_test), \n",
    "    np.sum(labels[idx_test]), \n",
    "    len(idx_test)-np.sum(labels[idx_test]))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ece78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = np.load(\"../../data/02_intermediate/user_behavior/user2vec_npy.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83e80817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features['data'].shape #787 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8de2071",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = np.load(\"../../data/02_intermediate/user_behavior/prod2vec_npy.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dcae036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features['data'].shape #47 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4749ccd",
   "metadata": {},
   "source": [
    "### setting up the model trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.append('/home/ec2-user/SageMaker/anomaly-detection-spatial-temporal-data/')\n",
    "sys.path.append('/home/ec2-user/SageMaker/anomaly-detection-spatial-temporal-data/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab73d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.data_loader import DynamicGraphWNFDataSet, DynamicGraphWNodeFeatDatasetLoader\n",
    "from model.dynamic_graph import Eland_e2e\n",
    "from model.model_config import ElandConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DynamicGraphWNodeFeatDatasetLoader(\n",
    "    labels, \n",
    "    u2index, \n",
    "    p2index, \n",
    "    edge_list, \n",
    "    tvt_nids, \n",
    "    user_features, \n",
    "    item_features\n",
    ")\n",
    "\n",
    "#sequential data loader\n",
    "dataset = DynamicGraphWNFDataSet(p2index, item_features, edge_list)\n",
    "lstm_dataloader = DataLoader(dataset, batch_size=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "        'graph': data_loader.graph, \n",
    "        'lstm_dataloader': lstm_dataloader,\n",
    "        'user_features': data_loader.user_features,\n",
    "        'item_features': data_loader.item_features,\n",
    "        'labels': data_loader.labels,\n",
    "        'tvt_nids': data_loader.tvt_idx,\n",
    "        'u2index': data_loader.u2index,\n",
    "        'p2index': data_loader.p2index\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13c5c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d3cfa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_file = '../../conf/base/parameters/eland.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f122cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eland_data_load_options': {'graph_num': 0.1, 'dataset': 'reddit', 'method': 'gcn', 'rnn': 'gru', 'baseline': 'store_true'}, 'eland_model_options': {'dim_feats': 300, 'cuda': 0, 'hidden_size': 128, 'n_layers': 2, 'epochs': 50, 'batch_size': 10, 'seed': -1, 'lr': 0.0001, 'log': True, 'weight_decay': 1e-06, 'dropout': 0.4, 'tensorboard': False, 'name': 'debug', 'gnnlayer_type': 'gcn', 'rnn_type': 'lstm', 'pretrain_bm': 25, 'pretrain_nc': 200, 'alpha': 0.05, 'bmloss_type': 'mse', 'device': 'cpu', 'base_pred': 400, 'save_directory': 'data/07_model_output/user_behavior'}}\n"
     ]
    }
   ],
   "source": [
    "with open(model_config_file, \"r\") as stream:\n",
    "    try:\n",
    "        mode_config=yaml.safe_load(stream)\n",
    "        print(mode_config)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0671e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ElandConfig(mode_config['eland_model_options'])\n",
    "model_obj = Eland_e2e(\n",
    "    data_dict['graph'], \n",
    "    data_dict['lstm_dataloader'], \n",
    "    data_dict['user_features'],\n",
    "    data_dict['item_features'], \n",
    "    data_dict['labels'], \n",
    "    data_dict['tvt_nids'], \n",
    "    data_dict['u2index'],\n",
    "    data_dict['p2index'], \n",
    "    data_dict['item_features'], \n",
    "    model_config\n",
    ")\n",
    "auc, ap = model_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7486b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 333536.62it/s]\n",
      "2022-06-30 00:54:22,325 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:54:22,511 - NCNet pretrain, Epoch [1 / 300]: loss 1.3692, training auc: 0.5713, val_auc 0.4652, test auc 0.4359\n",
      "2022-06-30 00:54:22,636 - NCNet pretrain, Epoch [2 / 300]: loss 0.8614, training auc: 0.4508, val_auc 0.4638\n",
      "2022-06-30 00:54:22,779 - NCNet pretrain, Epoch [3 / 300]: loss 0.5982, training auc: 0.4116, val_auc 0.4611\n",
      "2022-06-30 00:54:22,917 - NCNet pretrain, Epoch [4 / 300]: loss 0.5114, training auc: 0.4376, val_auc 0.4602\n",
      "2022-06-30 00:54:23,062 - NCNet pretrain, Epoch [5 / 300]: loss 0.5774, training auc: 0.4213, val_auc 0.4606\n",
      "2022-06-30 00:54:23,187 - NCNet pretrain, Epoch [6 / 300]: loss 0.6406, training auc: 0.4113, val_auc 0.4612\n",
      "2022-06-30 00:54:23,319 - NCNet pretrain, Epoch [7 / 300]: loss 0.6283, training auc: 0.4172, val_auc 0.4618\n",
      "2022-06-30 00:54:23,468 - NCNet pretrain, Epoch [8 / 300]: loss 0.5776, training auc: 0.4248, val_auc 0.4626\n",
      "2022-06-30 00:54:23,594 - NCNet pretrain, Epoch [9 / 300]: loss 0.5351, training auc: 0.4288, val_auc 0.4647\n",
      "2022-06-30 00:54:23,749 - NCNet pretrain, Epoch [10 / 300]: loss 0.5208, training auc: 0.4163, val_auc 0.4678, test auc 0.4415\n",
      "2022-06-30 00:54:23,893 - NCNet pretrain, Epoch [11 / 300]: loss 0.5081, training auc: 0.4744, val_auc 0.4704, test auc 0.4451\n",
      "2022-06-30 00:54:24,028 - NCNet pretrain, Epoch [12 / 300]: loss 0.5258, training auc: 0.4724, val_auc 0.4720, test auc 0.4473\n",
      "2022-06-30 00:54:24,163 - NCNet pretrain, Epoch [13 / 300]: loss 0.5336, training auc: 0.4571, val_auc 0.4727, test auc 0.4479\n",
      "2022-06-30 00:54:24,286 - NCNet pretrain, Epoch [14 / 300]: loss 0.5177, training auc: 0.4679, val_auc 0.4727\n",
      "2022-06-30 00:54:24,410 - NCNet pretrain, Epoch [15 / 300]: loss 0.5139, training auc: 0.4467, val_auc 0.4727\n",
      "2022-06-30 00:54:24,534 - NCNet pretrain, Epoch [16 / 300]: loss 0.5252, training auc: 0.3935, val_auc 0.4726\n",
      "2022-06-30 00:54:24,658 - NCNet pretrain, Epoch [17 / 300]: loss 0.5129, training auc: 0.4275, val_auc 0.4727\n",
      "2022-06-30 00:54:24,784 - NCNet pretrain, Epoch [18 / 300]: loss 0.5214, training auc: 0.4141, val_auc 0.4734, test auc 0.4467\n",
      "2022-06-30 00:54:24,911 - NCNet pretrain, Epoch [19 / 300]: loss 0.4911, training auc: 0.4933, val_auc 0.4743, test auc 0.4477\n",
      "2022-06-30 00:54:25,044 - NCNet pretrain, Epoch [20 / 300]: loss 0.5036, training auc: 0.4576, val_auc 0.4751, test auc 0.4489\n",
      "2022-06-30 00:54:25,177 - NCNet pretrain, Epoch [21 / 300]: loss 0.4952, training auc: 0.4787, val_auc 0.4760, test auc 0.4502\n",
      "2022-06-30 00:54:25,310 - NCNet pretrain, Epoch [22 / 300]: loss 0.5332, training auc: 0.3741, val_auc 0.4771, test auc 0.4519\n",
      "2022-06-30 00:54:25,444 - NCNet pretrain, Epoch [23 / 300]: loss 0.5062, training auc: 0.4415, val_auc 0.4786, test auc 0.4535\n",
      "2022-06-30 00:54:25,586 - NCNet pretrain, Epoch [24 / 300]: loss 0.5072, training auc: 0.4425, val_auc 0.4797, test auc 0.4549\n",
      "2022-06-30 00:54:25,724 - NCNet pretrain, Epoch [25 / 300]: loss 0.4922, training auc: 0.4764, val_auc 0.4805, test auc 0.4559\n",
      "2022-06-30 00:54:25,857 - NCNet pretrain, Epoch [26 / 300]: loss 0.5060, training auc: 0.4407, val_auc 0.4815, test auc 0.4568\n",
      "2022-06-30 00:54:25,993 - NCNet pretrain, Epoch [27 / 300]: loss 0.5051, training auc: 0.4416, val_auc 0.4824, test auc 0.4574\n",
      "2022-06-30 00:54:26,131 - NCNet pretrain, Epoch [28 / 300]: loss 0.4980, training auc: 0.4683, val_auc 0.4830, test auc 0.4578\n",
      "2022-06-30 00:54:26,263 - NCNet pretrain, Epoch [29 / 300]: loss 0.5150, training auc: 0.4008, val_auc 0.4838, test auc 0.4583\n",
      "2022-06-30 00:54:26,395 - NCNet pretrain, Epoch [30 / 300]: loss 0.4919, training auc: 0.4683, val_auc 0.4846, test auc 0.4587\n",
      "2022-06-30 00:54:26,527 - NCNet pretrain, Epoch [31 / 300]: loss 0.4832, training auc: 0.4872, val_auc 0.4851, test auc 0.4592\n",
      "2022-06-30 00:54:26,659 - NCNet pretrain, Epoch [32 / 300]: loss 0.5004, training auc: 0.4354, val_auc 0.4862, test auc 0.4600\n",
      "2022-06-30 00:54:26,793 - NCNet pretrain, Epoch [33 / 300]: loss 0.4972, training auc: 0.4446, val_auc 0.4873, test auc 0.4612\n",
      "2022-06-30 00:54:26,945 - NCNet pretrain, Epoch [34 / 300]: loss 0.4866, training auc: 0.4770, val_auc 0.4882, test auc 0.4624\n",
      "2022-06-30 00:54:27,078 - NCNet pretrain, Epoch [35 / 300]: loss 0.5020, training auc: 0.4340, val_auc 0.4896, test auc 0.4638\n",
      "2022-06-30 00:54:27,212 - NCNet pretrain, Epoch [36 / 300]: loss 0.4930, training auc: 0.4752, val_auc 0.4910, test auc 0.4651\n",
      "2022-06-30 00:54:27,345 - NCNet pretrain, Epoch [37 / 300]: loss 0.4756, training auc: 0.4983, val_auc 0.4925, test auc 0.4666\n",
      "2022-06-30 00:54:27,478 - NCNet pretrain, Epoch [38 / 300]: loss 0.4774, training auc: 0.4816, val_auc 0.4941, test auc 0.4683\n",
      "2022-06-30 00:54:27,611 - NCNet pretrain, Epoch [39 / 300]: loss 0.4984, training auc: 0.4282, val_auc 0.4959, test auc 0.4703\n",
      "2022-06-30 00:54:27,744 - NCNet pretrain, Epoch [40 / 300]: loss 0.4871, training auc: 0.4624, val_auc 0.4978, test auc 0.4721\n",
      "2022-06-30 00:54:27,877 - NCNet pretrain, Epoch [41 / 300]: loss 0.4760, training auc: 0.4942, val_auc 0.4993, test auc 0.4737\n",
      "2022-06-30 00:54:28,010 - NCNet pretrain, Epoch [42 / 300]: loss 0.4727, training auc: 0.5052, val_auc 0.5008, test auc 0.4751\n",
      "2022-06-30 00:54:28,147 - NCNet pretrain, Epoch [43 / 300]: loss 0.4884, training auc: 0.4512, val_auc 0.5028, test auc 0.4768\n",
      "2022-06-30 00:54:28,283 - NCNet pretrain, Epoch [44 / 300]: loss 0.4899, training auc: 0.4439, val_auc 0.5046, test auc 0.4788\n",
      "2022-06-30 00:54:28,414 - NCNet pretrain, Epoch [45 / 300]: loss 0.4709, training auc: 0.4959, val_auc 0.5070, test auc 0.4811\n",
      "2022-06-30 00:54:28,548 - NCNet pretrain, Epoch [46 / 300]: loss 0.4727, training auc: 0.5094, val_auc 0.5093, test auc 0.4834\n",
      "2022-06-30 00:54:28,686 - NCNet pretrain, Epoch [47 / 300]: loss 0.4840, training auc: 0.4622, val_auc 0.5126, test auc 0.4868\n",
      "2022-06-30 00:54:28,820 - NCNet pretrain, Epoch [48 / 300]: loss 0.4706, training auc: 0.4996, val_auc 0.5164, test auc 0.4906\n",
      "2022-06-30 00:54:28,952 - NCNet pretrain, Epoch [49 / 300]: loss 0.4638, training auc: 0.5258, val_auc 0.5195, test auc 0.4945\n",
      "2022-06-30 00:54:29,085 - NCNet pretrain, Epoch [50 / 300]: loss 0.4693, training auc: 0.5156, val_auc 0.5231, test auc 0.4979\n",
      "2022-06-30 00:54:29,217 - NCNet pretrain, Epoch [51 / 300]: loss 0.4751, training auc: 0.4820, val_auc 0.5270, test auc 0.5019\n",
      "2022-06-30 00:54:29,369 - NCNet pretrain, Epoch [52 / 300]: loss 0.4654, training auc: 0.5075, val_auc 0.5309, test auc 0.5060\n",
      "2022-06-30 00:54:29,502 - NCNet pretrain, Epoch [53 / 300]: loss 0.4634, training auc: 0.5084, val_auc 0.5355, test auc 0.5109\n",
      "2022-06-30 00:54:29,634 - NCNet pretrain, Epoch [54 / 300]: loss 0.4693, training auc: 0.4830, val_auc 0.5405, test auc 0.5161\n",
      "2022-06-30 00:54:29,767 - NCNet pretrain, Epoch [55 / 300]: loss 0.4596, training auc: 0.5224, val_auc 0.5444, test auc 0.5204\n",
      "2022-06-30 00:54:29,920 - NCNet pretrain, Epoch [56 / 300]: loss 0.4541, training auc: 0.5308, val_auc 0.5477, test auc 0.5233\n",
      "2022-06-30 00:54:30,053 - NCNet pretrain, Epoch [57 / 300]: loss 0.4465, training auc: 0.5596, val_auc 0.5522, test auc 0.5277\n",
      "2022-06-30 00:54:30,186 - NCNet pretrain, Epoch [58 / 300]: loss 0.4383, training auc: 0.5737, val_auc 0.5578, test auc 0.5333\n",
      "2022-06-30 00:54:30,323 - NCNet pretrain, Epoch [59 / 300]: loss 0.4594, training auc: 0.5057, val_auc 0.5679, test auc 0.5445\n",
      "2022-06-30 00:54:30,461 - NCNet pretrain, Epoch [60 / 300]: loss 0.4399, training auc: 0.5780, val_auc 0.5792, test auc 0.5570\n",
      "2022-06-30 00:54:30,592 - NCNet pretrain, Epoch [61 / 300]: loss 0.4515, training auc: 0.5520, val_auc 0.5869, test auc 0.5658\n",
      "2022-06-30 00:54:30,724 - NCNet pretrain, Epoch [62 / 300]: loss 0.4425, training auc: 0.5912, val_auc 0.5885, test auc 0.5663\n",
      "2022-06-30 00:54:30,852 - NCNet pretrain, Epoch [63 / 300]: loss 0.4344, training auc: 0.5872, val_auc 0.5887, test auc 0.5654\n",
      "2022-06-30 00:54:30,983 - NCNet pretrain, Epoch [64 / 300]: loss 0.4392, training auc: 0.5805, val_auc 0.5950, test auc 0.5719\n",
      "2022-06-30 00:54:31,115 - NCNet pretrain, Epoch [65 / 300]: loss 0.4401, training auc: 0.5751, val_auc 0.6089, test auc 0.5872\n",
      "2022-06-30 00:54:31,247 - NCNet pretrain, Epoch [66 / 300]: loss 0.4363, training auc: 0.5923, val_auc 0.6247, test auc 0.6047\n",
      "2022-06-30 00:54:31,380 - NCNet pretrain, Epoch [67 / 300]: loss 0.4393, training auc: 0.5991, val_auc 0.6326, test auc 0.6125\n",
      "2022-06-30 00:54:31,501 - NCNet pretrain, Epoch [68 / 300]: loss 0.4264, training auc: 0.6385, val_auc 0.6325\n",
      "2022-06-30 00:54:31,623 - NCNet pretrain, Epoch [69 / 300]: loss 0.4238, training auc: 0.6286, val_auc 0.6312\n",
      "2022-06-30 00:54:31,750 - NCNet pretrain, Epoch [70 / 300]: loss 0.4138, training auc: 0.6464, val_auc 0.6338, test auc 0.6105\n",
      "2022-06-30 00:54:31,889 - NCNet pretrain, Epoch [71 / 300]: loss 0.4286, training auc: 0.6123, val_auc 0.6491, test auc 0.6279\n",
      "2022-06-30 00:54:32,023 - NCNet pretrain, Epoch [72 / 300]: loss 0.4189, training auc: 0.6405, val_auc 0.6618, test auc 0.6424\n",
      "2022-06-30 00:54:32,156 - NCNet pretrain, Epoch [73 / 300]: loss 0.4109, training auc: 0.6703, val_auc 0.6678, test auc 0.6490\n",
      "2022-06-30 00:54:32,278 - NCNet pretrain, Epoch [74 / 300]: loss 0.4108, training auc: 0.6756, val_auc 0.6651\n",
      "2022-06-30 00:54:32,399 - NCNet pretrain, Epoch [75 / 300]: loss 0.4110, training auc: 0.6546, val_auc 0.6622\n",
      "2022-06-30 00:54:32,541 - NCNet pretrain, Epoch [76 / 300]: loss 0.4173, training auc: 0.6380, val_auc 0.6716, test auc 0.6497\n",
      "2022-06-30 00:54:32,673 - NCNet pretrain, Epoch [77 / 300]: loss 0.4035, training auc: 0.6763, val_auc 0.6848, test auc 0.6664\n",
      "2022-06-30 00:54:32,804 - NCNet pretrain, Epoch [78 / 300]: loss 0.3987, training auc: 0.6845, val_auc 0.6947, test auc 0.6797\n",
      "2022-06-30 00:54:32,925 - NCNet pretrain, Epoch [79 / 300]: loss 0.4056, training auc: 0.7011, val_auc 0.6929\n",
      "2022-06-30 00:54:33,045 - NCNet pretrain, Epoch [80 / 300]: loss 0.4017, training auc: 0.6862, val_auc 0.6861\n",
      "2022-06-30 00:54:33,164 - NCNet pretrain, Epoch [81 / 300]: loss 0.4039, training auc: 0.6704, val_auc 0.6924\n",
      "2022-06-30 00:54:33,294 - NCNet pretrain, Epoch [82 / 300]: loss 0.4068, training auc: 0.6688, val_auc 0.7048, test auc 0.6912\n",
      "2022-06-30 00:54:33,426 - NCNet pretrain, Epoch [83 / 300]: loss 0.3993, training auc: 0.6899, val_auc 0.7096, test auc 0.6983\n",
      "2022-06-30 00:54:33,546 - NCNet pretrain, Epoch [84 / 300]: loss 0.3975, training auc: 0.6973, val_auc 0.7077\n",
      "2022-06-30 00:54:33,675 - NCNet pretrain, Epoch [85 / 300]: loss 0.3872, training auc: 0.7116, val_auc 0.7036\n",
      "2022-06-30 00:54:33,796 - NCNet pretrain, Epoch [86 / 300]: loss 0.3947, training auc: 0.6995, val_auc 0.7079\n",
      "2022-06-30 00:54:33,929 - NCNet pretrain, Epoch [87 / 300]: loss 0.3932, training auc: 0.7034, val_auc 0.7146, test auc 0.7053\n",
      "2022-06-30 00:54:34,082 - NCNet pretrain, Epoch [88 / 300]: loss 0.3851, training auc: 0.7171, val_auc 0.7167, test auc 0.7088\n",
      "2022-06-30 00:54:34,202 - NCNet pretrain, Epoch [89 / 300]: loss 0.3954, training auc: 0.7022, val_auc 0.7164\n",
      "2022-06-30 00:54:34,323 - NCNet pretrain, Epoch [90 / 300]: loss 0.3913, training auc: 0.7166, val_auc 0.7155\n",
      "2022-06-30 00:54:34,444 - NCNet pretrain, Epoch [91 / 300]: loss 0.3868, training auc: 0.7021, val_auc 0.7163\n",
      "2022-06-30 00:54:34,576 - NCNet pretrain, Epoch [92 / 300]: loss 0.3852, training auc: 0.7180, val_auc 0.7229, test auc 0.7179\n",
      "2022-06-30 00:54:34,712 - NCNet pretrain, Epoch [93 / 300]: loss 0.3900, training auc: 0.7082, val_auc 0.7263, test auc 0.7227\n",
      "2022-06-30 00:54:34,830 - NCNet pretrain, Epoch [94 / 300]: loss 0.3800, training auc: 0.7390, val_auc 0.7173\n",
      "2022-06-30 00:54:34,948 - NCNet pretrain, Epoch [95 / 300]: loss 0.3854, training auc: 0.7147, val_auc 0.7185\n",
      "2022-06-30 00:54:35,077 - NCNet pretrain, Epoch [96 / 300]: loss 0.3732, training auc: 0.7319, val_auc 0.7296, test auc 0.7276\n",
      "2022-06-30 00:54:35,207 - NCNet pretrain, Epoch [97 / 300]: loss 0.3711, training auc: 0.7466, val_auc 0.7316, test auc 0.7299\n",
      "2022-06-30 00:54:35,324 - NCNet pretrain, Epoch [98 / 300]: loss 0.3756, training auc: 0.7320, val_auc 0.7262\n",
      "2022-06-30 00:54:35,442 - NCNet pretrain, Epoch [99 / 300]: loss 0.3759, training auc: 0.7407, val_auc 0.7197\n",
      "2022-06-30 00:54:35,561 - NCNet pretrain, Epoch [100 / 300]: loss 0.3814, training auc: 0.7225, val_auc 0.7259\n",
      "2022-06-30 00:54:35,689 - NCNet pretrain, Epoch [101 / 300]: loss 0.3670, training auc: 0.7473, val_auc 0.7387, test auc 0.7384\n",
      "2022-06-30 00:54:35,808 - NCNet pretrain, Epoch [102 / 300]: loss 0.3622, training auc: 0.7697, val_auc 0.7382\n",
      "2022-06-30 00:54:35,929 - NCNet pretrain, Epoch [103 / 300]: loss 0.3749, training auc: 0.7530, val_auc 0.7322\n",
      "2022-06-30 00:54:36,047 - NCNet pretrain, Epoch [104 / 300]: loss 0.3822, training auc: 0.7192, val_auc 0.7330\n",
      "2022-06-30 00:54:36,177 - NCNet pretrain, Epoch [105 / 300]: loss 0.3714, training auc: 0.7365, val_auc 0.7397, test auc 0.7408\n",
      "2022-06-30 00:54:36,306 - NCNet pretrain, Epoch [106 / 300]: loss 0.3617, training auc: 0.7601, val_auc 0.7420, test auc 0.7439\n",
      "2022-06-30 00:54:36,427 - NCNet pretrain, Epoch [107 / 300]: loss 0.3780, training auc: 0.7317, val_auc 0.7388\n",
      "2022-06-30 00:54:36,546 - NCNet pretrain, Epoch [108 / 300]: loss 0.3694, training auc: 0.7527, val_auc 0.7381\n",
      "2022-06-30 00:54:36,664 - NCNet pretrain, Epoch [109 / 300]: loss 0.3590, training auc: 0.7657, val_auc 0.7356\n",
      "2022-06-30 00:54:36,798 - NCNet pretrain, Epoch [110 / 300]: loss 0.3688, training auc: 0.7552, val_auc 0.7429, test auc 0.7474\n",
      "2022-06-30 00:54:36,932 - NCNet pretrain, Epoch [111 / 300]: loss 0.3602, training auc: 0.7603, val_auc 0.7453, test auc 0.7506\n",
      "2022-06-30 00:54:37,050 - NCNet pretrain, Epoch [112 / 300]: loss 0.3666, training auc: 0.7640, val_auc 0.7401\n",
      "2022-06-30 00:54:37,168 - NCNet pretrain, Epoch [113 / 300]: loss 0.3696, training auc: 0.7447, val_auc 0.7385\n",
      "2022-06-30 00:54:37,286 - NCNet pretrain, Epoch [114 / 300]: loss 0.3568, training auc: 0.7636, val_auc 0.7429\n",
      "2022-06-30 00:54:37,415 - NCNet pretrain, Epoch [115 / 300]: loss 0.3643, training auc: 0.7593, val_auc 0.7463, test auc 0.7529\n",
      "2022-06-30 00:54:37,533 - NCNet pretrain, Epoch [116 / 300]: loss 0.3517, training auc: 0.7810, val_auc 0.7458\n",
      "2022-06-30 00:54:37,651 - NCNet pretrain, Epoch [117 / 300]: loss 0.3547, training auc: 0.7712, val_auc 0.7460\n",
      "2022-06-30 00:54:37,780 - NCNet pretrain, Epoch [118 / 300]: loss 0.3541, training auc: 0.7756, val_auc 0.7484, test auc 0.7559\n",
      "2022-06-30 00:54:37,898 - NCNet pretrain, Epoch [119 / 300]: loss 0.3523, training auc: 0.7865, val_auc 0.7441\n",
      "2022-06-30 00:54:38,020 - NCNet pretrain, Epoch [120 / 300]: loss 0.3495, training auc: 0.7697, val_auc 0.7446\n",
      "2022-06-30 00:54:38,138 - NCNet pretrain, Epoch [121 / 300]: loss 0.3504, training auc: 0.7784, val_auc 0.7473\n",
      "2022-06-30 00:54:38,256 - NCNet pretrain, Epoch [122 / 300]: loss 0.3508, training auc: 0.7766, val_auc 0.7477\n",
      "2022-06-30 00:54:38,375 - NCNet pretrain, Epoch [123 / 300]: loss 0.3529, training auc: 0.7771, val_auc 0.7451\n",
      "2022-06-30 00:54:38,504 - NCNet pretrain, Epoch [124 / 300]: loss 0.3588, training auc: 0.7691, val_auc 0.7486, test auc 0.7577\n",
      "2022-06-30 00:54:38,635 - NCNet pretrain, Epoch [125 / 300]: loss 0.3476, training auc: 0.7839, val_auc 0.7520, test auc 0.7636\n",
      "2022-06-30 00:54:38,756 - NCNet pretrain, Epoch [126 / 300]: loss 0.3432, training auc: 0.7976, val_auc 0.7461\n",
      "2022-06-30 00:54:38,877 - NCNet pretrain, Epoch [127 / 300]: loss 0.3462, training auc: 0.7830, val_auc 0.7498\n",
      "2022-06-30 00:54:39,001 - NCNet pretrain, Epoch [128 / 300]: loss 0.3391, training auc: 0.7994, val_auc 0.7498\n",
      "2022-06-30 00:54:39,130 - NCNet pretrain, Epoch [129 / 300]: loss 0.3520, training auc: 0.7786, val_auc 0.7523, test auc 0.7644\n",
      "2022-06-30 00:54:39,279 - NCNet pretrain, Epoch [130 / 300]: loss 0.3485, training auc: 0.7896, val_auc 0.7538, test auc 0.7665\n",
      "2022-06-30 00:54:39,398 - NCNet pretrain, Epoch [131 / 300]: loss 0.3487, training auc: 0.7955, val_auc 0.7430\n",
      "2022-06-30 00:54:39,537 - NCNet pretrain, Epoch [132 / 300]: loss 0.3553, training auc: 0.7731, val_auc 0.7536\n",
      "2022-06-30 00:54:39,666 - NCNet pretrain, Epoch [133 / 300]: loss 0.3356, training auc: 0.8075, val_auc 0.7604, test auc 0.7764\n",
      "2022-06-30 00:54:39,785 - NCNet pretrain, Epoch [134 / 300]: loss 0.3525, training auc: 0.8155, val_auc 0.7456\n",
      "2022-06-30 00:54:39,903 - NCNet pretrain, Epoch [135 / 300]: loss 0.3400, training auc: 0.7999, val_auc 0.7481\n",
      "2022-06-30 00:54:40,034 - NCNet pretrain, Epoch [136 / 300]: loss 0.3474, training auc: 0.7949, val_auc 0.7614, test auc 0.7779\n",
      "2022-06-30 00:54:40,160 - NCNet pretrain, Epoch [137 / 300]: loss 0.3450, training auc: 0.8263, val_auc 0.7557\n",
      "2022-06-30 00:54:40,279 - NCNet pretrain, Epoch [138 / 300]: loss 0.3422, training auc: 0.7935, val_auc 0.7452\n",
      "2022-06-30 00:54:40,418 - NCNet pretrain, Epoch [139 / 300]: loss 0.3401, training auc: 0.7997, val_auc 0.7521\n",
      "2022-06-30 00:54:40,539 - NCNet pretrain, Epoch [140 / 300]: loss 0.3325, training auc: 0.8100, val_auc 0.7608\n",
      "2022-06-30 00:54:40,659 - NCNet pretrain, Epoch [141 / 300]: loss 0.3408, training auc: 0.8246, val_auc 0.7575\n",
      "2022-06-30 00:54:40,779 - NCNet pretrain, Epoch [142 / 300]: loss 0.3267, training auc: 0.8222, val_auc 0.7486\n",
      "2022-06-30 00:54:40,898 - NCNet pretrain, Epoch [143 / 300]: loss 0.3404, training auc: 0.7990, val_auc 0.7520\n",
      "2022-06-30 00:54:41,021 - NCNet pretrain, Epoch [144 / 300]: loss 0.3419, training auc: 0.8015, val_auc 0.7597\n",
      "2022-06-30 00:54:41,140 - NCNet pretrain, Epoch [145 / 300]: loss 0.3363, training auc: 0.8248, val_auc 0.7560\n",
      "2022-06-30 00:54:41,258 - NCNet pretrain, Epoch [146 / 300]: loss 0.3454, training auc: 0.8013, val_auc 0.7490\n",
      "2022-06-30 00:54:41,376 - NCNet pretrain, Epoch [147 / 300]: loss 0.3320, training auc: 0.8148, val_auc 0.7525\n",
      "2022-06-30 00:54:41,505 - NCNet pretrain, Epoch [148 / 300]: loss 0.3459, training auc: 0.7906, val_auc 0.7621, test auc 0.7816\n",
      "2022-06-30 00:54:41,624 - NCNet pretrain, Epoch [149 / 300]: loss 0.3628, training auc: 0.7933, val_auc 0.7562\n",
      "2022-06-30 00:54:41,749 - NCNet pretrain, Epoch [150 / 300]: loss 0.3366, training auc: 0.8053, val_auc 0.7504\n",
      "2022-06-30 00:54:41,868 - NCNet pretrain, Epoch [151 / 300]: loss 0.3327, training auc: 0.8064, val_auc 0.7552\n",
      "2022-06-30 00:54:41,987 - NCNet pretrain, Epoch [152 / 300]: loss 0.3367, training auc: 0.8096, val_auc 0.7617\n",
      "2022-06-30 00:54:42,106 - NCNet pretrain, Epoch [153 / 300]: loss 0.3372, training auc: 0.8237, val_auc 0.7555\n",
      "2022-06-30 00:54:42,224 - NCNet pretrain, Epoch [154 / 300]: loss 0.3449, training auc: 0.7982, val_auc 0.7486\n",
      "2022-06-30 00:54:42,342 - NCNet pretrain, Epoch [155 / 300]: loss 0.3296, training auc: 0.8111, val_auc 0.7537\n",
      "2022-06-30 00:54:42,461 - NCNet pretrain, Epoch [156 / 300]: loss 0.3265, training auc: 0.8126, val_auc 0.7595\n",
      "2022-06-30 00:54:42,579 - NCNet pretrain, Epoch [157 / 300]: loss 0.3301, training auc: 0.8308, val_auc 0.7575\n",
      "2022-06-30 00:54:42,698 - NCNet pretrain, Epoch [158 / 300]: loss 0.3372, training auc: 0.8104, val_auc 0.7461\n",
      "2022-06-30 00:54:42,839 - NCNet pretrain, Epoch [159 / 300]: loss 0.3359, training auc: 0.8092, val_auc 0.7518\n",
      "2022-06-30 00:54:42,978 - NCNet pretrain, Epoch [160 / 300]: loss 0.3411, training auc: 0.7944, val_auc 0.7606\n",
      "2022-06-30 00:54:43,120 - NCNet pretrain, Epoch [161 / 300]: loss 0.3474, training auc: 0.8142, val_auc 0.7565\n",
      "2022-06-30 00:54:43,242 - NCNet pretrain, Epoch [162 / 300]: loss 0.3202, training auc: 0.8296, val_auc 0.7472\n",
      "2022-06-30 00:54:43,378 - NCNet pretrain, Epoch [163 / 300]: loss 0.3344, training auc: 0.8036, val_auc 0.7546\n",
      "2022-06-30 00:54:43,515 - NCNet pretrain, Epoch [164 / 300]: loss 0.3261, training auc: 0.8146, val_auc 0.7604\n",
      "2022-06-30 00:54:43,634 - NCNet pretrain, Epoch [165 / 300]: loss 0.3467, training auc: 0.8131, val_auc 0.7561\n",
      "2022-06-30 00:54:43,768 - NCNet pretrain, Epoch [166 / 300]: loss 0.3318, training auc: 0.8143, val_auc 0.7454\n",
      "2022-06-30 00:54:43,885 - NCNet pretrain, Epoch [167 / 300]: loss 0.3443, training auc: 0.8002, val_auc 0.7543\n",
      "2022-06-30 00:54:44,004 - NCNet pretrain, Epoch [168 / 300]: loss 0.3232, training auc: 0.8188, val_auc 0.7591\n",
      "2022-06-30 00:54:44,122 - NCNet pretrain, Epoch [169 / 300]: loss 0.3231, training auc: 0.8314, val_auc 0.7579\n",
      "2022-06-30 00:54:44,259 - NCNet pretrain, Epoch [170 / 300]: loss 0.3246, training auc: 0.8244, val_auc 0.7516\n",
      "2022-06-30 00:54:44,382 - NCNet pretrain, Epoch [171 / 300]: loss 0.3255, training auc: 0.8154, val_auc 0.7498\n",
      "2022-06-30 00:54:44,507 - NCNet pretrain, Epoch [172 / 300]: loss 0.3270, training auc: 0.8201, val_auc 0.7584\n",
      "2022-06-30 00:54:44,644 - NCNet pretrain, Epoch [173 / 300]: loss 0.3198, training auc: 0.8329, val_auc 0.7601\n",
      "2022-06-30 00:54:44,774 - NCNet pretrain, Epoch [174 / 300]: loss 0.3191, training auc: 0.8478, val_auc 0.7507\n",
      "2022-06-30 00:54:44,894 - NCNet pretrain, Epoch [175 / 300]: loss 0.3261, training auc: 0.8167, val_auc 0.7506\n",
      "2022-06-30 00:54:45,013 - NCNet pretrain, Epoch [176 / 300]: loss 0.3120, training auc: 0.8322, val_auc 0.7592\n",
      "2022-06-30 00:54:45,132 - NCNet pretrain, Epoch [177 / 300]: loss 0.3301, training auc: 0.8266, val_auc 0.7592\n",
      "2022-06-30 00:54:45,254 - NCNet pretrain, Epoch [178 / 300]: loss 0.3186, training auc: 0.8383, val_auc 0.7514\n",
      "2022-06-30 00:54:45,376 - NCNet pretrain, Epoch [179 / 300]: loss 0.3262, training auc: 0.8174, val_auc 0.7529\n",
      "2022-06-30 00:54:45,494 - NCNet pretrain, Epoch [180 / 300]: loss 0.3207, training auc: 0.8233, val_auc 0.7608\n",
      "2022-06-30 00:54:45,613 - NCNet pretrain, Epoch [181 / 300]: loss 0.3303, training auc: 0.8338, val_auc 0.7582\n",
      "2022-06-30 00:54:45,731 - NCNet pretrain, Epoch [182 / 300]: loss 0.3331, training auc: 0.8082, val_auc 0.7503\n",
      "2022-06-30 00:54:45,851 - NCNet pretrain, Epoch [183 / 300]: loss 0.3239, training auc: 0.8260, val_auc 0.7542\n",
      "2022-06-30 00:54:45,969 - NCNet pretrain, Epoch [184 / 300]: loss 0.3082, training auc: 0.8411, val_auc 0.7603\n",
      "2022-06-30 00:54:46,089 - NCNet pretrain, Epoch [185 / 300]: loss 0.3186, training auc: 0.8337, val_auc 0.7611\n",
      "2022-06-30 00:54:46,207 - NCNet pretrain, Epoch [186 / 300]: loss 0.3287, training auc: 0.8215, val_auc 0.7552\n",
      "2022-06-30 00:54:46,326 - NCNet pretrain, Epoch [187 / 300]: loss 0.3303, training auc: 0.8083, val_auc 0.7505\n",
      "2022-06-30 00:54:46,444 - NCNet pretrain, Epoch [188 / 300]: loss 0.3305, training auc: 0.8205, val_auc 0.7578\n",
      "2022-06-30 00:54:46,577 - NCNet pretrain, Epoch [189 / 300]: loss 0.3313, training auc: 0.8099, val_auc 0.7609\n",
      "2022-06-30 00:54:46,695 - NCNet pretrain, Epoch [190 / 300]: loss 0.3245, training auc: 0.8264, val_auc 0.7584\n",
      "2022-06-30 00:54:46,814 - NCNet pretrain, Epoch [191 / 300]: loss 0.3298, training auc: 0.8171, val_auc 0.7554\n",
      "2022-06-30 00:54:46,933 - NCNet pretrain, Epoch [192 / 300]: loss 0.3118, training auc: 0.8290, val_auc 0.7558\n",
      "2022-06-30 00:54:47,058 - NCNet pretrain, Epoch [193 / 300]: loss 0.3193, training auc: 0.8265, val_auc 0.7576\n",
      "2022-06-30 00:54:47,177 - NCNet pretrain, Epoch [194 / 300]: loss 0.3166, training auc: 0.8344, val_auc 0.7559\n",
      "2022-06-30 00:54:47,296 - NCNet pretrain, Epoch [195 / 300]: loss 0.3225, training auc: 0.8281, val_auc 0.7567\n",
      "2022-06-30 00:54:47,419 - NCNet pretrain, Epoch [196 / 300]: loss 0.3116, training auc: 0.8408, val_auc 0.7592\n",
      "2022-06-30 00:54:47,541 - NCNet pretrain, Epoch [197 / 300]: loss 0.3176, training auc: 0.8328, val_auc 0.7582\n",
      "2022-06-30 00:54:47,659 - NCNet pretrain, Epoch [198 / 300]: loss 0.3096, training auc: 0.8426, val_auc 0.7516\n",
      "2022-06-30 00:54:47,660 - Early stop!\n",
      "2022-06-30 00:54:47,663 - Best Test Results: auc 0.7816, ap 0.3835, f1 0.2663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 252271.71it/s]\n",
      "2022-06-30 00:54:48,795 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:54:48,975 - NCNet pretrain, Epoch [1 / 300]: loss 0.5869, training auc: 0.5537, val_auc 0.4616, test auc 0.4315\n",
      "2022-06-30 00:54:49,094 - NCNet pretrain, Epoch [2 / 300]: loss 0.4500, training auc: 0.4336, val_auc 0.4614\n",
      "2022-06-30 00:54:49,215 - NCNet pretrain, Epoch [3 / 300]: loss 0.4639, training auc: 0.4493, val_auc 0.4608\n",
      "2022-06-30 00:54:49,344 - NCNet pretrain, Epoch [4 / 300]: loss 0.4802, training auc: 0.4061, val_auc 0.4623, test auc 0.4348\n",
      "2022-06-30 00:54:49,474 - NCNet pretrain, Epoch [5 / 300]: loss 0.4509, training auc: 0.4488, val_auc 0.4630, test auc 0.4370\n",
      "2022-06-30 00:54:49,607 - NCNet pretrain, Epoch [6 / 300]: loss 0.4483, training auc: 0.4305, val_auc 0.4656, test auc 0.4416\n",
      "2022-06-30 00:54:49,738 - NCNet pretrain, Epoch [7 / 300]: loss 0.4537, training auc: 0.3944, val_auc 0.4706, test auc 0.4483\n",
      "2022-06-30 00:54:49,867 - NCNet pretrain, Epoch [8 / 300]: loss 0.4520, training auc: 0.4427, val_auc 0.4741, test auc 0.4525\n",
      "2022-06-30 00:54:49,995 - NCNet pretrain, Epoch [9 / 300]: loss 0.4573, training auc: 0.3891, val_auc 0.4751, test auc 0.4532\n",
      "2022-06-30 00:54:50,115 - NCNet pretrain, Epoch [10 / 300]: loss 0.4479, training auc: 0.4513, val_auc 0.4744\n",
      "2022-06-30 00:54:50,233 - NCNet pretrain, Epoch [11 / 300]: loss 0.4425, training auc: 0.4566, val_auc 0.4739\n",
      "2022-06-30 00:54:50,359 - NCNet pretrain, Epoch [12 / 300]: loss 0.4503, training auc: 0.3954, val_auc 0.4744\n",
      "2022-06-30 00:54:50,482 - NCNet pretrain, Epoch [13 / 300]: loss 0.4388, training auc: 0.4750, val_auc 0.4753, test auc 0.4500\n",
      "2022-06-30 00:54:50,626 - NCNet pretrain, Epoch [14 / 300]: loss 0.4481, training auc: 0.4189, val_auc 0.4769, test auc 0.4517\n",
      "2022-06-30 00:54:50,759 - NCNet pretrain, Epoch [15 / 300]: loss 0.4395, training auc: 0.4672, val_auc 0.4789, test auc 0.4541\n",
      "2022-06-30 00:54:50,889 - NCNet pretrain, Epoch [16 / 300]: loss 0.4472, training auc: 0.4077, val_auc 0.4809, test auc 0.4567\n",
      "2022-06-30 00:54:51,019 - NCNet pretrain, Epoch [17 / 300]: loss 0.4434, training auc: 0.4327, val_auc 0.4831, test auc 0.4598\n",
      "2022-06-30 00:54:51,148 - NCNet pretrain, Epoch [18 / 300]: loss 0.4370, training auc: 0.4767, val_auc 0.4852, test auc 0.4623\n",
      "2022-06-30 00:54:51,278 - NCNet pretrain, Epoch [19 / 300]: loss 0.4420, training auc: 0.4389, val_auc 0.4871, test auc 0.4644\n",
      "2022-06-30 00:54:51,408 - NCNet pretrain, Epoch [20 / 300]: loss 0.4413, training auc: 0.4287, val_auc 0.4891, test auc 0.4660\n",
      "2022-06-30 00:54:51,539 - NCNet pretrain, Epoch [21 / 300]: loss 0.4417, training auc: 0.4250, val_auc 0.4912, test auc 0.4677\n",
      "2022-06-30 00:54:51,668 - NCNet pretrain, Epoch [22 / 300]: loss 0.4379, training auc: 0.4547, val_auc 0.4929, test auc 0.4688\n",
      "2022-06-30 00:54:51,807 - NCNet pretrain, Epoch [23 / 300]: loss 0.4397, training auc: 0.4435, val_auc 0.4949, test auc 0.4703\n",
      "2022-06-30 00:54:51,935 - NCNet pretrain, Epoch [24 / 300]: loss 0.4355, training auc: 0.4544, val_auc 0.4973, test auc 0.4726\n",
      "2022-06-30 00:54:52,066 - NCNet pretrain, Epoch [25 / 300]: loss 0.4302, training auc: 0.4910, val_auc 0.4999, test auc 0.4749\n",
      "2022-06-30 00:54:52,194 - NCNet pretrain, Epoch [26 / 300]: loss 0.4288, training auc: 0.4922, val_auc 0.5030, test auc 0.4781\n",
      "2022-06-30 00:54:52,324 - NCNet pretrain, Epoch [27 / 300]: loss 0.4303, training auc: 0.4955, val_auc 0.5069, test auc 0.4823\n",
      "2022-06-30 00:54:52,452 - NCNet pretrain, Epoch [28 / 300]: loss 0.4291, training auc: 0.4972, val_auc 0.5122, test auc 0.4879\n",
      "2022-06-30 00:54:52,583 - NCNet pretrain, Epoch [29 / 300]: loss 0.4255, training auc: 0.5157, val_auc 0.5188, test auc 0.4946\n",
      "2022-06-30 00:54:52,712 - NCNet pretrain, Epoch [30 / 300]: loss 0.4289, training auc: 0.4850, val_auc 0.5270, test auc 0.5035\n",
      "2022-06-30 00:54:52,841 - NCNet pretrain, Epoch [31 / 300]: loss 0.4223, training auc: 0.5328, val_auc 0.5359, test auc 0.5134\n",
      "2022-06-30 00:54:52,971 - NCNet pretrain, Epoch [32 / 300]: loss 0.4228, training auc: 0.5455, val_auc 0.5442, test auc 0.5216\n",
      "2022-06-30 00:54:53,101 - NCNet pretrain, Epoch [33 / 300]: loss 0.4241, training auc: 0.5144, val_auc 0.5546, test auc 0.5319\n",
      "2022-06-30 00:54:53,231 - NCNet pretrain, Epoch [34 / 300]: loss 0.4180, training auc: 0.5765, val_auc 0.5642, test auc 0.5407\n",
      "2022-06-30 00:54:53,361 - NCNet pretrain, Epoch [35 / 300]: loss 0.4195, training auc: 0.5500, val_auc 0.5799, test auc 0.5556\n",
      "2022-06-30 00:54:53,494 - NCNet pretrain, Epoch [36 / 300]: loss 0.4164, training auc: 0.5746, val_auc 0.6037, test auc 0.5779\n",
      "2022-06-30 00:54:53,624 - NCNet pretrain, Epoch [37 / 300]: loss 0.4172, training auc: 0.5585, val_auc 0.6399, test auc 0.6121\n",
      "2022-06-30 00:54:53,752 - NCNet pretrain, Epoch [38 / 300]: loss 0.4130, training auc: 0.6000, val_auc 0.6652, test auc 0.6364\n",
      "2022-06-30 00:54:53,877 - NCNet pretrain, Epoch [39 / 300]: loss 0.4064, training auc: 0.6474, val_auc 0.6626\n",
      "2022-06-30 00:54:54,006 - NCNet pretrain, Epoch [40 / 300]: loss 0.3972, training auc: 0.6661, val_auc 0.6669, test auc 0.6371\n",
      "2022-06-30 00:54:54,138 - NCNet pretrain, Epoch [41 / 300]: loss 0.3958, training auc: 0.6646, val_auc 0.6883, test auc 0.6622\n",
      "2022-06-30 00:54:54,266 - NCNet pretrain, Epoch [42 / 300]: loss 0.3984, training auc: 0.6542, val_auc 0.7082, test auc 0.6868\n",
      "2022-06-30 00:54:54,395 - NCNet pretrain, Epoch [43 / 300]: loss 0.3939, training auc: 0.6997, val_auc 0.7103, test auc 0.6891\n",
      "2022-06-30 00:54:54,524 - NCNet pretrain, Epoch [44 / 300]: loss 0.3991, training auc: 0.6657, val_auc 0.7168, test auc 0.6977\n",
      "2022-06-30 00:54:54,647 - NCNet pretrain, Epoch [45 / 300]: loss 0.3790, training auc: 0.7170, val_auc 0.7170, test auc 0.6973\n",
      "2022-06-30 00:54:54,776 - NCNet pretrain, Epoch [46 / 300]: loss 0.3934, training auc: 0.6717, val_auc 0.7339, test auc 0.7221\n",
      "2022-06-30 00:54:54,895 - NCNet pretrain, Epoch [47 / 300]: loss 0.3817, training auc: 0.7210, val_auc 0.7186\n",
      "2022-06-30 00:54:55,013 - NCNet pretrain, Epoch [48 / 300]: loss 0.3854, training auc: 0.6815, val_auc 0.7324\n",
      "2022-06-30 00:54:55,144 - NCNet pretrain, Epoch [49 / 300]: loss 0.3801, training auc: 0.7101, val_auc 0.7431, test auc 0.7346\n",
      "2022-06-30 00:54:55,263 - NCNet pretrain, Epoch [50 / 300]: loss 0.3733, training auc: 0.7306, val_auc 0.7379\n",
      "2022-06-30 00:54:55,381 - NCNet pretrain, Epoch [51 / 300]: loss 0.3720, training auc: 0.7185, val_auc 0.7372\n",
      "2022-06-30 00:54:55,511 - NCNet pretrain, Epoch [52 / 300]: loss 0.3717, training auc: 0.7261, val_auc 0.7486, test auc 0.7397\n",
      "2022-06-30 00:54:55,665 - NCNet pretrain, Epoch [53 / 300]: loss 0.3643, training auc: 0.7436, val_auc 0.7557, test auc 0.7478\n",
      "2022-06-30 00:54:55,783 - NCNet pretrain, Epoch [54 / 300]: loss 0.3676, training auc: 0.7477, val_auc 0.7507\n",
      "2022-06-30 00:54:55,921 - NCNet pretrain, Epoch [55 / 300]: loss 0.3608, training auc: 0.7590, val_auc 0.7460\n",
      "2022-06-30 00:54:56,054 - NCNet pretrain, Epoch [56 / 300]: loss 0.3636, training auc: 0.7440, val_auc 0.7658, test auc 0.7605\n",
      "2022-06-30 00:54:56,179 - NCNet pretrain, Epoch [57 / 300]: loss 0.3640, training auc: 0.7705, val_auc 0.7606\n",
      "2022-06-30 00:54:56,298 - NCNet pretrain, Epoch [58 / 300]: loss 0.3684, training auc: 0.7501, val_auc 0.7497\n",
      "2022-06-30 00:54:56,415 - NCNet pretrain, Epoch [59 / 300]: loss 0.3522, training auc: 0.7580, val_auc 0.7648\n",
      "2022-06-30 00:54:56,553 - NCNet pretrain, Epoch [60 / 300]: loss 0.3517, training auc: 0.7748, val_auc 0.7628\n",
      "2022-06-30 00:54:56,693 - NCNet pretrain, Epoch [61 / 300]: loss 0.3538, training auc: 0.7655, val_auc 0.7639\n",
      "2022-06-30 00:54:56,842 - NCNet pretrain, Epoch [62 / 300]: loss 0.3512, training auc: 0.7649, val_auc 0.7693, test auc 0.7684\n",
      "2022-06-30 00:54:56,960 - NCNet pretrain, Epoch [63 / 300]: loss 0.3433, training auc: 0.7929, val_auc 0.7650\n",
      "2022-06-30 00:54:57,089 - NCNet pretrain, Epoch [64 / 300]: loss 0.3562, training auc: 0.7705, val_auc 0.7706, test auc 0.7695\n",
      "2022-06-30 00:54:57,208 - NCNet pretrain, Epoch [65 / 300]: loss 0.3436, training auc: 0.7858, val_auc 0.7675\n",
      "2022-06-30 00:54:57,332 - NCNet pretrain, Epoch [66 / 300]: loss 0.3396, training auc: 0.7902, val_auc 0.7708, test auc 0.7686\n",
      "2022-06-30 00:54:57,481 - NCNet pretrain, Epoch [67 / 300]: loss 0.3362, training auc: 0.7933, val_auc 0.7782, test auc 0.7812\n",
      "2022-06-30 00:54:57,621 - NCNet pretrain, Epoch [68 / 300]: loss 0.3403, training auc: 0.7988, val_auc 0.7680\n",
      "2022-06-30 00:54:57,764 - NCNet pretrain, Epoch [69 / 300]: loss 0.3438, training auc: 0.7816, val_auc 0.7696\n",
      "2022-06-30 00:54:57,894 - NCNet pretrain, Epoch [70 / 300]: loss 0.3405, training auc: 0.7832, val_auc 0.7825, test auc 0.7895\n",
      "2022-06-30 00:54:58,012 - NCNet pretrain, Epoch [71 / 300]: loss 0.3530, training auc: 0.8014, val_auc 0.7685\n",
      "2022-06-30 00:54:58,140 - NCNet pretrain, Epoch [72 / 300]: loss 0.3308, training auc: 0.7927, val_auc 0.7654\n",
      "2022-06-30 00:54:58,270 - NCNet pretrain, Epoch [73 / 300]: loss 0.3380, training auc: 0.8016, val_auc 0.7840, test auc 0.7899\n",
      "2022-06-30 00:54:58,399 - NCNet pretrain, Epoch [74 / 300]: loss 0.3464, training auc: 0.8124, val_auc 0.7752\n",
      "2022-06-30 00:54:58,517 - NCNet pretrain, Epoch [75 / 300]: loss 0.3307, training auc: 0.8050, val_auc 0.7614\n",
      "2022-06-30 00:54:58,635 - NCNet pretrain, Epoch [76 / 300]: loss 0.3498, training auc: 0.7780, val_auc 0.7772\n",
      "2022-06-30 00:54:58,764 - NCNet pretrain, Epoch [77 / 300]: loss 0.3441, training auc: 0.7913, val_auc 0.7866, test auc 0.7946\n",
      "2022-06-30 00:54:58,882 - NCNet pretrain, Epoch [78 / 300]: loss 0.3456, training auc: 0.8108, val_auc 0.7774\n",
      "2022-06-30 00:54:58,999 - NCNet pretrain, Epoch [79 / 300]: loss 0.3287, training auc: 0.8146, val_auc 0.7640\n",
      "2022-06-30 00:54:59,117 - NCNet pretrain, Epoch [80 / 300]: loss 0.3455, training auc: 0.7818, val_auc 0.7753\n",
      "2022-06-30 00:54:59,249 - NCNet pretrain, Epoch [81 / 300]: loss 0.3380, training auc: 0.7930, val_auc 0.7869, test auc 0.7949\n",
      "2022-06-30 00:54:59,371 - NCNet pretrain, Epoch [82 / 300]: loss 0.3358, training auc: 0.8197, val_auc 0.7791\n",
      "2022-06-30 00:54:59,509 - NCNet pretrain, Epoch [83 / 300]: loss 0.3245, training auc: 0.8205, val_auc 0.7722\n",
      "2022-06-30 00:54:59,628 - NCNet pretrain, Epoch [84 / 300]: loss 0.3290, training auc: 0.8122, val_auc 0.7780\n",
      "2022-06-30 00:54:59,758 - NCNet pretrain, Epoch [85 / 300]: loss 0.3296, training auc: 0.8045, val_auc 0.7874, test auc 0.7957\n",
      "2022-06-30 00:54:59,878 - NCNet pretrain, Epoch [86 / 300]: loss 0.3326, training auc: 0.8250, val_auc 0.7813\n",
      "2022-06-30 00:54:59,996 - NCNet pretrain, Epoch [87 / 300]: loss 0.3240, training auc: 0.8193, val_auc 0.7716\n",
      "2022-06-30 00:55:00,116 - NCNet pretrain, Epoch [88 / 300]: loss 0.3329, training auc: 0.8073, val_auc 0.7791\n",
      "2022-06-30 00:55:00,251 - NCNet pretrain, Epoch [89 / 300]: loss 0.3252, training auc: 0.8088, val_auc 0.7904, test auc 0.8005\n",
      "2022-06-30 00:55:00,373 - NCNet pretrain, Epoch [90 / 300]: loss 0.3331, training auc: 0.8330, val_auc 0.7821\n",
      "2022-06-30 00:55:00,492 - NCNet pretrain, Epoch [91 / 300]: loss 0.3225, training auc: 0.8248, val_auc 0.7733\n",
      "2022-06-30 00:55:00,611 - NCNet pretrain, Epoch [92 / 300]: loss 0.3263, training auc: 0.8065, val_auc 0.7799\n",
      "2022-06-30 00:55:00,729 - NCNet pretrain, Epoch [93 / 300]: loss 0.3220, training auc: 0.8153, val_auc 0.7895\n",
      "2022-06-30 00:55:00,866 - NCNet pretrain, Epoch [94 / 300]: loss 0.3308, training auc: 0.8305, val_auc 0.7827\n",
      "2022-06-30 00:55:00,985 - NCNet pretrain, Epoch [95 / 300]: loss 0.3195, training auc: 0.8279, val_auc 0.7749\n",
      "2022-06-30 00:55:01,108 - NCNet pretrain, Epoch [96 / 300]: loss 0.3186, training auc: 0.8190, val_auc 0.7814\n",
      "2022-06-30 00:55:01,240 - NCNet pretrain, Epoch [97 / 300]: loss 0.3179, training auc: 0.8231, val_auc 0.7875\n",
      "2022-06-30 00:55:01,379 - NCNet pretrain, Epoch [98 / 300]: loss 0.3283, training auc: 0.8146, val_auc 0.7869\n",
      "2022-06-30 00:55:01,516 - NCNet pretrain, Epoch [99 / 300]: loss 0.3129, training auc: 0.8411, val_auc 0.7783\n",
      "2022-06-30 00:55:01,642 - NCNet pretrain, Epoch [100 / 300]: loss 0.3202, training auc: 0.8229, val_auc 0.7779\n",
      "2022-06-30 00:55:01,763 - NCNet pretrain, Epoch [101 / 300]: loss 0.3279, training auc: 0.8128, val_auc 0.7878\n",
      "2022-06-30 00:55:01,900 - NCNet pretrain, Epoch [102 / 300]: loss 0.3197, training auc: 0.8361, val_auc 0.7850\n",
      "2022-06-30 00:55:02,022 - NCNet pretrain, Epoch [103 / 300]: loss 0.3106, training auc: 0.8442, val_auc 0.7797\n",
      "2022-06-30 00:55:02,142 - NCNet pretrain, Epoch [104 / 300]: loss 0.3159, training auc: 0.8296, val_auc 0.7847\n",
      "2022-06-30 00:55:02,264 - NCNet pretrain, Epoch [105 / 300]: loss 0.3237, training auc: 0.8223, val_auc 0.7887\n",
      "2022-06-30 00:55:02,385 - NCNet pretrain, Epoch [106 / 300]: loss 0.3174, training auc: 0.8423, val_auc 0.7796\n",
      "2022-06-30 00:55:02,511 - NCNet pretrain, Epoch [107 / 300]: loss 0.3096, training auc: 0.8355, val_auc 0.7757\n",
      "2022-06-30 00:55:02,630 - NCNet pretrain, Epoch [108 / 300]: loss 0.3169, training auc: 0.8238, val_auc 0.7873\n",
      "2022-06-30 00:55:02,750 - NCNet pretrain, Epoch [109 / 300]: loss 0.3168, training auc: 0.8353, val_auc 0.7892\n",
      "2022-06-30 00:55:02,870 - NCNet pretrain, Epoch [110 / 300]: loss 0.3136, training auc: 0.8457, val_auc 0.7800\n",
      "2022-06-30 00:55:02,990 - NCNet pretrain, Epoch [111 / 300]: loss 0.3165, training auc: 0.8239, val_auc 0.7810\n",
      "2022-06-30 00:55:03,110 - NCNet pretrain, Epoch [112 / 300]: loss 0.3145, training auc: 0.8313, val_auc 0.7899\n",
      "2022-06-30 00:55:03,249 - NCNet pretrain, Epoch [113 / 300]: loss 0.3141, training auc: 0.8456, val_auc 0.7839\n",
      "2022-06-30 00:55:03,366 - NCNet pretrain, Epoch [114 / 300]: loss 0.3088, training auc: 0.8337, val_auc 0.7828\n",
      "2022-06-30 00:55:03,485 - NCNet pretrain, Epoch [115 / 300]: loss 0.3010, training auc: 0.8492, val_auc 0.7876\n",
      "2022-06-30 00:55:03,603 - NCNet pretrain, Epoch [116 / 300]: loss 0.3046, training auc: 0.8475, val_auc 0.7871\n",
      "2022-06-30 00:55:03,722 - NCNet pretrain, Epoch [117 / 300]: loss 0.3079, training auc: 0.8355, val_auc 0.7840\n",
      "2022-06-30 00:55:03,840 - NCNet pretrain, Epoch [118 / 300]: loss 0.3044, training auc: 0.8404, val_auc 0.7854\n",
      "2022-06-30 00:55:03,959 - NCNet pretrain, Epoch [119 / 300]: loss 0.3079, training auc: 0.8442, val_auc 0.7897\n",
      "2022-06-30 00:55:04,077 - NCNet pretrain, Epoch [120 / 300]: loss 0.3029, training auc: 0.8455, val_auc 0.7868\n",
      "2022-06-30 00:55:04,198 - NCNet pretrain, Epoch [121 / 300]: loss 0.3005, training auc: 0.8498, val_auc 0.7851\n",
      "2022-06-30 00:55:04,318 - NCNet pretrain, Epoch [122 / 300]: loss 0.3028, training auc: 0.8458, val_auc 0.7877\n",
      "2022-06-30 00:55:04,456 - NCNet pretrain, Epoch [123 / 300]: loss 0.3080, training auc: 0.8405, val_auc 0.7868\n",
      "2022-06-30 00:55:04,580 - NCNet pretrain, Epoch [124 / 300]: loss 0.3028, training auc: 0.8468, val_auc 0.7845\n",
      "2022-06-30 00:55:04,702 - NCNet pretrain, Epoch [125 / 300]: loss 0.3135, training auc: 0.8326, val_auc 0.7902\n",
      "2022-06-30 00:55:04,821 - NCNet pretrain, Epoch [126 / 300]: loss 0.3036, training auc: 0.8525, val_auc 0.7871\n",
      "2022-06-30 00:55:04,939 - NCNet pretrain, Epoch [127 / 300]: loss 0.3003, training auc: 0.8479, val_auc 0.7821\n",
      "2022-06-30 00:55:05,057 - NCNet pretrain, Epoch [128 / 300]: loss 0.3017, training auc: 0.8502, val_auc 0.7893\n",
      "2022-06-30 00:55:05,176 - NCNet pretrain, Epoch [129 / 300]: loss 0.3008, training auc: 0.8518, val_auc 0.7884\n",
      "2022-06-30 00:55:05,294 - NCNet pretrain, Epoch [130 / 300]: loss 0.2974, training auc: 0.8575, val_auc 0.7795\n",
      "2022-06-30 00:55:05,412 - NCNet pretrain, Epoch [131 / 300]: loss 0.3085, training auc: 0.8389, val_auc 0.7891\n",
      "2022-06-30 00:55:05,530 - NCNet pretrain, Epoch [132 / 300]: loss 0.3032, training auc: 0.8488, val_auc 0.7882\n",
      "2022-06-30 00:55:05,648 - NCNet pretrain, Epoch [133 / 300]: loss 0.3036, training auc: 0.8509, val_auc 0.7799\n",
      "2022-06-30 00:55:05,767 - NCNet pretrain, Epoch [134 / 300]: loss 0.3071, training auc: 0.8375, val_auc 0.7864\n",
      "2022-06-30 00:55:05,890 - NCNet pretrain, Epoch [135 / 300]: loss 0.2949, training auc: 0.8515, val_auc 0.7889\n",
      "2022-06-30 00:55:06,008 - NCNet pretrain, Epoch [136 / 300]: loss 0.3052, training auc: 0.8545, val_auc 0.7815\n",
      "2022-06-30 00:55:06,132 - NCNet pretrain, Epoch [137 / 300]: loss 0.3053, training auc: 0.8388, val_auc 0.7845\n",
      "2022-06-30 00:55:06,252 - NCNet pretrain, Epoch [138 / 300]: loss 0.2935, training auc: 0.8504, val_auc 0.7874\n",
      "2022-06-30 00:55:06,374 - NCNet pretrain, Epoch [139 / 300]: loss 0.2921, training auc: 0.8631, val_auc 0.7817\n",
      "2022-06-30 00:55:06,377 - Early stop!\n",
      "2022-06-30 00:55:06,377 - Best Test Results: auc 0.8005, ap 0.4189, f1 0.3135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 251565.01it/s]\n",
      "2022-06-30 00:55:07,515 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:55:07,721 - NCNet pretrain, Epoch [1 / 300]: loss 0.9614, training auc: 0.3978, val_auc 0.4735, test auc 0.4495\n",
      "2022-06-30 00:55:07,839 - NCNet pretrain, Epoch [2 / 300]: loss 0.6709, training auc: 0.5026, val_auc 0.4694\n",
      "2022-06-30 00:55:07,977 - NCNet pretrain, Epoch [3 / 300]: loss 0.5252, training auc: 0.4168, val_auc 0.4655\n",
      "2022-06-30 00:55:08,113 - NCNet pretrain, Epoch [4 / 300]: loss 0.4992, training auc: 0.4208, val_auc 0.4642\n",
      "2022-06-30 00:55:08,252 - NCNet pretrain, Epoch [5 / 300]: loss 0.5412, training auc: 0.4392, val_auc 0.4646\n",
      "2022-06-30 00:55:08,390 - NCNet pretrain, Epoch [6 / 300]: loss 0.5386, training auc: 0.4653, val_auc 0.4660\n",
      "2022-06-30 00:55:08,511 - NCNet pretrain, Epoch [7 / 300]: loss 0.5308, training auc: 0.4417, val_auc 0.4678\n",
      "2022-06-30 00:55:08,633 - NCNet pretrain, Epoch [8 / 300]: loss 0.5060, training auc: 0.4456, val_auc 0.4709\n",
      "2022-06-30 00:55:08,763 - NCNet pretrain, Epoch [9 / 300]: loss 0.5009, training auc: 0.4136, val_auc 0.4758, test auc 0.4532\n",
      "2022-06-30 00:55:08,896 - NCNet pretrain, Epoch [10 / 300]: loss 0.5068, training auc: 0.4032, val_auc 0.4785, test auc 0.4576\n",
      "2022-06-30 00:55:09,023 - NCNet pretrain, Epoch [11 / 300]: loss 0.4972, training auc: 0.5027, val_auc 0.4786, test auc 0.4572\n",
      "2022-06-30 00:55:09,141 - NCNet pretrain, Epoch [12 / 300]: loss 0.4979, training auc: 0.4539, val_auc 0.4781\n",
      "2022-06-30 00:55:09,259 - NCNet pretrain, Epoch [13 / 300]: loss 0.4807, training auc: 0.5018, val_auc 0.4771\n",
      "2022-06-30 00:55:09,378 - NCNet pretrain, Epoch [14 / 300]: loss 0.4833, training auc: 0.4652, val_auc 0.4759\n",
      "2022-06-30 00:55:09,496 - NCNet pretrain, Epoch [15 / 300]: loss 0.4967, training auc: 0.4207, val_auc 0.4761\n",
      "2022-06-30 00:55:09,614 - NCNet pretrain, Epoch [16 / 300]: loss 0.4947, training auc: 0.4221, val_auc 0.4766\n",
      "2022-06-30 00:55:09,752 - NCNet pretrain, Epoch [17 / 300]: loss 0.4857, training auc: 0.4482, val_auc 0.4775\n",
      "2022-06-30 00:55:09,875 - NCNet pretrain, Epoch [18 / 300]: loss 0.4832, training auc: 0.4472, val_auc 0.4789, test auc 0.4547\n",
      "2022-06-30 00:55:09,999 - NCNet pretrain, Epoch [19 / 300]: loss 0.4780, training auc: 0.4706, val_auc 0.4806, test auc 0.4568\n",
      "2022-06-30 00:55:10,130 - NCNet pretrain, Epoch [20 / 300]: loss 0.4914, training auc: 0.4172, val_auc 0.4819, test auc 0.4586\n",
      "2022-06-30 00:55:10,263 - NCNet pretrain, Epoch [21 / 300]: loss 0.4689, training auc: 0.5032, val_auc 0.4831, test auc 0.4599\n",
      "2022-06-30 00:55:10,393 - NCNet pretrain, Epoch [22 / 300]: loss 0.4742, training auc: 0.4716, val_auc 0.4839, test auc 0.4606\n",
      "2022-06-30 00:55:10,522 - NCNet pretrain, Epoch [23 / 300]: loss 0.4689, training auc: 0.4894, val_auc 0.4847, test auc 0.4612\n",
      "2022-06-30 00:55:10,663 - NCNet pretrain, Epoch [24 / 300]: loss 0.4849, training auc: 0.4188, val_auc 0.4857, test auc 0.4618\n",
      "2022-06-30 00:55:10,812 - NCNet pretrain, Epoch [25 / 300]: loss 0.4825, training auc: 0.4251, val_auc 0.4870, test auc 0.4627\n",
      "2022-06-30 00:55:10,942 - NCNet pretrain, Epoch [26 / 300]: loss 0.4872, training auc: 0.4024, val_auc 0.4886, test auc 0.4641\n",
      "2022-06-30 00:55:11,093 - NCNet pretrain, Epoch [27 / 300]: loss 0.4644, training auc: 0.4908, val_auc 0.4901, test auc 0.4655\n",
      "2022-06-30 00:55:11,240 - NCNet pretrain, Epoch [28 / 300]: loss 0.4660, training auc: 0.4801, val_auc 0.4915, test auc 0.4668\n",
      "2022-06-30 00:55:11,369 - NCNet pretrain, Epoch [29 / 300]: loss 0.4622, training auc: 0.4947, val_auc 0.4930, test auc 0.4682\n",
      "2022-06-30 00:55:11,497 - NCNet pretrain, Epoch [30 / 300]: loss 0.4617, training auc: 0.4911, val_auc 0.4945, test auc 0.4696\n",
      "2022-06-30 00:55:11,624 - NCNet pretrain, Epoch [31 / 300]: loss 0.4680, training auc: 0.4703, val_auc 0.4962, test auc 0.4717\n",
      "2022-06-30 00:55:11,753 - NCNet pretrain, Epoch [32 / 300]: loss 0.4755, training auc: 0.4355, val_auc 0.4990, test auc 0.4748\n",
      "2022-06-30 00:55:11,882 - NCNet pretrain, Epoch [33 / 300]: loss 0.4692, training auc: 0.4500, val_auc 0.5022, test auc 0.4785\n",
      "2022-06-30 00:55:12,010 - NCNet pretrain, Epoch [34 / 300]: loss 0.4599, training auc: 0.4886, val_auc 0.5057, test auc 0.4822\n",
      "2022-06-30 00:55:12,140 - NCNet pretrain, Epoch [35 / 300]: loss 0.4567, training auc: 0.5057, val_auc 0.5087, test auc 0.4852\n",
      "2022-06-30 00:55:12,269 - NCNet pretrain, Epoch [36 / 300]: loss 0.4616, training auc: 0.4649, val_auc 0.5117, test auc 0.4884\n",
      "2022-06-30 00:55:12,401 - NCNet pretrain, Epoch [37 / 300]: loss 0.4617, training auc: 0.4746, val_auc 0.5154, test auc 0.4918\n",
      "2022-06-30 00:55:12,530 - NCNet pretrain, Epoch [38 / 300]: loss 0.4526, training auc: 0.4983, val_auc 0.5186, test auc 0.4951\n",
      "2022-06-30 00:55:12,660 - NCNet pretrain, Epoch [39 / 300]: loss 0.4534, training auc: 0.4964, val_auc 0.5227, test auc 0.4991\n",
      "2022-06-30 00:55:12,790 - NCNet pretrain, Epoch [40 / 300]: loss 0.4444, training auc: 0.5406, val_auc 0.5263, test auc 0.5029\n",
      "2022-06-30 00:55:12,919 - NCNet pretrain, Epoch [41 / 300]: loss 0.4568, training auc: 0.4787, val_auc 0.5328, test auc 0.5094\n",
      "2022-06-30 00:55:13,052 - NCNet pretrain, Epoch [42 / 300]: loss 0.4472, training auc: 0.5158, val_auc 0.5408, test auc 0.5182\n",
      "2022-06-30 00:55:13,185 - NCNet pretrain, Epoch [43 / 300]: loss 0.4474, training auc: 0.5175, val_auc 0.5500, test auc 0.5281\n",
      "2022-06-30 00:55:13,314 - NCNet pretrain, Epoch [44 / 300]: loss 0.4407, training auc: 0.5479, val_auc 0.5580, test auc 0.5365\n",
      "2022-06-30 00:55:13,442 - NCNet pretrain, Epoch [45 / 300]: loss 0.4286, training auc: 0.5942, val_auc 0.5632, test auc 0.5418\n",
      "2022-06-30 00:55:13,588 - NCNet pretrain, Epoch [46 / 300]: loss 0.4406, training auc: 0.5325, val_auc 0.5692, test auc 0.5476\n",
      "2022-06-30 00:55:13,735 - NCNet pretrain, Epoch [47 / 300]: loss 0.4370, training auc: 0.5439, val_auc 0.5798, test auc 0.5594\n",
      "2022-06-30 00:55:13,871 - NCNet pretrain, Epoch [48 / 300]: loss 0.4390, training auc: 0.5178, val_auc 0.5952, test auc 0.5765\n",
      "2022-06-30 00:55:13,999 - NCNet pretrain, Epoch [49 / 300]: loss 0.4257, training auc: 0.5840, val_auc 0.6097, test auc 0.5917\n",
      "2022-06-30 00:55:14,128 - NCNet pretrain, Epoch [50 / 300]: loss 0.4242, training auc: 0.5897, val_auc 0.6227, test auc 0.6042\n",
      "2022-06-30 00:55:14,278 - NCNet pretrain, Epoch [51 / 300]: loss 0.4279, training auc: 0.5742, val_auc 0.6326, test auc 0.6136\n",
      "2022-06-30 00:55:14,407 - NCNet pretrain, Epoch [52 / 300]: loss 0.4242, training auc: 0.6045, val_auc 0.6352, test auc 0.6152\n",
      "2022-06-30 00:55:14,555 - NCNet pretrain, Epoch [53 / 300]: loss 0.4161, training auc: 0.6230, val_auc 0.6427, test auc 0.6228\n",
      "2022-06-30 00:55:14,685 - NCNet pretrain, Epoch [54 / 300]: loss 0.4167, training auc: 0.6076, val_auc 0.6601, test auc 0.6400\n",
      "2022-06-30 00:55:14,820 - NCNet pretrain, Epoch [55 / 300]: loss 0.4166, training auc: 0.6019, val_auc 0.6774, test auc 0.6587\n",
      "2022-06-30 00:55:14,956 - NCNet pretrain, Epoch [56 / 300]: loss 0.4098, training auc: 0.6633, val_auc 0.6829, test auc 0.6636\n",
      "2022-06-30 00:55:15,080 - NCNet pretrain, Epoch [57 / 300]: loss 0.4027, training auc: 0.6672, val_auc 0.6833, test auc 0.6628\n",
      "2022-06-30 00:55:15,209 - NCNet pretrain, Epoch [58 / 300]: loss 0.3968, training auc: 0.6700, val_auc 0.6903, test auc 0.6697\n",
      "2022-06-30 00:55:15,352 - NCNet pretrain, Epoch [59 / 300]: loss 0.3995, training auc: 0.6658, val_auc 0.7052, test auc 0.6871\n",
      "2022-06-30 00:55:15,485 - NCNet pretrain, Epoch [60 / 300]: loss 0.3998, training auc: 0.6815, val_auc 0.7136, test auc 0.6983\n",
      "2022-06-30 00:55:15,622 - NCNet pretrain, Epoch [61 / 300]: loss 0.3926, training auc: 0.6952, val_auc 0.7115\n",
      "2022-06-30 00:55:15,744 - NCNet pretrain, Epoch [62 / 300]: loss 0.3931, training auc: 0.6858, val_auc 0.7101\n",
      "2022-06-30 00:55:15,872 - NCNet pretrain, Epoch [63 / 300]: loss 0.3877, training auc: 0.6873, val_auc 0.7208, test auc 0.7074\n",
      "2022-06-30 00:55:16,001 - NCNet pretrain, Epoch [64 / 300]: loss 0.3855, training auc: 0.7026, val_auc 0.7280, test auc 0.7181\n",
      "2022-06-30 00:55:16,119 - NCNet pretrain, Epoch [65 / 300]: loss 0.3817, training auc: 0.7239, val_auc 0.7254\n",
      "2022-06-30 00:55:16,237 - NCNet pretrain, Epoch [66 / 300]: loss 0.3845, training auc: 0.7092, val_auc 0.7254\n",
      "2022-06-30 00:55:16,366 - NCNet pretrain, Epoch [67 / 300]: loss 0.3756, training auc: 0.7122, val_auc 0.7346, test auc 0.7260\n",
      "2022-06-30 00:55:16,494 - NCNet pretrain, Epoch [68 / 300]: loss 0.3791, training auc: 0.7195, val_auc 0.7382, test auc 0.7318\n",
      "2022-06-30 00:55:16,644 - NCNet pretrain, Epoch [69 / 300]: loss 0.3721, training auc: 0.7399, val_auc 0.7392, test auc 0.7327\n",
      "2022-06-30 00:55:16,775 - NCNet pretrain, Epoch [70 / 300]: loss 0.3641, training auc: 0.7536, val_auc 0.7403, test auc 0.7334\n",
      "2022-06-30 00:55:16,904 - NCNet pretrain, Epoch [71 / 300]: loss 0.3804, training auc: 0.7233, val_auc 0.7461, test auc 0.7434\n",
      "2022-06-30 00:55:17,023 - NCNet pretrain, Epoch [72 / 300]: loss 0.3795, training auc: 0.7245, val_auc 0.7457\n",
      "2022-06-30 00:55:17,142 - NCNet pretrain, Epoch [73 / 300]: loss 0.3669, training auc: 0.7487, val_auc 0.7410\n",
      "2022-06-30 00:55:17,272 - NCNet pretrain, Epoch [74 / 300]: loss 0.3701, training auc: 0.7438, val_auc 0.7508, test auc 0.7518\n",
      "2022-06-30 00:55:17,409 - NCNet pretrain, Epoch [75 / 300]: loss 0.3640, training auc: 0.7605, val_auc 0.7545, test auc 0.7602\n",
      "2022-06-30 00:55:17,530 - NCNet pretrain, Epoch [76 / 300]: loss 0.3561, training auc: 0.7760, val_auc 0.7498\n",
      "2022-06-30 00:55:17,649 - NCNet pretrain, Epoch [77 / 300]: loss 0.3533, training auc: 0.7654, val_auc 0.7506\n",
      "2022-06-30 00:55:17,777 - NCNet pretrain, Epoch [78 / 300]: loss 0.3601, training auc: 0.7565, val_auc 0.7591, test auc 0.7684\n",
      "2022-06-30 00:55:17,896 - NCNet pretrain, Epoch [79 / 300]: loss 0.3618, training auc: 0.7823, val_auc 0.7577\n",
      "2022-06-30 00:55:18,013 - NCNet pretrain, Epoch [80 / 300]: loss 0.3523, training auc: 0.7741, val_auc 0.7507\n",
      "2022-06-30 00:55:18,130 - NCNet pretrain, Epoch [81 / 300]: loss 0.3570, training auc: 0.7689, val_auc 0.7565\n",
      "2022-06-30 00:55:18,259 - NCNet pretrain, Epoch [82 / 300]: loss 0.3472, training auc: 0.7773, val_auc 0.7625, test auc 0.7734\n",
      "2022-06-30 00:55:18,378 - NCNet pretrain, Epoch [83 / 300]: loss 0.3571, training auc: 0.7775, val_auc 0.7616\n",
      "2022-06-30 00:55:18,496 - NCNet pretrain, Epoch [84 / 300]: loss 0.3549, training auc: 0.7776, val_auc 0.7563\n",
      "2022-06-30 00:55:18,634 - NCNet pretrain, Epoch [85 / 300]: loss 0.3482, training auc: 0.7746, val_auc 0.7593\n",
      "2022-06-30 00:55:18,764 - NCNet pretrain, Epoch [86 / 300]: loss 0.3570, training auc: 0.7650, val_auc 0.7651, test auc 0.7771\n",
      "2022-06-30 00:55:18,882 - NCNet pretrain, Epoch [87 / 300]: loss 0.3402, training auc: 0.8064, val_auc 0.7633\n",
      "2022-06-30 00:55:19,000 - NCNet pretrain, Epoch [88 / 300]: loss 0.3569, training auc: 0.7755, val_auc 0.7544\n",
      "2022-06-30 00:55:19,120 - NCNet pretrain, Epoch [89 / 300]: loss 0.3519, training auc: 0.7749, val_auc 0.7639\n",
      "2022-06-30 00:55:19,250 - NCNet pretrain, Epoch [90 / 300]: loss 0.3461, training auc: 0.7980, val_auc 0.7657, test auc 0.7796\n",
      "2022-06-30 00:55:19,369 - NCNet pretrain, Epoch [91 / 300]: loss 0.3406, training auc: 0.8010, val_auc 0.7616\n",
      "2022-06-30 00:55:19,492 - NCNet pretrain, Epoch [92 / 300]: loss 0.3409, training auc: 0.7930, val_auc 0.7588\n",
      "2022-06-30 00:55:19,614 - NCNet pretrain, Epoch [93 / 300]: loss 0.3484, training auc: 0.7855, val_auc 0.7646\n",
      "2022-06-30 00:55:19,732 - NCNet pretrain, Epoch [94 / 300]: loss 0.3387, training auc: 0.8050, val_auc 0.7636\n",
      "2022-06-30 00:55:19,849 - NCNet pretrain, Epoch [95 / 300]: loss 0.3432, training auc: 0.7931, val_auc 0.7622\n",
      "2022-06-30 00:55:19,985 - NCNet pretrain, Epoch [96 / 300]: loss 0.3370, training auc: 0.7970, val_auc 0.7633\n",
      "2022-06-30 00:55:20,121 - NCNet pretrain, Epoch [97 / 300]: loss 0.3342, training auc: 0.7994, val_auc 0.7646\n",
      "2022-06-30 00:55:20,243 - NCNet pretrain, Epoch [98 / 300]: loss 0.3367, training auc: 0.7961, val_auc 0.7656\n",
      "2022-06-30 00:55:20,360 - NCNet pretrain, Epoch [99 / 300]: loss 0.3498, training auc: 0.7847, val_auc 0.7655\n",
      "2022-06-30 00:55:20,491 - NCNet pretrain, Epoch [100 / 300]: loss 0.3400, training auc: 0.7957, val_auc 0.7670, test auc 0.7827\n",
      "2022-06-30 00:55:20,609 - NCNet pretrain, Epoch [101 / 300]: loss 0.3293, training auc: 0.8160, val_auc 0.7653\n",
      "2022-06-30 00:55:20,740 - NCNet pretrain, Epoch [102 / 300]: loss 0.3416, training auc: 0.7927, val_auc 0.7642\n",
      "2022-06-30 00:55:20,858 - NCNet pretrain, Epoch [103 / 300]: loss 0.3308, training auc: 0.8115, val_auc 0.7668\n",
      "2022-06-30 00:55:20,987 - NCNet pretrain, Epoch [104 / 300]: loss 0.3440, training auc: 0.7897, val_auc 0.7689, test auc 0.7870\n",
      "2022-06-30 00:55:21,126 - NCNet pretrain, Epoch [105 / 300]: loss 0.3394, training auc: 0.8061, val_auc 0.7624\n",
      "2022-06-30 00:55:21,264 - NCNet pretrain, Epoch [106 / 300]: loss 0.3369, training auc: 0.8003, val_auc 0.7654\n",
      "2022-06-30 00:55:21,391 - NCNet pretrain, Epoch [107 / 300]: loss 0.3415, training auc: 0.7969, val_auc 0.7692, test auc 0.7869\n",
      "2022-06-30 00:55:21,529 - NCNet pretrain, Epoch [108 / 300]: loss 0.3322, training auc: 0.8129, val_auc 0.7680\n",
      "2022-06-30 00:55:21,655 - NCNet pretrain, Epoch [109 / 300]: loss 0.3327, training auc: 0.8061, val_auc 0.7661\n",
      "2022-06-30 00:55:21,777 - NCNet pretrain, Epoch [110 / 300]: loss 0.3310, training auc: 0.8124, val_auc 0.7687\n",
      "2022-06-30 00:55:21,907 - NCNet pretrain, Epoch [111 / 300]: loss 0.3385, training auc: 0.8048, val_auc 0.7695, test auc 0.7878\n",
      "2022-06-30 00:55:22,024 - NCNet pretrain, Epoch [112 / 300]: loss 0.3405, training auc: 0.8005, val_auc 0.7653\n",
      "2022-06-30 00:55:22,142 - NCNet pretrain, Epoch [113 / 300]: loss 0.3225, training auc: 0.8209, val_auc 0.7681\n",
      "2022-06-30 00:55:22,270 - NCNet pretrain, Epoch [114 / 300]: loss 0.3250, training auc: 0.8181, val_auc 0.7707, test auc 0.7907\n",
      "2022-06-30 00:55:22,388 - NCNet pretrain, Epoch [115 / 300]: loss 0.3335, training auc: 0.8210, val_auc 0.7662\n",
      "2022-06-30 00:55:22,508 - NCNet pretrain, Epoch [116 / 300]: loss 0.3253, training auc: 0.8204, val_auc 0.7678\n",
      "2022-06-30 00:55:22,637 - NCNet pretrain, Epoch [117 / 300]: loss 0.3221, training auc: 0.8239, val_auc 0.7711, test auc 0.7916\n",
      "2022-06-30 00:55:22,755 - NCNet pretrain, Epoch [118 / 300]: loss 0.3197, training auc: 0.8407, val_auc 0.7678\n",
      "2022-06-30 00:55:22,874 - NCNet pretrain, Epoch [119 / 300]: loss 0.3316, training auc: 0.8114, val_auc 0.7652\n",
      "2022-06-30 00:55:22,992 - NCNet pretrain, Epoch [120 / 300]: loss 0.3255, training auc: 0.8203, val_auc 0.7709\n",
      "2022-06-30 00:55:23,110 - NCNet pretrain, Epoch [121 / 300]: loss 0.3330, training auc: 0.8160, val_auc 0.7705\n",
      "2022-06-30 00:55:23,228 - NCNet pretrain, Epoch [122 / 300]: loss 0.3228, training auc: 0.8290, val_auc 0.7642\n",
      "2022-06-30 00:55:23,347 - NCNet pretrain, Epoch [123 / 300]: loss 0.3326, training auc: 0.8060, val_auc 0.7662\n",
      "2022-06-30 00:55:23,469 - NCNet pretrain, Epoch [124 / 300]: loss 0.3231, training auc: 0.8185, val_auc 0.7700\n",
      "2022-06-30 00:55:23,587 - NCNet pretrain, Epoch [125 / 300]: loss 0.3237, training auc: 0.8375, val_auc 0.7699\n",
      "2022-06-30 00:55:23,710 - NCNet pretrain, Epoch [126 / 300]: loss 0.3215, training auc: 0.8291, val_auc 0.7616\n",
      "2022-06-30 00:55:23,844 - NCNet pretrain, Epoch [127 / 300]: loss 0.3429, training auc: 0.8041, val_auc 0.7715, test auc 0.7935\n",
      "2022-06-30 00:55:23,991 - NCNet pretrain, Epoch [128 / 300]: loss 0.3305, training auc: 0.8196, val_auc 0.7716, test auc 0.7949\n",
      "2022-06-30 00:55:24,127 - NCNet pretrain, Epoch [129 / 300]: loss 0.3219, training auc: 0.8397, val_auc 0.7656\n",
      "2022-06-30 00:55:24,262 - NCNet pretrain, Epoch [130 / 300]: loss 0.3196, training auc: 0.8287, val_auc 0.7647\n",
      "2022-06-30 00:55:24,399 - NCNet pretrain, Epoch [131 / 300]: loss 0.3212, training auc: 0.8263, val_auc 0.7705\n",
      "2022-06-30 00:55:24,520 - NCNet pretrain, Epoch [132 / 300]: loss 0.3129, training auc: 0.8333, val_auc 0.7715\n",
      "2022-06-30 00:55:24,637 - NCNet pretrain, Epoch [133 / 300]: loss 0.3219, training auc: 0.8305, val_auc 0.7681\n",
      "2022-06-30 00:55:24,755 - NCNet pretrain, Epoch [134 / 300]: loss 0.3113, training auc: 0.8347, val_auc 0.7657\n",
      "2022-06-30 00:55:24,893 - NCNet pretrain, Epoch [135 / 300]: loss 0.3110, training auc: 0.8370, val_auc 0.7722, test auc 0.7954\n",
      "2022-06-30 00:55:25,012 - NCNet pretrain, Epoch [136 / 300]: loss 0.3189, training auc: 0.8373, val_auc 0.7711\n",
      "2022-06-30 00:55:25,130 - NCNet pretrain, Epoch [137 / 300]: loss 0.3170, training auc: 0.8318, val_auc 0.7674\n",
      "2022-06-30 00:55:25,269 - NCNet pretrain, Epoch [138 / 300]: loss 0.3254, training auc: 0.8265, val_auc 0.7705\n",
      "2022-06-30 00:55:25,429 - NCNet pretrain, Epoch [139 / 300]: loss 0.3154, training auc: 0.8330, val_auc 0.7728, test auc 0.7959\n",
      "2022-06-30 00:55:25,574 - NCNet pretrain, Epoch [140 / 300]: loss 0.3164, training auc: 0.8347, val_auc 0.7702\n",
      "2022-06-30 00:55:25,715 - NCNet pretrain, Epoch [141 / 300]: loss 0.3224, training auc: 0.8233, val_auc 0.7652\n",
      "2022-06-30 00:55:25,878 - NCNet pretrain, Epoch [142 / 300]: loss 0.3245, training auc: 0.8240, val_auc 0.7736, test auc 0.7993\n",
      "2022-06-30 00:55:26,020 - NCNet pretrain, Epoch [143 / 300]: loss 0.3195, training auc: 0.8434, val_auc 0.7707\n",
      "2022-06-30 00:55:26,161 - NCNet pretrain, Epoch [144 / 300]: loss 0.3248, training auc: 0.8208, val_auc 0.7649\n",
      "2022-06-30 00:55:26,301 - NCNet pretrain, Epoch [145 / 300]: loss 0.3125, training auc: 0.8365, val_auc 0.7725\n",
      "2022-06-30 00:55:26,442 - NCNet pretrain, Epoch [146 / 300]: loss 0.3073, training auc: 0.8430, val_auc 0.7729\n",
      "2022-06-30 00:55:26,584 - NCNet pretrain, Epoch [147 / 300]: loss 0.3165, training auc: 0.8395, val_auc 0.7693\n",
      "2022-06-30 00:55:26,714 - NCNet pretrain, Epoch [148 / 300]: loss 0.3157, training auc: 0.8262, val_auc 0.7665\n",
      "2022-06-30 00:55:26,842 - NCNet pretrain, Epoch [149 / 300]: loss 0.3069, training auc: 0.8423, val_auc 0.7712\n",
      "2022-06-30 00:55:26,971 - NCNet pretrain, Epoch [150 / 300]: loss 0.3088, training auc: 0.8422, val_auc 0.7705\n",
      "2022-06-30 00:55:27,112 - NCNet pretrain, Epoch [151 / 300]: loss 0.3054, training auc: 0.8499, val_auc 0.7677\n",
      "2022-06-30 00:55:27,255 - NCNet pretrain, Epoch [152 / 300]: loss 0.3086, training auc: 0.8390, val_auc 0.7694\n",
      "2022-06-30 00:55:27,397 - NCNet pretrain, Epoch [153 / 300]: loss 0.3030, training auc: 0.8544, val_auc 0.7711\n",
      "2022-06-30 00:55:27,536 - NCNet pretrain, Epoch [154 / 300]: loss 0.3080, training auc: 0.8467, val_auc 0.7684\n",
      "2022-06-30 00:55:27,679 - NCNet pretrain, Epoch [155 / 300]: loss 0.3078, training auc: 0.8420, val_auc 0.7659\n",
      "2022-06-30 00:55:27,821 - NCNet pretrain, Epoch [156 / 300]: loss 0.3039, training auc: 0.8496, val_auc 0.7711\n",
      "2022-06-30 00:55:27,966 - NCNet pretrain, Epoch [157 / 300]: loss 0.3095, training auc: 0.8490, val_auc 0.7683\n",
      "2022-06-30 00:55:28,111 - NCNet pretrain, Epoch [158 / 300]: loss 0.3008, training auc: 0.8446, val_auc 0.7664\n",
      "2022-06-30 00:55:28,243 - NCNet pretrain, Epoch [159 / 300]: loss 0.3064, training auc: 0.8473, val_auc 0.7703\n",
      "2022-06-30 00:55:28,376 - NCNet pretrain, Epoch [160 / 300]: loss 0.3078, training auc: 0.8518, val_auc 0.7681\n",
      "2022-06-30 00:55:28,509 - NCNet pretrain, Epoch [161 / 300]: loss 0.3161, training auc: 0.8298, val_auc 0.7670\n",
      "2022-06-30 00:55:28,641 - NCNet pretrain, Epoch [162 / 300]: loss 0.3069, training auc: 0.8389, val_auc 0.7706\n",
      "2022-06-30 00:55:28,825 - NCNet pretrain, Epoch [163 / 300]: loss 0.3090, training auc: 0.8508, val_auc 0.7660\n",
      "2022-06-30 00:55:28,966 - NCNet pretrain, Epoch [164 / 300]: loss 0.3043, training auc: 0.8491, val_auc 0.7691\n",
      "2022-06-30 00:55:29,107 - NCNet pretrain, Epoch [165 / 300]: loss 0.3029, training auc: 0.8511, val_auc 0.7685\n",
      "2022-06-30 00:55:29,228 - NCNet pretrain, Epoch [166 / 300]: loss 0.3079, training auc: 0.8411, val_auc 0.7660\n",
      "2022-06-30 00:55:29,370 - NCNet pretrain, Epoch [167 / 300]: loss 0.2982, training auc: 0.8530, val_auc 0.7689\n",
      "2022-06-30 00:55:29,506 - NCNet pretrain, Epoch [168 / 300]: loss 0.3040, training auc: 0.8512, val_auc 0.7690\n",
      "2022-06-30 00:55:29,629 - NCNet pretrain, Epoch [169 / 300]: loss 0.3074, training auc: 0.8460, val_auc 0.7653\n",
      "2022-06-30 00:55:29,758 - NCNet pretrain, Epoch [170 / 300]: loss 0.2950, training auc: 0.8535, val_auc 0.7661\n",
      "2022-06-30 00:55:29,900 - NCNet pretrain, Epoch [171 / 300]: loss 0.3037, training auc: 0.8497, val_auc 0.7693\n",
      "2022-06-30 00:55:30,038 - NCNet pretrain, Epoch [172 / 300]: loss 0.3028, training auc: 0.8625, val_auc 0.7621\n",
      "2022-06-30 00:55:30,184 - NCNet pretrain, Epoch [173 / 300]: loss 0.3018, training auc: 0.8518, val_auc 0.7680\n",
      "2022-06-30 00:55:30,325 - NCNet pretrain, Epoch [174 / 300]: loss 0.2995, training auc: 0.8543, val_auc 0.7694\n",
      "2022-06-30 00:55:30,447 - NCNet pretrain, Epoch [175 / 300]: loss 0.3074, training auc: 0.8584, val_auc 0.7591\n",
      "2022-06-30 00:55:30,571 - NCNet pretrain, Epoch [176 / 300]: loss 0.3144, training auc: 0.8443, val_auc 0.7674\n",
      "2022-06-30 00:55:30,709 - NCNet pretrain, Epoch [177 / 300]: loss 0.3041, training auc: 0.8533, val_auc 0.7678\n",
      "2022-06-30 00:55:30,828 - NCNet pretrain, Epoch [178 / 300]: loss 0.3045, training auc: 0.8735, val_auc 0.7549\n",
      "2022-06-30 00:55:30,947 - NCNet pretrain, Epoch [179 / 300]: loss 0.3277, training auc: 0.8362, val_auc 0.7647\n",
      "2022-06-30 00:55:31,086 - NCNet pretrain, Epoch [180 / 300]: loss 0.2934, training auc: 0.8591, val_auc 0.7645\n",
      "2022-06-30 00:55:31,206 - NCNet pretrain, Epoch [181 / 300]: loss 0.3316, training auc: 0.8604, val_auc 0.7579\n",
      "2022-06-30 00:55:31,325 - NCNet pretrain, Epoch [182 / 300]: loss 0.3218, training auc: 0.8275, val_auc 0.7548\n",
      "2022-06-30 00:55:31,445 - NCNet pretrain, Epoch [183 / 300]: loss 0.3236, training auc: 0.8402, val_auc 0.7661\n",
      "2022-06-30 00:55:31,565 - NCNet pretrain, Epoch [184 / 300]: loss 0.3221, training auc: 0.8550, val_auc 0.7671\n",
      "2022-06-30 00:55:31,686 - NCNet pretrain, Epoch [185 / 300]: loss 0.3099, training auc: 0.8614, val_auc 0.7564\n",
      "2022-06-30 00:55:31,809 - NCNet pretrain, Epoch [186 / 300]: loss 0.3138, training auc: 0.8489, val_auc 0.7591\n",
      "2022-06-30 00:55:31,929 - NCNet pretrain, Epoch [187 / 300]: loss 0.3169, training auc: 0.8411, val_auc 0.7668\n",
      "2022-06-30 00:55:32,049 - NCNet pretrain, Epoch [188 / 300]: loss 0.3043, training auc: 0.8744, val_auc 0.7671\n",
      "2022-06-30 00:55:32,173 - NCNet pretrain, Epoch [189 / 300]: loss 0.3068, training auc: 0.8672, val_auc 0.7598\n",
      "2022-06-30 00:55:32,315 - NCNet pretrain, Epoch [190 / 300]: loss 0.3083, training auc: 0.8452, val_auc 0.7573\n",
      "2022-06-30 00:55:32,433 - NCNet pretrain, Epoch [191 / 300]: loss 0.3154, training auc: 0.8465, val_auc 0.7677\n",
      "2022-06-30 00:55:32,552 - NCNet pretrain, Epoch [192 / 300]: loss 0.3031, training auc: 0.8602, val_auc 0.7677\n",
      "2022-06-30 00:55:32,553 - Early stop!\n",
      "2022-06-30 00:55:32,554 - Best Test Results: auc 0.7993, ap 0.4096, f1 0.3110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 329947.06it/s]\n",
      "2022-06-30 00:55:33,507 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:55:33,725 - NCNet pretrain, Epoch [1 / 300]: loss 0.4419, training auc: 0.5241, val_auc 0.4688, test auc 0.4419\n",
      "2022-06-30 00:55:33,856 - NCNet pretrain, Epoch [2 / 300]: loss 0.4365, training auc: 0.4332, val_auc 0.4695, test auc 0.4448\n",
      "2022-06-30 00:55:33,987 - NCNet pretrain, Epoch [3 / 300]: loss 0.4364, training auc: 0.4086, val_auc 0.4782, test auc 0.4571\n",
      "2022-06-30 00:55:34,119 - NCNet pretrain, Epoch [4 / 300]: loss 0.4224, training auc: 0.4933, val_auc 0.4884, test auc 0.4703\n",
      "2022-06-30 00:55:34,254 - NCNet pretrain, Epoch [5 / 300]: loss 0.4314, training auc: 0.3917, val_auc 0.4970, test auc 0.4813\n",
      "2022-06-30 00:55:34,394 - NCNet pretrain, Epoch [6 / 300]: loss 0.4278, training auc: 0.4809, val_auc 0.4967\n",
      "2022-06-30 00:55:34,514 - NCNet pretrain, Epoch [7 / 300]: loss 0.4303, training auc: 0.4110, val_auc 0.4945\n",
      "2022-06-30 00:55:34,634 - NCNet pretrain, Epoch [8 / 300]: loss 0.4204, training auc: 0.5217, val_auc 0.4931\n",
      "2022-06-30 00:55:34,753 - NCNet pretrain, Epoch [9 / 300]: loss 0.4248, training auc: 0.4714, val_auc 0.4939\n",
      "2022-06-30 00:55:34,874 - NCNet pretrain, Epoch [10 / 300]: loss 0.4267, training auc: 0.4209, val_auc 0.4966\n",
      "2022-06-30 00:55:35,012 - NCNet pretrain, Epoch [11 / 300]: loss 0.4212, training auc: 0.4891, val_auc 0.5018, test auc 0.4782\n",
      "2022-06-30 00:55:35,160 - NCNet pretrain, Epoch [12 / 300]: loss 0.4218, training auc: 0.4661, val_auc 0.5076, test auc 0.4823\n",
      "2022-06-30 00:55:35,291 - NCNet pretrain, Epoch [13 / 300]: loss 0.4176, training auc: 0.5315, val_auc 0.5125, test auc 0.4857\n",
      "2022-06-30 00:55:35,421 - NCNet pretrain, Epoch [14 / 300]: loss 0.4154, training auc: 0.5559, val_auc 0.5189, test auc 0.4905\n",
      "2022-06-30 00:55:35,552 - NCNet pretrain, Epoch [15 / 300]: loss 0.4153, training auc: 0.5345, val_auc 0.5260, test auc 0.4962\n",
      "2022-06-30 00:55:35,686 - NCNet pretrain, Epoch [16 / 300]: loss 0.4151, training auc: 0.5438, val_auc 0.5344, test auc 0.5031\n",
      "2022-06-30 00:55:35,816 - NCNet pretrain, Epoch [17 / 300]: loss 0.4179, training auc: 0.5058, val_auc 0.5514, test auc 0.5167\n",
      "2022-06-30 00:55:35,947 - NCNet pretrain, Epoch [18 / 300]: loss 0.4148, training auc: 0.5307, val_auc 0.5855, test auc 0.5424\n",
      "2022-06-30 00:55:36,078 - NCNet pretrain, Epoch [19 / 300]: loss 0.4161, training auc: 0.5253, val_auc 0.6271, test auc 0.5771\n",
      "2022-06-30 00:55:36,209 - NCNet pretrain, Epoch [20 / 300]: loss 0.4151, training auc: 0.5265, val_auc 0.6542, test auc 0.6016\n",
      "2022-06-30 00:55:36,329 - NCNet pretrain, Epoch [21 / 300]: loss 0.4112, training auc: 0.5973, val_auc 0.6536\n",
      "2022-06-30 00:55:36,475 - NCNet pretrain, Epoch [22 / 300]: loss 0.4050, training auc: 0.6468, val_auc 0.6512\n",
      "2022-06-30 00:55:36,605 - NCNet pretrain, Epoch [23 / 300]: loss 0.4070, training auc: 0.6082, val_auc 0.6874, test auc 0.6374\n",
      "2022-06-30 00:55:36,753 - NCNet pretrain, Epoch [24 / 300]: loss 0.3963, training auc: 0.7049, val_auc 0.7221, test auc 0.6852\n",
      "2022-06-30 00:55:36,883 - NCNet pretrain, Epoch [25 / 300]: loss 0.4000, training auc: 0.6797, val_auc 0.7318, test auc 0.6924\n",
      "2022-06-30 00:55:37,002 - NCNet pretrain, Epoch [26 / 300]: loss 0.3935, training auc: 0.7103, val_auc 0.7174\n",
      "2022-06-30 00:55:37,132 - NCNet pretrain, Epoch [27 / 300]: loss 0.3979, training auc: 0.6262, val_auc 0.7455, test auc 0.7078\n",
      "2022-06-30 00:55:37,265 - NCNet pretrain, Epoch [28 / 300]: loss 0.3873, training auc: 0.7179, val_auc 0.7692, test auc 0.7415\n",
      "2022-06-30 00:55:37,389 - NCNet pretrain, Epoch [29 / 300]: loss 0.3896, training auc: 0.7151, val_auc 0.7568\n",
      "2022-06-30 00:55:37,538 - NCNet pretrain, Epoch [30 / 300]: loss 0.3857, training auc: 0.6894, val_auc 0.7726, test auc 0.7526\n",
      "2022-06-30 00:55:37,683 - NCNet pretrain, Epoch [31 / 300]: loss 0.3731, training auc: 0.7608, val_auc 0.7726, test auc 0.7571\n",
      "2022-06-30 00:55:37,807 - NCNet pretrain, Epoch [32 / 300]: loss 0.3814, training auc: 0.7398, val_auc 0.7675\n",
      "2022-06-30 00:55:37,927 - NCNet pretrain, Epoch [33 / 300]: loss 0.3649, training auc: 0.7596, val_auc 0.7592\n",
      "2022-06-30 00:55:38,047 - NCNet pretrain, Epoch [34 / 300]: loss 0.3605, training auc: 0.7543, val_auc 0.7677\n",
      "2022-06-30 00:55:38,168 - NCNet pretrain, Epoch [35 / 300]: loss 0.3712, training auc: 0.7657, val_auc 0.7397\n",
      "2022-06-30 00:55:38,289 - NCNet pretrain, Epoch [36 / 300]: loss 0.3680, training auc: 0.7367, val_auc 0.7597\n",
      "2022-06-30 00:55:38,407 - NCNet pretrain, Epoch [37 / 300]: loss 0.3558, training auc: 0.7605, val_auc 0.7681\n",
      "2022-06-30 00:55:38,526 - NCNet pretrain, Epoch [38 / 300]: loss 0.3598, training auc: 0.7883, val_auc 0.7517\n",
      "2022-06-30 00:55:38,648 - NCNet pretrain, Epoch [39 / 300]: loss 0.3647, training auc: 0.7362, val_auc 0.7605\n",
      "2022-06-30 00:55:38,782 - NCNet pretrain, Epoch [40 / 300]: loss 0.3503, training auc: 0.7723, val_auc 0.7738, test auc 0.7798\n",
      "2022-06-30 00:55:38,919 - NCNet pretrain, Epoch [41 / 300]: loss 0.3555, training auc: 0.7876, val_auc 0.7672\n",
      "2022-06-30 00:55:39,051 - NCNet pretrain, Epoch [42 / 300]: loss 0.3533, training auc: 0.7655, val_auc 0.7620\n",
      "2022-06-30 00:55:39,175 - NCNet pretrain, Epoch [43 / 300]: loss 0.3567, training auc: 0.7608, val_auc 0.7760, test auc 0.7771\n",
      "2022-06-30 00:55:39,304 - NCNet pretrain, Epoch [44 / 300]: loss 0.3457, training auc: 0.7898, val_auc 0.7798, test auc 0.7834\n",
      "2022-06-30 00:55:39,421 - NCNet pretrain, Epoch [45 / 300]: loss 0.3489, training auc: 0.7884, val_auc 0.7717\n",
      "2022-06-30 00:55:39,558 - NCNet pretrain, Epoch [46 / 300]: loss 0.3472, training auc: 0.7832, val_auc 0.7743\n",
      "2022-06-30 00:55:39,691 - NCNet pretrain, Epoch [47 / 300]: loss 0.3496, training auc: 0.7800, val_auc 0.7856, test auc 0.7891\n",
      "2022-06-30 00:55:39,809 - NCNet pretrain, Epoch [48 / 300]: loss 0.3482, training auc: 0.7981, val_auc 0.7821\n",
      "2022-06-30 00:55:39,948 - NCNet pretrain, Epoch [49 / 300]: loss 0.3391, training auc: 0.7930, val_auc 0.7752\n",
      "2022-06-30 00:55:40,087 - NCNet pretrain, Epoch [50 / 300]: loss 0.3457, training auc: 0.7811, val_auc 0.7843\n",
      "2022-06-30 00:55:40,236 - NCNet pretrain, Epoch [51 / 300]: loss 0.3382, training auc: 0.8011, val_auc 0.7892, test auc 0.7944\n",
      "2022-06-30 00:55:40,360 - NCNet pretrain, Epoch [52 / 300]: loss 0.3388, training auc: 0.8034, val_auc 0.7814\n",
      "2022-06-30 00:55:40,481 - NCNet pretrain, Epoch [53 / 300]: loss 0.3341, training auc: 0.7884, val_auc 0.7798\n",
      "2022-06-30 00:55:40,614 - NCNet pretrain, Epoch [54 / 300]: loss 0.3401, training auc: 0.7909, val_auc 0.7895, test auc 0.7947\n",
      "2022-06-30 00:55:40,753 - NCNet pretrain, Epoch [55 / 300]: loss 0.3367, training auc: 0.8091, val_auc 0.7887\n",
      "2022-06-30 00:55:40,894 - NCNet pretrain, Epoch [56 / 300]: loss 0.3363, training auc: 0.8076, val_auc 0.7803\n",
      "2022-06-30 00:55:41,032 - NCNet pretrain, Epoch [57 / 300]: loss 0.3369, training auc: 0.7921, val_auc 0.7839\n",
      "2022-06-30 00:55:41,160 - NCNet pretrain, Epoch [58 / 300]: loss 0.3309, training auc: 0.8011, val_auc 0.7908, test auc 0.7963\n",
      "2022-06-30 00:55:41,278 - NCNet pretrain, Epoch [59 / 300]: loss 0.3311, training auc: 0.8130, val_auc 0.7859\n",
      "2022-06-30 00:55:41,396 - NCNet pretrain, Epoch [60 / 300]: loss 0.3310, training auc: 0.8024, val_auc 0.7827\n",
      "2022-06-30 00:55:41,532 - NCNet pretrain, Epoch [61 / 300]: loss 0.3309, training auc: 0.7991, val_auc 0.7879\n",
      "2022-06-30 00:55:41,662 - NCNet pretrain, Epoch [62 / 300]: loss 0.3233, training auc: 0.8160, val_auc 0.7926, test auc 0.7980\n",
      "2022-06-30 00:55:41,800 - NCNet pretrain, Epoch [63 / 300]: loss 0.3309, training auc: 0.8183, val_auc 0.7875\n",
      "2022-06-30 00:55:41,928 - NCNet pretrain, Epoch [64 / 300]: loss 0.3233, training auc: 0.8216, val_auc 0.7877\n",
      "2022-06-30 00:55:42,051 - NCNet pretrain, Epoch [65 / 300]: loss 0.3263, training auc: 0.8106, val_auc 0.7926, test auc 0.7970\n",
      "2022-06-30 00:55:42,175 - NCNet pretrain, Epoch [66 / 300]: loss 0.3229, training auc: 0.8153, val_auc 0.7930, test auc 0.7976\n",
      "2022-06-30 00:55:42,293 - NCNet pretrain, Epoch [67 / 300]: loss 0.3252, training auc: 0.8105, val_auc 0.7909\n",
      "2022-06-30 00:55:42,422 - NCNet pretrain, Epoch [68 / 300]: loss 0.3221, training auc: 0.8197, val_auc 0.7938, test auc 0.7987\n",
      "2022-06-30 00:55:42,573 - NCNet pretrain, Epoch [69 / 300]: loss 0.3205, training auc: 0.8225, val_auc 0.7975, test auc 0.8057\n",
      "2022-06-30 00:55:42,694 - NCNet pretrain, Epoch [70 / 300]: loss 0.3186, training auc: 0.8357, val_auc 0.7828\n",
      "2022-06-30 00:55:42,812 - NCNet pretrain, Epoch [71 / 300]: loss 0.3291, training auc: 0.8106, val_auc 0.7908\n",
      "2022-06-30 00:55:42,947 - NCNet pretrain, Epoch [72 / 300]: loss 0.3216, training auc: 0.8222, val_auc 0.7990, test auc 0.8095\n",
      "2022-06-30 00:55:43,064 - NCNet pretrain, Epoch [73 / 300]: loss 0.3314, training auc: 0.8304, val_auc 0.7835\n",
      "2022-06-30 00:55:43,202 - NCNet pretrain, Epoch [74 / 300]: loss 0.3231, training auc: 0.8152, val_auc 0.7870\n",
      "2022-06-30 00:55:43,340 - NCNet pretrain, Epoch [75 / 300]: loss 0.3199, training auc: 0.8227, val_auc 0.7983\n",
      "2022-06-30 00:55:43,477 - NCNet pretrain, Epoch [76 / 300]: loss 0.3202, training auc: 0.8332, val_auc 0.7935\n",
      "2022-06-30 00:55:43,598 - NCNet pretrain, Epoch [77 / 300]: loss 0.3134, training auc: 0.8300, val_auc 0.7850\n",
      "2022-06-30 00:55:43,732 - NCNet pretrain, Epoch [78 / 300]: loss 0.3231, training auc: 0.8140, val_auc 0.7932\n",
      "2022-06-30 00:55:43,861 - NCNet pretrain, Epoch [79 / 300]: loss 0.3065, training auc: 0.8360, val_auc 0.7991, test auc 0.8114\n",
      "2022-06-30 00:55:43,979 - NCNet pretrain, Epoch [80 / 300]: loss 0.3273, training auc: 0.8368, val_auc 0.7851\n",
      "2022-06-30 00:55:44,097 - NCNet pretrain, Epoch [81 / 300]: loss 0.3272, training auc: 0.8175, val_auc 0.7904\n",
      "2022-06-30 00:55:44,227 - NCNet pretrain, Epoch [82 / 300]: loss 0.3168, training auc: 0.8292, val_auc 0.7999, test auc 0.8131\n",
      "2022-06-30 00:55:44,346 - NCNet pretrain, Epoch [83 / 300]: loss 0.3222, training auc: 0.8412, val_auc 0.7938\n",
      "2022-06-30 00:55:44,465 - NCNet pretrain, Epoch [84 / 300]: loss 0.3029, training auc: 0.8462, val_auc 0.7826\n",
      "2022-06-30 00:55:44,584 - NCNet pretrain, Epoch [85 / 300]: loss 0.3193, training auc: 0.8260, val_auc 0.7927\n",
      "2022-06-30 00:55:44,715 - NCNet pretrain, Epoch [86 / 300]: loss 0.3083, training auc: 0.8374, val_auc 0.8004, test auc 0.8157\n",
      "2022-06-30 00:55:44,836 - NCNet pretrain, Epoch [87 / 300]: loss 0.3199, training auc: 0.8485, val_auc 0.7904\n",
      "2022-06-30 00:55:44,955 - NCNet pretrain, Epoch [88 / 300]: loss 0.3055, training auc: 0.8400, val_auc 0.7826\n",
      "2022-06-30 00:55:45,082 - NCNet pretrain, Epoch [89 / 300]: loss 0.3207, training auc: 0.8234, val_auc 0.7954\n",
      "2022-06-30 00:55:45,200 - NCNet pretrain, Epoch [90 / 300]: loss 0.3128, training auc: 0.8446, val_auc 0.7992\n",
      "2022-06-30 00:55:45,319 - NCNet pretrain, Epoch [91 / 300]: loss 0.3136, training auc: 0.8560, val_auc 0.7876\n",
      "2022-06-30 00:55:45,438 - NCNet pretrain, Epoch [92 / 300]: loss 0.3073, training auc: 0.8337, val_auc 0.7836\n",
      "2022-06-30 00:55:45,557 - NCNet pretrain, Epoch [93 / 300]: loss 0.3195, training auc: 0.8266, val_auc 0.7963\n",
      "2022-06-30 00:55:45,695 - NCNet pretrain, Epoch [94 / 300]: loss 0.3118, training auc: 0.8517, val_auc 0.7969\n",
      "2022-06-30 00:55:45,831 - NCNet pretrain, Epoch [95 / 300]: loss 0.3096, training auc: 0.8540, val_auc 0.7841\n",
      "2022-06-30 00:55:45,969 - NCNet pretrain, Epoch [96 / 300]: loss 0.3124, training auc: 0.8279, val_auc 0.7871\n",
      "2022-06-30 00:55:46,088 - NCNet pretrain, Epoch [97 / 300]: loss 0.3042, training auc: 0.8371, val_auc 0.7992\n",
      "2022-06-30 00:55:46,206 - NCNet pretrain, Epoch [98 / 300]: loss 0.3093, training auc: 0.8521, val_auc 0.7963\n",
      "2022-06-30 00:55:46,324 - NCNet pretrain, Epoch [99 / 300]: loss 0.3093, training auc: 0.8488, val_auc 0.7887\n",
      "2022-06-30 00:55:46,442 - NCNet pretrain, Epoch [100 / 300]: loss 0.3099, training auc: 0.8391, val_auc 0.7914\n",
      "2022-06-30 00:55:46,566 - NCNet pretrain, Epoch [101 / 300]: loss 0.3023, training auc: 0.8432, val_auc 0.7969\n",
      "2022-06-30 00:55:46,700 - NCNet pretrain, Epoch [102 / 300]: loss 0.3074, training auc: 0.8482, val_auc 0.7938\n",
      "2022-06-30 00:55:46,821 - NCNet pretrain, Epoch [103 / 300]: loss 0.3037, training auc: 0.8482, val_auc 0.7925\n",
      "2022-06-30 00:55:46,939 - NCNet pretrain, Epoch [104 / 300]: loss 0.2989, training auc: 0.8493, val_auc 0.7931\n",
      "2022-06-30 00:55:47,057 - NCNet pretrain, Epoch [105 / 300]: loss 0.3007, training auc: 0.8456, val_auc 0.7937\n",
      "2022-06-30 00:55:47,204 - NCNet pretrain, Epoch [106 / 300]: loss 0.2948, training auc: 0.8498, val_auc 0.7940\n",
      "2022-06-30 00:55:47,321 - NCNet pretrain, Epoch [107 / 300]: loss 0.3016, training auc: 0.8494, val_auc 0.7940\n",
      "2022-06-30 00:55:47,439 - NCNet pretrain, Epoch [108 / 300]: loss 0.3038, training auc: 0.8452, val_auc 0.7921\n",
      "2022-06-30 00:55:47,557 - NCNet pretrain, Epoch [109 / 300]: loss 0.2985, training auc: 0.8508, val_auc 0.7940\n",
      "2022-06-30 00:55:47,675 - NCNet pretrain, Epoch [110 / 300]: loss 0.2964, training auc: 0.8541, val_auc 0.7928\n",
      "2022-06-30 00:55:47,801 - NCNet pretrain, Epoch [111 / 300]: loss 0.2899, training auc: 0.8622, val_auc 0.7905\n",
      "2022-06-30 00:55:47,919 - NCNet pretrain, Epoch [112 / 300]: loss 0.2972, training auc: 0.8496, val_auc 0.7918\n",
      "2022-06-30 00:55:48,036 - NCNet pretrain, Epoch [113 / 300]: loss 0.2966, training auc: 0.8496, val_auc 0.7905\n",
      "2022-06-30 00:55:48,154 - NCNet pretrain, Epoch [114 / 300]: loss 0.2996, training auc: 0.8526, val_auc 0.7893\n",
      "2022-06-30 00:55:48,293 - NCNet pretrain, Epoch [115 / 300]: loss 0.2953, training auc: 0.8505, val_auc 0.7918\n",
      "2022-06-30 00:55:48,411 - NCNet pretrain, Epoch [116 / 300]: loss 0.2984, training auc: 0.8459, val_auc 0.7916\n",
      "2022-06-30 00:55:48,530 - NCNet pretrain, Epoch [117 / 300]: loss 0.2931, training auc: 0.8568, val_auc 0.7908\n",
      "2022-06-30 00:55:48,648 - NCNet pretrain, Epoch [118 / 300]: loss 0.3003, training auc: 0.8521, val_auc 0.7911\n",
      "2022-06-30 00:55:48,770 - NCNet pretrain, Epoch [119 / 300]: loss 0.2939, training auc: 0.8599, val_auc 0.7899\n",
      "2022-06-30 00:55:48,890 - NCNet pretrain, Epoch [120 / 300]: loss 0.2914, training auc: 0.8616, val_auc 0.7900\n",
      "2022-06-30 00:55:49,009 - NCNet pretrain, Epoch [121 / 300]: loss 0.2904, training auc: 0.8545, val_auc 0.7887\n",
      "2022-06-30 00:55:49,130 - NCNet pretrain, Epoch [122 / 300]: loss 0.2878, training auc: 0.8632, val_auc 0.7957\n",
      "2022-06-30 00:55:49,266 - NCNet pretrain, Epoch [123 / 300]: loss 0.3024, training auc: 0.8640, val_auc 0.7821\n",
      "2022-06-30 00:55:49,389 - NCNet pretrain, Epoch [124 / 300]: loss 0.3011, training auc: 0.8513, val_auc 0.7899\n",
      "2022-06-30 00:55:49,507 - NCNet pretrain, Epoch [125 / 300]: loss 0.2881, training auc: 0.8638, val_auc 0.7941\n",
      "2022-06-30 00:55:49,626 - NCNet pretrain, Epoch [126 / 300]: loss 0.2930, training auc: 0.8698, val_auc 0.7751\n",
      "2022-06-30 00:55:49,744 - NCNet pretrain, Epoch [127 / 300]: loss 0.3139, training auc: 0.8442, val_auc 0.7862\n",
      "2022-06-30 00:55:49,861 - NCNet pretrain, Epoch [128 / 300]: loss 0.2937, training auc: 0.8550, val_auc 0.7923\n",
      "2022-06-30 00:55:49,979 - NCNet pretrain, Epoch [129 / 300]: loss 0.3314, training auc: 0.8707, val_auc 0.7634\n",
      "2022-06-30 00:55:50,102 - NCNet pretrain, Epoch [130 / 300]: loss 0.3333, training auc: 0.8380, val_auc 0.7612\n",
      "2022-06-30 00:55:50,219 - NCNet pretrain, Epoch [131 / 300]: loss 0.3309, training auc: 0.8362, val_auc 0.7904\n",
      "2022-06-30 00:55:50,338 - NCNet pretrain, Epoch [132 / 300]: loss 0.2954, training auc: 0.8611, val_auc 0.7886\n",
      "2022-06-30 00:55:50,457 - NCNet pretrain, Epoch [133 / 300]: loss 0.3646, training auc: 0.8654, val_auc 0.7711\n",
      "2022-06-30 00:55:50,584 - NCNet pretrain, Epoch [134 / 300]: loss 0.3289, training auc: 0.8388, val_auc 0.7474\n",
      "2022-06-30 00:55:50,704 - NCNet pretrain, Epoch [135 / 300]: loss 0.3698, training auc: 0.8168, val_auc 0.7677\n",
      "2022-06-30 00:55:50,842 - NCNet pretrain, Epoch [136 / 300]: loss 0.3290, training auc: 0.8398, val_auc 0.7943\n",
      "2022-06-30 00:55:50,843 - Early stop!\n",
      "2022-06-30 00:55:50,843 - Best Test Results: auc 0.8157, ap 0.4403, f1 0.3800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 253757.76it/s]\n",
      "2022-06-30 00:55:51,977 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:55:52,192 - NCNet pretrain, Epoch [1 / 300]: loss 0.6584, training auc: 0.5446, val_auc 0.4674, test auc 0.4415\n",
      "2022-06-30 00:55:52,310 - NCNet pretrain, Epoch [2 / 300]: loss 0.4783, training auc: 0.4343, val_auc 0.4641\n",
      "2022-06-30 00:55:52,428 - NCNet pretrain, Epoch [3 / 300]: loss 0.4807, training auc: 0.4292, val_auc 0.4635\n",
      "2022-06-30 00:55:52,547 - NCNet pretrain, Epoch [4 / 300]: loss 0.4946, training auc: 0.4374, val_auc 0.4645\n",
      "2022-06-30 00:55:52,666 - NCNet pretrain, Epoch [5 / 300]: loss 0.4886, training auc: 0.4146, val_auc 0.4670\n",
      "2022-06-30 00:55:52,809 - NCNet pretrain, Epoch [6 / 300]: loss 0.4796, training auc: 0.3950, val_auc 0.4713, test auc 0.4481\n",
      "2022-06-30 00:55:52,939 - NCNet pretrain, Epoch [7 / 300]: loss 0.4627, training auc: 0.4379, val_auc 0.4749, test auc 0.4546\n",
      "2022-06-30 00:55:53,072 - NCNet pretrain, Epoch [8 / 300]: loss 0.4668, training auc: 0.4541, val_auc 0.4780, test auc 0.4590\n",
      "2022-06-30 00:55:53,202 - NCNet pretrain, Epoch [9 / 300]: loss 0.4670, training auc: 0.4825, val_auc 0.4794, test auc 0.4603\n",
      "2022-06-30 00:55:53,334 - NCNet pretrain, Epoch [10 / 300]: loss 0.4608, training auc: 0.5058, val_auc 0.4796, test auc 0.4592\n",
      "2022-06-30 00:55:53,456 - NCNet pretrain, Epoch [11 / 300]: loss 0.4513, training auc: 0.5140, val_auc 0.4789\n",
      "2022-06-30 00:55:53,597 - NCNet pretrain, Epoch [12 / 300]: loss 0.4681, training auc: 0.4089, val_auc 0.4791\n",
      "2022-06-30 00:55:53,723 - NCNet pretrain, Epoch [13 / 300]: loss 0.4671, training auc: 0.4291, val_auc 0.4796, test auc 0.4561\n",
      "2022-06-30 00:55:53,847 - NCNet pretrain, Epoch [14 / 300]: loss 0.4659, training auc: 0.4413, val_auc 0.4806, test auc 0.4573\n",
      "2022-06-30 00:55:53,970 - NCNet pretrain, Epoch [15 / 300]: loss 0.4618, training auc: 0.4393, val_auc 0.4820, test auc 0.4593\n",
      "2022-06-30 00:55:54,117 - NCNet pretrain, Epoch [16 / 300]: loss 0.4543, training auc: 0.4702, val_auc 0.4835, test auc 0.4614\n",
      "2022-06-30 00:55:54,265 - NCNet pretrain, Epoch [17 / 300]: loss 0.4437, training auc: 0.5105, val_auc 0.4848, test auc 0.4632\n",
      "2022-06-30 00:55:54,395 - NCNet pretrain, Epoch [18 / 300]: loss 0.4566, training auc: 0.4501, val_auc 0.4862, test auc 0.4649\n",
      "2022-06-30 00:55:54,543 - NCNet pretrain, Epoch [19 / 300]: loss 0.4527, training auc: 0.4769, val_auc 0.4873, test auc 0.4661\n",
      "2022-06-30 00:55:54,692 - NCNet pretrain, Epoch [20 / 300]: loss 0.4559, training auc: 0.4601, val_auc 0.4881, test auc 0.4667\n",
      "2022-06-30 00:55:54,816 - NCNet pretrain, Epoch [21 / 300]: loss 0.4474, training auc: 0.5053, val_auc 0.4885, test auc 0.4667\n",
      "2022-06-30 00:55:54,940 - NCNet pretrain, Epoch [22 / 300]: loss 0.4437, training auc: 0.5172, val_auc 0.4886, test auc 0.4658\n",
      "2022-06-30 00:55:55,064 - NCNet pretrain, Epoch [23 / 300]: loss 0.4485, training auc: 0.4790, val_auc 0.4888, test auc 0.4653\n",
      "2022-06-30 00:55:55,189 - NCNet pretrain, Epoch [24 / 300]: loss 0.4523, training auc: 0.4424, val_auc 0.4899, test auc 0.4656\n",
      "2022-06-30 00:55:55,314 - NCNet pretrain, Epoch [25 / 300]: loss 0.4455, training auc: 0.4947, val_auc 0.4909, test auc 0.4663\n",
      "2022-06-30 00:55:55,443 - NCNet pretrain, Epoch [26 / 300]: loss 0.4597, training auc: 0.4128, val_auc 0.4927, test auc 0.4679\n",
      "2022-06-30 00:55:55,577 - NCNet pretrain, Epoch [27 / 300]: loss 0.4636, training auc: 0.3942, val_auc 0.4952, test auc 0.4706\n",
      "2022-06-30 00:55:55,731 - NCNet pretrain, Epoch [28 / 300]: loss 0.4375, training auc: 0.5122, val_auc 0.4983, test auc 0.4735\n",
      "2022-06-30 00:55:55,864 - NCNet pretrain, Epoch [29 / 300]: loss 0.4455, training auc: 0.4766, val_auc 0.5015, test auc 0.4769\n",
      "2022-06-30 00:55:55,992 - NCNet pretrain, Epoch [30 / 300]: loss 0.4399, training auc: 0.5178, val_auc 0.5040, test auc 0.4795\n",
      "2022-06-30 00:55:56,120 - NCNet pretrain, Epoch [31 / 300]: loss 0.4499, training auc: 0.4420, val_auc 0.5072, test auc 0.4826\n",
      "2022-06-30 00:55:56,250 - NCNet pretrain, Epoch [32 / 300]: loss 0.4412, training auc: 0.4875, val_auc 0.5104, test auc 0.4855\n",
      "2022-06-30 00:55:56,398 - NCNet pretrain, Epoch [33 / 300]: loss 0.4385, training auc: 0.4952, val_auc 0.5140, test auc 0.4888\n",
      "2022-06-30 00:55:56,541 - NCNet pretrain, Epoch [34 / 300]: loss 0.4402, training auc: 0.4763, val_auc 0.5185, test auc 0.4936\n",
      "2022-06-30 00:55:56,679 - NCNet pretrain, Epoch [35 / 300]: loss 0.4506, training auc: 0.4372, val_auc 0.5246, test auc 0.5001\n",
      "2022-06-30 00:55:56,810 - NCNet pretrain, Epoch [36 / 300]: loss 0.4353, training auc: 0.5194, val_auc 0.5313, test auc 0.5068\n",
      "2022-06-30 00:55:56,961 - NCNet pretrain, Epoch [37 / 300]: loss 0.4389, training auc: 0.4876, val_auc 0.5389, test auc 0.5147\n",
      "2022-06-30 00:55:57,090 - NCNet pretrain, Epoch [38 / 300]: loss 0.4367, training auc: 0.5033, val_auc 0.5467, test auc 0.5226\n",
      "2022-06-30 00:55:57,220 - NCNet pretrain, Epoch [39 / 300]: loss 0.4346, training auc: 0.5121, val_auc 0.5553, test auc 0.5313\n",
      "2022-06-30 00:55:57,348 - NCNet pretrain, Epoch [40 / 300]: loss 0.4323, training auc: 0.5186, val_auc 0.5649, test auc 0.5415\n",
      "2022-06-30 00:55:57,481 - NCNet pretrain, Epoch [41 / 300]: loss 0.4296, training auc: 0.5339, val_auc 0.5739, test auc 0.5509\n",
      "2022-06-30 00:55:57,630 - NCNet pretrain, Epoch [42 / 300]: loss 0.4213, training auc: 0.5732, val_auc 0.5837, test auc 0.5608\n",
      "2022-06-30 00:55:57,779 - NCNet pretrain, Epoch [43 / 300]: loss 0.4137, training auc: 0.6063, val_auc 0.5926, test auc 0.5698\n",
      "2022-06-30 00:55:57,932 - NCNet pretrain, Epoch [44 / 300]: loss 0.4223, training auc: 0.5672, val_auc 0.6122, test auc 0.5906\n",
      "2022-06-30 00:55:58,081 - NCNet pretrain, Epoch [45 / 300]: loss 0.4122, training auc: 0.6215, val_auc 0.6273, test auc 0.6051\n",
      "2022-06-30 00:55:58,209 - NCNet pretrain, Epoch [46 / 300]: loss 0.4134, training auc: 0.6011, val_auc 0.6365, test auc 0.6136\n",
      "2022-06-30 00:55:58,326 - NCNet pretrain, Epoch [47 / 300]: loss 0.4081, training auc: 0.6435, val_auc 0.6358\n",
      "2022-06-30 00:55:58,455 - NCNet pretrain, Epoch [48 / 300]: loss 0.4050, training auc: 0.6336, val_auc 0.6482, test auc 0.6228\n",
      "2022-06-30 00:55:58,584 - NCNet pretrain, Epoch [49 / 300]: loss 0.4004, training auc: 0.6502, val_auc 0.6676, test auc 0.6430\n",
      "2022-06-30 00:55:58,713 - NCNet pretrain, Epoch [50 / 300]: loss 0.4073, training auc: 0.6380, val_auc 0.6832, test auc 0.6603\n",
      "2022-06-30 00:55:58,843 - NCNet pretrain, Epoch [51 / 300]: loss 0.3992, training auc: 0.6708, val_auc 0.6836, test auc 0.6594\n",
      "2022-06-30 00:55:58,972 - NCNet pretrain, Epoch [52 / 300]: loss 0.4028, training auc: 0.6382, val_auc 0.6884, test auc 0.6642\n",
      "2022-06-30 00:55:59,103 - NCNet pretrain, Epoch [53 / 300]: loss 0.3947, training auc: 0.6692, val_auc 0.7030, test auc 0.6840\n",
      "2022-06-30 00:55:59,233 - NCNet pretrain, Epoch [54 / 300]: loss 0.3917, training auc: 0.6884, val_auc 0.7083, test auc 0.6906\n",
      "2022-06-30 00:55:59,353 - NCNet pretrain, Epoch [55 / 300]: loss 0.3905, training auc: 0.6899, val_auc 0.7034\n",
      "2022-06-30 00:55:59,485 - NCNet pretrain, Epoch [56 / 300]: loss 0.3818, training auc: 0.7013, val_auc 0.7126, test auc 0.6955\n",
      "2022-06-30 00:55:59,618 - NCNet pretrain, Epoch [57 / 300]: loss 0.3878, training auc: 0.7006, val_auc 0.7221, test auc 0.7089\n",
      "2022-06-30 00:55:59,739 - NCNet pretrain, Epoch [58 / 300]: loss 0.3834, training auc: 0.7144, val_auc 0.7207\n",
      "2022-06-30 00:55:59,860 - NCNet pretrain, Epoch [59 / 300]: loss 0.3834, training auc: 0.7129, val_auc 0.7206\n",
      "2022-06-30 00:55:59,992 - NCNet pretrain, Epoch [60 / 300]: loss 0.3783, training auc: 0.7174, val_auc 0.7304, test auc 0.7195\n",
      "2022-06-30 00:56:00,128 - NCNet pretrain, Epoch [61 / 300]: loss 0.3821, training auc: 0.7199, val_auc 0.7341, test auc 0.7243\n",
      "2022-06-30 00:56:00,267 - NCNet pretrain, Epoch [62 / 300]: loss 0.3782, training auc: 0.7283, val_auc 0.7286\n",
      "2022-06-30 00:56:00,386 - NCNet pretrain, Epoch [63 / 300]: loss 0.3705, training auc: 0.7415, val_auc 0.7304\n",
      "2022-06-30 00:56:00,516 - NCNet pretrain, Epoch [64 / 300]: loss 0.3718, training auc: 0.7306, val_auc 0.7456, test auc 0.7380\n",
      "2022-06-30 00:56:00,636 - NCNet pretrain, Epoch [65 / 300]: loss 0.3740, training auc: 0.7420, val_auc 0.7411\n",
      "2022-06-30 00:56:00,756 - NCNet pretrain, Epoch [66 / 300]: loss 0.3734, training auc: 0.7334, val_auc 0.7379\n",
      "2022-06-30 00:56:00,886 - NCNet pretrain, Epoch [67 / 300]: loss 0.3701, training auc: 0.7358, val_auc 0.7522, test auc 0.7442\n",
      "2022-06-30 00:56:01,037 - NCNet pretrain, Epoch [68 / 300]: loss 0.3668, training auc: 0.7449, val_auc 0.7550, test auc 0.7471\n",
      "2022-06-30 00:56:01,176 - NCNet pretrain, Epoch [69 / 300]: loss 0.3656, training auc: 0.7544, val_auc 0.7446\n",
      "2022-06-30 00:56:01,310 - NCNet pretrain, Epoch [70 / 300]: loss 0.3657, training auc: 0.7469, val_auc 0.7562, test auc 0.7484\n",
      "2022-06-30 00:56:01,441 - NCNet pretrain, Epoch [71 / 300]: loss 0.3581, training auc: 0.7629, val_auc 0.7628, test auc 0.7565\n",
      "2022-06-30 00:56:01,563 - NCNet pretrain, Epoch [72 / 300]: loss 0.3602, training auc: 0.7737, val_auc 0.7555\n",
      "2022-06-30 00:56:01,692 - NCNet pretrain, Epoch [73 / 300]: loss 0.3594, training auc: 0.7565, val_auc 0.7580\n",
      "2022-06-30 00:56:01,827 - NCNet pretrain, Epoch [74 / 300]: loss 0.3563, training auc: 0.7578, val_auc 0.7717, test auc 0.7670\n",
      "2022-06-30 00:56:01,953 - NCNet pretrain, Epoch [75 / 300]: loss 0.3563, training auc: 0.7897, val_auc 0.7557\n",
      "2022-06-30 00:56:02,094 - NCNet pretrain, Epoch [76 / 300]: loss 0.3535, training auc: 0.7683, val_auc 0.7586\n",
      "2022-06-30 00:56:02,233 - NCNet pretrain, Epoch [77 / 300]: loss 0.3504, training auc: 0.7752, val_auc 0.7718, test auc 0.7684\n",
      "2022-06-30 00:56:02,354 - NCNet pretrain, Epoch [78 / 300]: loss 0.3496, training auc: 0.7908, val_auc 0.7603\n",
      "2022-06-30 00:56:02,474 - NCNet pretrain, Epoch [79 / 300]: loss 0.3601, training auc: 0.7594, val_auc 0.7689\n",
      "2022-06-30 00:56:02,594 - NCNet pretrain, Epoch [80 / 300]: loss 0.3495, training auc: 0.7858, val_auc 0.7610\n",
      "2022-06-30 00:56:02,714 - NCNet pretrain, Epoch [81 / 300]: loss 0.3490, training auc: 0.7757, val_auc 0.7634\n",
      "2022-06-30 00:56:02,853 - NCNet pretrain, Epoch [82 / 300]: loss 0.3484, training auc: 0.7834, val_auc 0.7599\n",
      "2022-06-30 00:56:02,992 - NCNet pretrain, Epoch [83 / 300]: loss 0.3518, training auc: 0.7732, val_auc 0.7635\n",
      "2022-06-30 00:56:03,131 - NCNet pretrain, Epoch [84 / 300]: loss 0.3380, training auc: 0.7975, val_auc 0.7704\n",
      "2022-06-30 00:56:03,251 - NCNet pretrain, Epoch [85 / 300]: loss 0.3459, training auc: 0.7983, val_auc 0.7573\n",
      "2022-06-30 00:56:03,370 - NCNet pretrain, Epoch [86 / 300]: loss 0.3427, training auc: 0.7949, val_auc 0.7657\n",
      "2022-06-30 00:56:03,500 - NCNet pretrain, Epoch [87 / 300]: loss 0.3361, training auc: 0.8020, val_auc 0.7722, test auc 0.7780\n",
      "2022-06-30 00:56:03,619 - NCNet pretrain, Epoch [88 / 300]: loss 0.3364, training auc: 0.8145, val_auc 0.7636\n",
      "2022-06-30 00:56:03,737 - NCNet pretrain, Epoch [89 / 300]: loss 0.3389, training auc: 0.7955, val_auc 0.7636\n",
      "2022-06-30 00:56:03,887 - NCNet pretrain, Epoch [90 / 300]: loss 0.3351, training auc: 0.8006, val_auc 0.7755, test auc 0.7825\n",
      "2022-06-30 00:56:04,008 - NCNet pretrain, Epoch [91 / 300]: loss 0.3382, training auc: 0.8176, val_auc 0.7620\n",
      "2022-06-30 00:56:04,127 - NCNet pretrain, Epoch [92 / 300]: loss 0.3347, training auc: 0.7984, val_auc 0.7707\n",
      "2022-06-30 00:56:04,245 - NCNet pretrain, Epoch [93 / 300]: loss 0.3265, training auc: 0.8187, val_auc 0.7740\n",
      "2022-06-30 00:56:04,389 - NCNet pretrain, Epoch [94 / 300]: loss 0.3347, training auc: 0.8130, val_auc 0.7594\n",
      "2022-06-30 00:56:04,506 - NCNet pretrain, Epoch [95 / 300]: loss 0.3396, training auc: 0.8010, val_auc 0.7752\n",
      "2022-06-30 00:56:04,624 - NCNet pretrain, Epoch [96 / 300]: loss 0.3390, training auc: 0.8147, val_auc 0.7726\n",
      "2022-06-30 00:56:04,756 - NCNet pretrain, Epoch [97 / 300]: loss 0.3377, training auc: 0.8062, val_auc 0.7603\n",
      "2022-06-30 00:56:04,874 - NCNet pretrain, Epoch [98 / 300]: loss 0.3262, training auc: 0.8149, val_auc 0.7714\n",
      "2022-06-30 00:56:05,022 - NCNet pretrain, Epoch [99 / 300]: loss 0.3316, training auc: 0.8091, val_auc 0.7764, test auc 0.7860\n",
      "2022-06-30 00:56:05,150 - NCNet pretrain, Epoch [100 / 300]: loss 0.3353, training auc: 0.8124, val_auc 0.7657\n",
      "2022-06-30 00:56:05,268 - NCNet pretrain, Epoch [101 / 300]: loss 0.3237, training auc: 0.8169, val_auc 0.7679\n",
      "2022-06-30 00:56:05,386 - NCNet pretrain, Epoch [102 / 300]: loss 0.3266, training auc: 0.8186, val_auc 0.7720\n",
      "2022-06-30 00:56:05,504 - NCNet pretrain, Epoch [103 / 300]: loss 0.3305, training auc: 0.8176, val_auc 0.7693\n",
      "2022-06-30 00:56:05,640 - NCNet pretrain, Epoch [104 / 300]: loss 0.3245, training auc: 0.8138, val_auc 0.7697\n",
      "2022-06-30 00:56:05,780 - NCNet pretrain, Epoch [105 / 300]: loss 0.3293, training auc: 0.8094, val_auc 0.7712\n",
      "2022-06-30 00:56:05,899 - NCNet pretrain, Epoch [106 / 300]: loss 0.3176, training auc: 0.8315, val_auc 0.7705\n",
      "2022-06-30 00:56:06,017 - NCNet pretrain, Epoch [107 / 300]: loss 0.3160, training auc: 0.8290, val_auc 0.7687\n",
      "2022-06-30 00:56:06,137 - NCNet pretrain, Epoch [108 / 300]: loss 0.3198, training auc: 0.8241, val_auc 0.7743\n",
      "2022-06-30 00:56:06,256 - NCNet pretrain, Epoch [109 / 300]: loss 0.3277, training auc: 0.8238, val_auc 0.7720\n",
      "2022-06-30 00:56:06,374 - NCNet pretrain, Epoch [110 / 300]: loss 0.3220, training auc: 0.8257, val_auc 0.7680\n",
      "2022-06-30 00:56:06,508 - NCNet pretrain, Epoch [111 / 300]: loss 0.3240, training auc: 0.8176, val_auc 0.7770, test auc 0.7899\n",
      "2022-06-30 00:56:06,628 - NCNet pretrain, Epoch [112 / 300]: loss 0.3222, training auc: 0.8272, val_auc 0.7735\n",
      "2022-06-30 00:56:06,746 - NCNet pretrain, Epoch [113 / 300]: loss 0.3238, training auc: 0.8253, val_auc 0.7679\n",
      "2022-06-30 00:56:06,876 - NCNet pretrain, Epoch [114 / 300]: loss 0.3216, training auc: 0.8200, val_auc 0.7794, test auc 0.7936\n",
      "2022-06-30 00:56:06,994 - NCNet pretrain, Epoch [115 / 300]: loss 0.3225, training auc: 0.8381, val_auc 0.7662\n",
      "2022-06-30 00:56:07,112 - NCNet pretrain, Epoch [116 / 300]: loss 0.3144, training auc: 0.8348, val_auc 0.7710\n",
      "2022-06-30 00:56:07,230 - NCNet pretrain, Epoch [117 / 300]: loss 0.3134, training auc: 0.8286, val_auc 0.7773\n",
      "2022-06-30 00:56:07,348 - NCNet pretrain, Epoch [118 / 300]: loss 0.3234, training auc: 0.8259, val_auc 0.7692\n",
      "2022-06-30 00:56:07,471 - NCNet pretrain, Epoch [119 / 300]: loss 0.3213, training auc: 0.8160, val_auc 0.7753\n",
      "2022-06-30 00:56:07,589 - NCNet pretrain, Epoch [120 / 300]: loss 0.3132, training auc: 0.8425, val_auc 0.7699\n",
      "2022-06-30 00:56:07,708 - NCNet pretrain, Epoch [121 / 300]: loss 0.3129, training auc: 0.8308, val_auc 0.7746\n",
      "2022-06-30 00:56:07,826 - NCNet pretrain, Epoch [122 / 300]: loss 0.3091, training auc: 0.8389, val_auc 0.7751\n",
      "2022-06-30 00:56:07,945 - NCNet pretrain, Epoch [123 / 300]: loss 0.3121, training auc: 0.8363, val_auc 0.7674\n",
      "2022-06-30 00:56:08,064 - NCNet pretrain, Epoch [124 / 300]: loss 0.3157, training auc: 0.8296, val_auc 0.7782\n",
      "2022-06-30 00:56:08,184 - NCNet pretrain, Epoch [125 / 300]: loss 0.3147, training auc: 0.8415, val_auc 0.7662\n",
      "2022-06-30 00:56:08,304 - NCNet pretrain, Epoch [126 / 300]: loss 0.3121, training auc: 0.8300, val_auc 0.7667\n",
      "2022-06-30 00:56:08,422 - NCNet pretrain, Epoch [127 / 300]: loss 0.3172, training auc: 0.8297, val_auc 0.7775\n",
      "2022-06-30 00:56:08,541 - NCNet pretrain, Epoch [128 / 300]: loss 0.3129, training auc: 0.8482, val_auc 0.7666\n",
      "2022-06-30 00:56:08,666 - NCNet pretrain, Epoch [129 / 300]: loss 0.3138, training auc: 0.8320, val_auc 0.7715\n",
      "2022-06-30 00:56:08,796 - NCNet pretrain, Epoch [130 / 300]: loss 0.3143, training auc: 0.8274, val_auc 0.7797, test auc 0.7977\n",
      "2022-06-30 00:56:08,914 - NCNet pretrain, Epoch [131 / 300]: loss 0.3204, training auc: 0.8471, val_auc 0.7582\n",
      "2022-06-30 00:56:09,034 - NCNet pretrain, Epoch [132 / 300]: loss 0.3229, training auc: 0.8276, val_auc 0.7726\n",
      "2022-06-30 00:56:09,163 - NCNet pretrain, Epoch [133 / 300]: loss 0.3110, training auc: 0.8357, val_auc 0.7802, test auc 0.7999\n",
      "2022-06-30 00:56:09,281 - NCNet pretrain, Epoch [134 / 300]: loss 0.3180, training auc: 0.8587, val_auc 0.7598\n",
      "2022-06-30 00:56:09,399 - NCNet pretrain, Epoch [135 / 300]: loss 0.3074, training auc: 0.8453, val_auc 0.7632\n",
      "2022-06-30 00:56:09,516 - NCNet pretrain, Epoch [136 / 300]: loss 0.3150, training auc: 0.8318, val_auc 0.7790\n",
      "2022-06-30 00:56:09,634 - NCNet pretrain, Epoch [137 / 300]: loss 0.3287, training auc: 0.8447, val_auc 0.7717\n",
      "2022-06-30 00:56:09,773 - NCNet pretrain, Epoch [138 / 300]: loss 0.3037, training auc: 0.8452, val_auc 0.7592\n",
      "2022-06-30 00:56:09,909 - NCNet pretrain, Epoch [139 / 300]: loss 0.3236, training auc: 0.8193, val_auc 0.7720\n",
      "2022-06-30 00:56:10,029 - NCNet pretrain, Epoch [140 / 300]: loss 0.3126, training auc: 0.8410, val_auc 0.7775\n",
      "2022-06-30 00:56:10,148 - NCNet pretrain, Epoch [141 / 300]: loss 0.3159, training auc: 0.8528, val_auc 0.7634\n",
      "2022-06-30 00:56:10,266 - NCNet pretrain, Epoch [142 / 300]: loss 0.3162, training auc: 0.8316, val_auc 0.7652\n",
      "2022-06-30 00:56:10,384 - NCNet pretrain, Epoch [143 / 300]: loss 0.3068, training auc: 0.8442, val_auc 0.7759\n",
      "2022-06-30 00:56:10,503 - NCNet pretrain, Epoch [144 / 300]: loss 0.3121, training auc: 0.8460, val_auc 0.7704\n",
      "2022-06-30 00:56:10,641 - NCNet pretrain, Epoch [145 / 300]: loss 0.3104, training auc: 0.8437, val_auc 0.7661\n",
      "2022-06-30 00:56:10,761 - NCNet pretrain, Epoch [146 / 300]: loss 0.3041, training auc: 0.8446, val_auc 0.7701\n",
      "2022-06-30 00:56:10,894 - NCNet pretrain, Epoch [147 / 300]: loss 0.3050, training auc: 0.8474, val_auc 0.7764\n",
      "2022-06-30 00:56:11,016 - NCNet pretrain, Epoch [148 / 300]: loss 0.3130, training auc: 0.8487, val_auc 0.7710\n",
      "2022-06-30 00:56:11,134 - NCNet pretrain, Epoch [149 / 300]: loss 0.2980, training auc: 0.8527, val_auc 0.7644\n",
      "2022-06-30 00:56:11,251 - NCNet pretrain, Epoch [150 / 300]: loss 0.3007, training auc: 0.8470, val_auc 0.7709\n",
      "2022-06-30 00:56:11,370 - NCNet pretrain, Epoch [151 / 300]: loss 0.3068, training auc: 0.8405, val_auc 0.7770\n",
      "2022-06-30 00:56:11,505 - NCNet pretrain, Epoch [152 / 300]: loss 0.3170, training auc: 0.8488, val_auc 0.7625\n",
      "2022-06-30 00:56:11,641 - NCNet pretrain, Epoch [153 / 300]: loss 0.3033, training auc: 0.8442, val_auc 0.7668\n",
      "2022-06-30 00:56:11,758 - NCNet pretrain, Epoch [154 / 300]: loss 0.3017, training auc: 0.8425, val_auc 0.7770\n",
      "2022-06-30 00:56:11,876 - NCNet pretrain, Epoch [155 / 300]: loss 0.3112, training auc: 0.8543, val_auc 0.7707\n",
      "2022-06-30 00:56:11,995 - NCNet pretrain, Epoch [156 / 300]: loss 0.3093, training auc: 0.8408, val_auc 0.7635\n",
      "2022-06-30 00:56:12,113 - NCNet pretrain, Epoch [157 / 300]: loss 0.3009, training auc: 0.8535, val_auc 0.7723\n",
      "2022-06-30 00:56:12,231 - NCNet pretrain, Epoch [158 / 300]: loss 0.2966, training auc: 0.8585, val_auc 0.7759\n",
      "2022-06-30 00:56:12,349 - NCNet pretrain, Epoch [159 / 300]: loss 0.3061, training auc: 0.8628, val_auc 0.7655\n",
      "2022-06-30 00:56:12,468 - NCNet pretrain, Epoch [160 / 300]: loss 0.2955, training auc: 0.8534, val_auc 0.7650\n",
      "2022-06-30 00:56:12,602 - NCNet pretrain, Epoch [161 / 300]: loss 0.3071, training auc: 0.8473, val_auc 0.7751\n",
      "2022-06-30 00:56:12,720 - NCNet pretrain, Epoch [162 / 300]: loss 0.3075, training auc: 0.8519, val_auc 0.7737\n",
      "2022-06-30 00:56:12,838 - NCNet pretrain, Epoch [163 / 300]: loss 0.2985, training auc: 0.8612, val_auc 0.7632\n",
      "2022-06-30 00:56:12,960 - NCNet pretrain, Epoch [164 / 300]: loss 0.3027, training auc: 0.8454, val_auc 0.7700\n",
      "2022-06-30 00:56:13,083 - NCNet pretrain, Epoch [165 / 300]: loss 0.2981, training auc: 0.8544, val_auc 0.7757\n",
      "2022-06-30 00:56:13,200 - NCNet pretrain, Epoch [166 / 300]: loss 0.3075, training auc: 0.8615, val_auc 0.7677\n",
      "2022-06-30 00:56:13,318 - NCNet pretrain, Epoch [167 / 300]: loss 0.2969, training auc: 0.8556, val_auc 0.7646\n",
      "2022-06-30 00:56:13,435 - NCNet pretrain, Epoch [168 / 300]: loss 0.3013, training auc: 0.8509, val_auc 0.7747\n",
      "2022-06-30 00:56:13,572 - NCNet pretrain, Epoch [169 / 300]: loss 0.3063, training auc: 0.8553, val_auc 0.7733\n",
      "2022-06-30 00:56:13,689 - NCNet pretrain, Epoch [170 / 300]: loss 0.3037, training auc: 0.8571, val_auc 0.7683\n",
      "2022-06-30 00:56:13,807 - NCNet pretrain, Epoch [171 / 300]: loss 0.2898, training auc: 0.8621, val_auc 0.7680\n",
      "2022-06-30 00:56:13,940 - NCNet pretrain, Epoch [172 / 300]: loss 0.3011, training auc: 0.8496, val_auc 0.7741\n",
      "2022-06-30 00:56:14,072 - NCNet pretrain, Epoch [173 / 300]: loss 0.2999, training auc: 0.8643, val_auc 0.7710\n",
      "2022-06-30 00:56:14,198 - NCNet pretrain, Epoch [174 / 300]: loss 0.2889, training auc: 0.8645, val_auc 0.7691\n",
      "2022-06-30 00:56:14,316 - NCNet pretrain, Epoch [175 / 300]: loss 0.2934, training auc: 0.8594, val_auc 0.7725\n",
      "2022-06-30 00:56:14,434 - NCNet pretrain, Epoch [176 / 300]: loss 0.3004, training auc: 0.8587, val_auc 0.7735\n",
      "2022-06-30 00:56:14,553 - NCNet pretrain, Epoch [177 / 300]: loss 0.3034, training auc: 0.8558, val_auc 0.7691\n",
      "2022-06-30 00:56:14,673 - NCNet pretrain, Epoch [178 / 300]: loss 0.2982, training auc: 0.8554, val_auc 0.7720\n",
      "2022-06-30 00:56:14,793 - NCNet pretrain, Epoch [179 / 300]: loss 0.2808, training auc: 0.8725, val_auc 0.7729\n",
      "2022-06-30 00:56:14,931 - NCNet pretrain, Epoch [180 / 300]: loss 0.2952, training auc: 0.8586, val_auc 0.7707\n",
      "2022-06-30 00:56:15,068 - NCNet pretrain, Epoch [181 / 300]: loss 0.2851, training auc: 0.8699, val_auc 0.7699\n",
      "2022-06-30 00:56:15,190 - NCNet pretrain, Epoch [182 / 300]: loss 0.2885, training auc: 0.8679, val_auc 0.7727\n",
      "2022-06-30 00:56:15,307 - NCNet pretrain, Epoch [183 / 300]: loss 0.2893, training auc: 0.8708, val_auc 0.7710\n",
      "2022-06-30 00:56:15,308 - Early stop!\n",
      "2022-06-30 00:56:15,309 - Best Test Results: auc 0.7999, ap 0.4249, f1 0.3229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 249887.24it/s]\n",
      "2022-06-30 00:56:16,446 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:56:16,626 - NCNet pretrain, Epoch [1 / 300]: loss 0.8572, training auc: 0.5209, val_auc 0.4722, test auc 0.4471\n",
      "2022-06-30 00:56:16,744 - NCNet pretrain, Epoch [2 / 300]: loss 0.5538, training auc: 0.4130, val_auc 0.4679\n",
      "2022-06-30 00:56:16,868 - NCNet pretrain, Epoch [3 / 300]: loss 0.4790, training auc: 0.4453, val_auc 0.4655\n",
      "2022-06-30 00:56:16,986 - NCNet pretrain, Epoch [4 / 300]: loss 0.5317, training auc: 0.4112, val_auc 0.4654\n",
      "2022-06-30 00:56:17,106 - NCNet pretrain, Epoch [5 / 300]: loss 0.5313, training auc: 0.4515, val_auc 0.4674\n",
      "2022-06-30 00:56:17,227 - NCNet pretrain, Epoch [6 / 300]: loss 0.5215, training auc: 0.4308, val_auc 0.4695\n",
      "2022-06-30 00:56:17,378 - NCNet pretrain, Epoch [7 / 300]: loss 0.5019, training auc: 0.4158, val_auc 0.4737, test auc 0.4511\n",
      "2022-06-30 00:56:17,507 - NCNet pretrain, Epoch [8 / 300]: loss 0.4724, training auc: 0.4792, val_auc 0.4783, test auc 0.4587\n",
      "2022-06-30 00:56:17,636 - NCNet pretrain, Epoch [9 / 300]: loss 0.4812, training auc: 0.4927, val_auc 0.4810, test auc 0.4621\n",
      "2022-06-30 00:56:17,755 - NCNet pretrain, Epoch [10 / 300]: loss 0.4946, training auc: 0.4568, val_auc 0.4810\n",
      "2022-06-30 00:56:17,873 - NCNet pretrain, Epoch [11 / 300]: loss 0.4868, training auc: 0.4769, val_auc 0.4800\n",
      "2022-06-30 00:56:17,991 - NCNet pretrain, Epoch [12 / 300]: loss 0.4772, training auc: 0.4597, val_auc 0.4791\n",
      "2022-06-30 00:56:18,110 - NCNet pretrain, Epoch [13 / 300]: loss 0.4789, training auc: 0.4353, val_auc 0.4789\n",
      "2022-06-30 00:56:18,240 - NCNet pretrain, Epoch [14 / 300]: loss 0.4751, training auc: 0.4612, val_auc 0.4793\n",
      "2022-06-30 00:56:18,358 - NCNet pretrain, Epoch [15 / 300]: loss 0.4667, training auc: 0.4862, val_auc 0.4798\n",
      "2022-06-30 00:56:18,476 - NCNet pretrain, Epoch [16 / 300]: loss 0.4904, training auc: 0.4166, val_auc 0.4808\n",
      "2022-06-30 00:56:18,600 - NCNet pretrain, Epoch [17 / 300]: loss 0.4655, training auc: 0.4949, val_auc 0.4820, test auc 0.4594\n",
      "2022-06-30 00:56:18,724 - NCNet pretrain, Epoch [18 / 300]: loss 0.4725, training auc: 0.4618, val_auc 0.4829, test auc 0.4608\n",
      "2022-06-30 00:56:18,849 - NCNet pretrain, Epoch [19 / 300]: loss 0.4709, training auc: 0.4553, val_auc 0.4837, test auc 0.4620\n",
      "2022-06-30 00:56:18,978 - NCNet pretrain, Epoch [20 / 300]: loss 0.4744, training auc: 0.4395, val_auc 0.4847, test auc 0.4634\n",
      "2022-06-30 00:56:19,112 - NCNet pretrain, Epoch [21 / 300]: loss 0.4737, training auc: 0.4423, val_auc 0.4856, test auc 0.4645\n",
      "2022-06-30 00:56:19,241 - NCNet pretrain, Epoch [22 / 300]: loss 0.4730, training auc: 0.4595, val_auc 0.4869, test auc 0.4654\n",
      "2022-06-30 00:56:19,370 - NCNet pretrain, Epoch [23 / 300]: loss 0.4617, training auc: 0.4910, val_auc 0.4878, test auc 0.4660\n",
      "2022-06-30 00:56:19,504 - NCNet pretrain, Epoch [24 / 300]: loss 0.4660, training auc: 0.4754, val_auc 0.4884, test auc 0.4663\n",
      "2022-06-30 00:56:19,627 - NCNet pretrain, Epoch [25 / 300]: loss 0.4596, training auc: 0.4900, val_auc 0.4891, test auc 0.4663\n",
      "2022-06-30 00:56:19,755 - NCNet pretrain, Epoch [26 / 300]: loss 0.4625, training auc: 0.4704, val_auc 0.4900, test auc 0.4664\n",
      "2022-06-30 00:56:19,884 - NCNet pretrain, Epoch [27 / 300]: loss 0.4705, training auc: 0.4399, val_auc 0.4912, test auc 0.4670\n",
      "2022-06-30 00:56:20,012 - NCNet pretrain, Epoch [28 / 300]: loss 0.4553, training auc: 0.4841, val_auc 0.4925, test auc 0.4680\n",
      "2022-06-30 00:56:20,159 - NCNet pretrain, Epoch [29 / 300]: loss 0.4516, training auc: 0.5043, val_auc 0.4940, test auc 0.4693\n",
      "2022-06-30 00:56:20,306 - NCNet pretrain, Epoch [30 / 300]: loss 0.4490, training auc: 0.5134, val_auc 0.4954, test auc 0.4708\n",
      "2022-06-30 00:56:20,434 - NCNet pretrain, Epoch [31 / 300]: loss 0.4563, training auc: 0.4780, val_auc 0.4976, test auc 0.4730\n",
      "2022-06-30 00:56:20,563 - NCNet pretrain, Epoch [32 / 300]: loss 0.4482, training auc: 0.5080, val_auc 0.4996, test auc 0.4752\n",
      "2022-06-30 00:56:20,692 - NCNet pretrain, Epoch [33 / 300]: loss 0.4557, training auc: 0.4669, val_auc 0.5018, test auc 0.4777\n",
      "2022-06-30 00:56:20,821 - NCNet pretrain, Epoch [34 / 300]: loss 0.4536, training auc: 0.4833, val_auc 0.5045, test auc 0.4804\n",
      "2022-06-30 00:56:20,950 - NCNet pretrain, Epoch [35 / 300]: loss 0.4562, training auc: 0.4563, val_auc 0.5080, test auc 0.4837\n",
      "2022-06-30 00:56:21,081 - NCNet pretrain, Epoch [36 / 300]: loss 0.4449, training auc: 0.5253, val_auc 0.5107, test auc 0.4864\n",
      "2022-06-30 00:56:21,211 - NCNet pretrain, Epoch [37 / 300]: loss 0.4418, training auc: 0.5305, val_auc 0.5130, test auc 0.4883\n",
      "2022-06-30 00:56:21,341 - NCNet pretrain, Epoch [38 / 300]: loss 0.4513, training auc: 0.4873, val_auc 0.5162, test auc 0.4907\n",
      "2022-06-30 00:56:21,470 - NCNet pretrain, Epoch [39 / 300]: loss 0.4535, training auc: 0.4648, val_auc 0.5196, test auc 0.4940\n",
      "2022-06-30 00:56:21,605 - NCNet pretrain, Epoch [40 / 300]: loss 0.4394, training auc: 0.5282, val_auc 0.5234, test auc 0.4977\n",
      "2022-06-30 00:56:21,734 - NCNet pretrain, Epoch [41 / 300]: loss 0.4493, training auc: 0.4757, val_auc 0.5288, test auc 0.5031\n",
      "2022-06-30 00:56:21,863 - NCNet pretrain, Epoch [42 / 300]: loss 0.4454, training auc: 0.4898, val_auc 0.5350, test auc 0.5099\n",
      "2022-06-30 00:56:21,992 - NCNet pretrain, Epoch [43 / 300]: loss 0.4432, training auc: 0.4896, val_auc 0.5432, test auc 0.5187\n",
      "2022-06-30 00:56:22,139 - NCNet pretrain, Epoch [44 / 300]: loss 0.4448, training auc: 0.4894, val_auc 0.5525, test auc 0.5290\n",
      "2022-06-30 00:56:22,267 - NCNet pretrain, Epoch [45 / 300]: loss 0.4364, training auc: 0.5223, val_auc 0.5619, test auc 0.5396\n",
      "2022-06-30 00:56:22,402 - NCNet pretrain, Epoch [46 / 300]: loss 0.4482, training auc: 0.4734, val_auc 0.5721, test auc 0.5505\n",
      "2022-06-30 00:56:22,530 - NCNet pretrain, Epoch [47 / 300]: loss 0.4351, training auc: 0.5405, val_auc 0.5801, test auc 0.5581\n",
      "2022-06-30 00:56:22,658 - NCNet pretrain, Epoch [48 / 300]: loss 0.4261, training auc: 0.5739, val_auc 0.5870, test auc 0.5639\n",
      "2022-06-30 00:56:22,788 - NCNet pretrain, Epoch [49 / 300]: loss 0.4202, training auc: 0.5877, val_auc 0.5948, test auc 0.5723\n",
      "2022-06-30 00:56:22,937 - NCNet pretrain, Epoch [50 / 300]: loss 0.4164, training auc: 0.6082, val_auc 0.6091, test auc 0.5871\n",
      "2022-06-30 00:56:23,086 - NCNet pretrain, Epoch [51 / 300]: loss 0.4085, training auc: 0.6324, val_auc 0.6270, test auc 0.6053\n",
      "2022-06-30 00:56:23,216 - NCNet pretrain, Epoch [52 / 300]: loss 0.4116, training auc: 0.6057, val_auc 0.6534, test auc 0.6303\n",
      "2022-06-30 00:56:23,366 - NCNet pretrain, Epoch [53 / 300]: loss 0.4055, training auc: 0.6684, val_auc 0.6558, test auc 0.6314\n",
      "2022-06-30 00:56:23,502 - NCNet pretrain, Epoch [54 / 300]: loss 0.4053, training auc: 0.6500, val_auc 0.6530\n",
      "2022-06-30 00:56:23,631 - NCNet pretrain, Epoch [55 / 300]: loss 0.4099, training auc: 0.6233, val_auc 0.6625, test auc 0.6356\n",
      "2022-06-30 00:56:23,767 - NCNet pretrain, Epoch [56 / 300]: loss 0.3997, training auc: 0.6540, val_auc 0.6829, test auc 0.6560\n",
      "2022-06-30 00:56:23,896 - NCNet pretrain, Epoch [57 / 300]: loss 0.3995, training auc: 0.6680, val_auc 0.7011, test auc 0.6759\n",
      "2022-06-30 00:56:24,024 - NCNet pretrain, Epoch [58 / 300]: loss 0.3972, training auc: 0.6515, val_auc 0.7120, test auc 0.6897\n",
      "2022-06-30 00:56:24,144 - NCNet pretrain, Epoch [59 / 300]: loss 0.3948, training auc: 0.6830, val_auc 0.7120\n",
      "2022-06-30 00:56:24,273 - NCNet pretrain, Epoch [60 / 300]: loss 0.4061, training auc: 0.6343, val_auc 0.7185, test auc 0.6976\n",
      "2022-06-30 00:56:24,402 - NCNet pretrain, Epoch [61 / 300]: loss 0.3898, training auc: 0.6898, val_auc 0.7258, test auc 0.7087\n",
      "2022-06-30 00:56:24,531 - NCNet pretrain, Epoch [62 / 300]: loss 0.3927, training auc: 0.6944, val_auc 0.7310, test auc 0.7173\n",
      "2022-06-30 00:56:24,659 - NCNet pretrain, Epoch [63 / 300]: loss 0.3862, training auc: 0.7129, val_auc 0.7321, test auc 0.7192\n",
      "2022-06-30 00:56:24,776 - NCNet pretrain, Epoch [64 / 300]: loss 0.3828, training auc: 0.7178, val_auc 0.7303\n",
      "2022-06-30 00:56:24,905 - NCNet pretrain, Epoch [65 / 300]: loss 0.3741, training auc: 0.7325, val_auc 0.7327, test auc 0.7197\n",
      "2022-06-30 00:56:25,033 - NCNet pretrain, Epoch [66 / 300]: loss 0.3741, training auc: 0.7234, val_auc 0.7357, test auc 0.7247\n",
      "2022-06-30 00:56:25,163 - NCNet pretrain, Epoch [67 / 300]: loss 0.3752, training auc: 0.7257, val_auc 0.7408, test auc 0.7341\n",
      "2022-06-30 00:56:25,280 - NCNet pretrain, Epoch [68 / 300]: loss 0.3679, training auc: 0.7479, val_auc 0.7395\n",
      "2022-06-30 00:56:25,399 - NCNet pretrain, Epoch [69 / 300]: loss 0.3683, training auc: 0.7376, val_auc 0.7379\n",
      "2022-06-30 00:56:25,539 - NCNet pretrain, Epoch [70 / 300]: loss 0.3688, training auc: 0.7340, val_auc 0.7449, test auc 0.7389\n",
      "2022-06-30 00:56:25,673 - NCNet pretrain, Epoch [71 / 300]: loss 0.3717, training auc: 0.7358, val_auc 0.7506, test auc 0.7496\n",
      "2022-06-30 00:56:25,791 - NCNet pretrain, Epoch [72 / 300]: loss 0.3718, training auc: 0.7486, val_auc 0.7484\n",
      "2022-06-30 00:56:25,919 - NCNet pretrain, Epoch [73 / 300]: loss 0.3599, training auc: 0.7570, val_auc 0.7442\n",
      "2022-06-30 00:56:26,049 - NCNet pretrain, Epoch [74 / 300]: loss 0.3673, training auc: 0.7433, val_auc 0.7531, test auc 0.7512\n",
      "2022-06-30 00:56:26,179 - NCNet pretrain, Epoch [75 / 300]: loss 0.3576, training auc: 0.7610, val_auc 0.7598, test auc 0.7664\n",
      "2022-06-30 00:56:26,297 - NCNet pretrain, Epoch [76 / 300]: loss 0.3630, training auc: 0.7755, val_auc 0.7532\n",
      "2022-06-30 00:56:26,414 - NCNet pretrain, Epoch [77 / 300]: loss 0.3628, training auc: 0.7517, val_auc 0.7524\n",
      "2022-06-30 00:56:26,543 - NCNet pretrain, Epoch [78 / 300]: loss 0.3566, training auc: 0.7613, val_auc 0.7617, test auc 0.7676\n",
      "2022-06-30 00:56:26,672 - NCNet pretrain, Epoch [79 / 300]: loss 0.3540, training auc: 0.7798, val_auc 0.7624, test auc 0.7685\n",
      "2022-06-30 00:56:26,790 - NCNet pretrain, Epoch [80 / 300]: loss 0.3625, training auc: 0.7556, val_auc 0.7593\n",
      "2022-06-30 00:56:26,908 - NCNet pretrain, Epoch [81 / 300]: loss 0.3536, training auc: 0.7688, val_auc 0.7623\n",
      "2022-06-30 00:56:27,037 - NCNet pretrain, Epoch [82 / 300]: loss 0.3532, training auc: 0.7693, val_auc 0.7675, test auc 0.7774\n",
      "2022-06-30 00:56:27,156 - NCNet pretrain, Epoch [83 / 300]: loss 0.3512, training auc: 0.7834, val_auc 0.7650\n",
      "2022-06-30 00:56:27,294 - NCNet pretrain, Epoch [84 / 300]: loss 0.3431, training auc: 0.7889, val_auc 0.7624\n",
      "2022-06-30 00:56:27,437 - NCNet pretrain, Epoch [85 / 300]: loss 0.3443, training auc: 0.7873, val_auc 0.7680, test auc 0.7755\n",
      "2022-06-30 00:56:27,566 - NCNet pretrain, Epoch [86 / 300]: loss 0.3396, training auc: 0.7938, val_auc 0.7712, test auc 0.7816\n",
      "2022-06-30 00:56:27,705 - NCNet pretrain, Epoch [87 / 300]: loss 0.3461, training auc: 0.7950, val_auc 0.7658\n",
      "2022-06-30 00:56:27,823 - NCNet pretrain, Epoch [88 / 300]: loss 0.3491, training auc: 0.7790, val_auc 0.7695\n",
      "2022-06-30 00:56:27,942 - NCNet pretrain, Epoch [89 / 300]: loss 0.3421, training auc: 0.7965, val_auc 0.7710\n",
      "2022-06-30 00:56:28,066 - NCNet pretrain, Epoch [90 / 300]: loss 0.3583, training auc: 0.7702, val_auc 0.7705\n",
      "2022-06-30 00:56:28,190 - NCNet pretrain, Epoch [91 / 300]: loss 0.3331, training auc: 0.8020, val_auc 0.7683\n",
      "2022-06-30 00:56:28,307 - NCNet pretrain, Epoch [92 / 300]: loss 0.3428, training auc: 0.7847, val_auc 0.7708\n",
      "2022-06-30 00:56:28,436 - NCNet pretrain, Epoch [93 / 300]: loss 0.3299, training auc: 0.8040, val_auc 0.7730, test auc 0.7851\n",
      "2022-06-30 00:56:28,574 - NCNet pretrain, Epoch [94 / 300]: loss 0.3352, training auc: 0.8091, val_auc 0.7697\n",
      "2022-06-30 00:56:28,691 - NCNet pretrain, Epoch [95 / 300]: loss 0.3410, training auc: 0.7853, val_auc 0.7678\n",
      "2022-06-30 00:56:28,809 - NCNet pretrain, Epoch [96 / 300]: loss 0.3416, training auc: 0.7914, val_auc 0.7725\n",
      "2022-06-30 00:56:28,945 - NCNet pretrain, Epoch [97 / 300]: loss 0.3258, training auc: 0.8182, val_auc 0.7729\n",
      "2022-06-30 00:56:29,062 - NCNet pretrain, Epoch [98 / 300]: loss 0.3368, training auc: 0.8020, val_auc 0.7672\n",
      "2022-06-30 00:56:29,191 - NCNet pretrain, Epoch [99 / 300]: loss 0.3402, training auc: 0.7938, val_auc 0.7733, test auc 0.7868\n",
      "2022-06-30 00:56:29,320 - NCNet pretrain, Epoch [100 / 300]: loss 0.3408, training auc: 0.7992, val_auc 0.7748, test auc 0.7898\n",
      "2022-06-30 00:56:29,439 - NCNet pretrain, Epoch [101 / 300]: loss 0.3229, training auc: 0.8250, val_auc 0.7714\n",
      "2022-06-30 00:56:29,557 - NCNet pretrain, Epoch [102 / 300]: loss 0.3276, training auc: 0.8075, val_auc 0.7723\n",
      "2022-06-30 00:56:29,686 - NCNet pretrain, Epoch [103 / 300]: loss 0.3405, training auc: 0.7987, val_auc 0.7766, test auc 0.7943\n",
      "2022-06-30 00:56:29,806 - NCNet pretrain, Epoch [104 / 300]: loss 0.3370, training auc: 0.8151, val_auc 0.7754\n",
      "2022-06-30 00:56:29,925 - NCNet pretrain, Epoch [105 / 300]: loss 0.3240, training auc: 0.8190, val_auc 0.7734\n",
      "2022-06-30 00:56:30,062 - NCNet pretrain, Epoch [106 / 300]: loss 0.3246, training auc: 0.8190, val_auc 0.7753\n",
      "2022-06-30 00:56:30,217 - NCNet pretrain, Epoch [107 / 300]: loss 0.3320, training auc: 0.8082, val_auc 0.7778, test auc 0.7950\n",
      "2022-06-30 00:56:30,354 - NCNet pretrain, Epoch [108 / 300]: loss 0.3296, training auc: 0.8187, val_auc 0.7753\n",
      "2022-06-30 00:56:30,491 - NCNet pretrain, Epoch [109 / 300]: loss 0.3174, training auc: 0.8234, val_auc 0.7771\n",
      "2022-06-30 00:56:30,608 - NCNet pretrain, Epoch [110 / 300]: loss 0.3217, training auc: 0.8253, val_auc 0.7775\n",
      "2022-06-30 00:56:30,726 - NCNet pretrain, Epoch [111 / 300]: loss 0.3194, training auc: 0.8287, val_auc 0.7757\n",
      "2022-06-30 00:56:30,844 - NCNet pretrain, Epoch [112 / 300]: loss 0.3253, training auc: 0.8173, val_auc 0.7775\n",
      "2022-06-30 00:56:30,981 - NCNet pretrain, Epoch [113 / 300]: loss 0.3224, training auc: 0.8254, val_auc 0.7772\n",
      "2022-06-30 00:56:31,118 - NCNet pretrain, Epoch [114 / 300]: loss 0.3197, training auc: 0.8230, val_auc 0.7777\n",
      "2022-06-30 00:56:31,254 - NCNet pretrain, Epoch [115 / 300]: loss 0.3154, training auc: 0.8328, val_auc 0.7784, test auc 0.7964\n",
      "2022-06-30 00:56:31,393 - NCNet pretrain, Epoch [116 / 300]: loss 0.3170, training auc: 0.8279, val_auc 0.7770\n",
      "2022-06-30 00:56:31,522 - NCNet pretrain, Epoch [117 / 300]: loss 0.3259, training auc: 0.8151, val_auc 0.7787, test auc 0.7969\n",
      "2022-06-30 00:56:31,652 - NCNet pretrain, Epoch [118 / 300]: loss 0.3146, training auc: 0.8304, val_auc 0.7801, test auc 0.8000\n",
      "2022-06-30 00:56:31,771 - NCNet pretrain, Epoch [119 / 300]: loss 0.3236, training auc: 0.8201, val_auc 0.7780\n",
      "2022-06-30 00:56:31,891 - NCNet pretrain, Epoch [120 / 300]: loss 0.3133, training auc: 0.8326, val_auc 0.7770\n",
      "2022-06-30 00:56:32,020 - NCNet pretrain, Epoch [121 / 300]: loss 0.3231, training auc: 0.8228, val_auc 0.7805, test auc 0.8014\n",
      "2022-06-30 00:56:32,141 - NCNet pretrain, Epoch [122 / 300]: loss 0.3218, training auc: 0.8300, val_auc 0.7763\n",
      "2022-06-30 00:56:32,267 - NCNet pretrain, Epoch [123 / 300]: loss 0.3141, training auc: 0.8316, val_auc 0.7794\n",
      "2022-06-30 00:56:32,394 - NCNet pretrain, Epoch [124 / 300]: loss 0.3140, training auc: 0.8354, val_auc 0.7805, test auc 0.8002\n",
      "2022-06-30 00:56:32,512 - NCNet pretrain, Epoch [125 / 300]: loss 0.3235, training auc: 0.8222, val_auc 0.7765\n",
      "2022-06-30 00:56:32,654 - NCNet pretrain, Epoch [126 / 300]: loss 0.3100, training auc: 0.8345, val_auc 0.7807, test auc 0.8006\n",
      "2022-06-30 00:56:32,787 - NCNet pretrain, Epoch [127 / 300]: loss 0.3119, training auc: 0.8437, val_auc 0.7769\n",
      "2022-06-30 00:56:32,904 - NCNet pretrain, Epoch [128 / 300]: loss 0.3123, training auc: 0.8292, val_auc 0.7783\n",
      "2022-06-30 00:56:33,021 - NCNet pretrain, Epoch [129 / 300]: loss 0.3161, training auc: 0.8272, val_auc 0.7805\n",
      "2022-06-30 00:56:33,139 - NCNet pretrain, Epoch [130 / 300]: loss 0.3072, training auc: 0.8421, val_auc 0.7797\n",
      "2022-06-30 00:56:33,259 - NCNet pretrain, Epoch [131 / 300]: loss 0.3084, training auc: 0.8336, val_auc 0.7772\n",
      "2022-06-30 00:56:33,388 - NCNet pretrain, Epoch [132 / 300]: loss 0.3126, training auc: 0.8336, val_auc 0.7821, test auc 0.8044\n",
      "2022-06-30 00:56:33,506 - NCNet pretrain, Epoch [133 / 300]: loss 0.3124, training auc: 0.8482, val_auc 0.7784\n",
      "2022-06-30 00:56:33,624 - NCNet pretrain, Epoch [134 / 300]: loss 0.3165, training auc: 0.8235, val_auc 0.7805\n",
      "2022-06-30 00:56:33,742 - NCNet pretrain, Epoch [135 / 300]: loss 0.3022, training auc: 0.8469, val_auc 0.7784\n",
      "2022-06-30 00:56:33,860 - NCNet pretrain, Epoch [136 / 300]: loss 0.3086, training auc: 0.8385, val_auc 0.7809\n",
      "2022-06-30 00:56:33,979 - NCNet pretrain, Epoch [137 / 300]: loss 0.3105, training auc: 0.8419, val_auc 0.7763\n",
      "2022-06-30 00:56:34,098 - NCNet pretrain, Epoch [138 / 300]: loss 0.3150, training auc: 0.8295, val_auc 0.7807\n",
      "2022-06-30 00:56:34,216 - NCNet pretrain, Epoch [139 / 300]: loss 0.3099, training auc: 0.8397, val_auc 0.7819\n",
      "2022-06-30 00:56:34,336 - NCNet pretrain, Epoch [140 / 300]: loss 0.3096, training auc: 0.8427, val_auc 0.7743\n",
      "2022-06-30 00:56:34,488 - NCNet pretrain, Epoch [141 / 300]: loss 0.3146, training auc: 0.8384, val_auc 0.7825, test auc 0.8092\n",
      "2022-06-30 00:56:34,606 - NCNet pretrain, Epoch [142 / 300]: loss 0.3221, training auc: 0.8481, val_auc 0.7756\n",
      "2022-06-30 00:56:34,724 - NCNet pretrain, Epoch [143 / 300]: loss 0.3076, training auc: 0.8385, val_auc 0.7755\n",
      "2022-06-30 00:56:34,842 - NCNet pretrain, Epoch [144 / 300]: loss 0.3113, training auc: 0.8389, val_auc 0.7817\n",
      "2022-06-30 00:56:34,978 - NCNet pretrain, Epoch [145 / 300]: loss 0.3152, training auc: 0.8363, val_auc 0.7787\n",
      "2022-06-30 00:56:35,113 - NCNet pretrain, Epoch [146 / 300]: loss 0.2991, training auc: 0.8489, val_auc 0.7729\n",
      "2022-06-30 00:56:35,251 - NCNet pretrain, Epoch [147 / 300]: loss 0.3125, training auc: 0.8302, val_auc 0.7812\n",
      "2022-06-30 00:56:35,389 - NCNet pretrain, Epoch [148 / 300]: loss 0.3105, training auc: 0.8411, val_auc 0.7814\n",
      "2022-06-30 00:56:35,524 - NCNet pretrain, Epoch [149 / 300]: loss 0.3009, training auc: 0.8616, val_auc 0.7746\n",
      "2022-06-30 00:56:35,644 - NCNet pretrain, Epoch [150 / 300]: loss 0.3058, training auc: 0.8423, val_auc 0.7789\n",
      "2022-06-30 00:56:35,763 - NCNet pretrain, Epoch [151 / 300]: loss 0.3006, training auc: 0.8502, val_auc 0.7807\n",
      "2022-06-30 00:56:35,882 - NCNet pretrain, Epoch [152 / 300]: loss 0.2969, training auc: 0.8541, val_auc 0.7773\n",
      "2022-06-30 00:56:36,002 - NCNet pretrain, Epoch [153 / 300]: loss 0.3031, training auc: 0.8446, val_auc 0.7797\n",
      "2022-06-30 00:56:36,122 - NCNet pretrain, Epoch [154 / 300]: loss 0.2997, training auc: 0.8523, val_auc 0.7802\n",
      "2022-06-30 00:56:36,240 - NCNet pretrain, Epoch [155 / 300]: loss 0.3025, training auc: 0.8495, val_auc 0.7783\n",
      "2022-06-30 00:56:36,362 - NCNet pretrain, Epoch [156 / 300]: loss 0.3006, training auc: 0.8541, val_auc 0.7789\n",
      "2022-06-30 00:56:36,486 - NCNet pretrain, Epoch [157 / 300]: loss 0.3067, training auc: 0.8494, val_auc 0.7794\n",
      "2022-06-30 00:56:36,608 - NCNet pretrain, Epoch [158 / 300]: loss 0.2967, training auc: 0.8661, val_auc 0.7761\n",
      "2022-06-30 00:56:36,727 - NCNet pretrain, Epoch [159 / 300]: loss 0.3048, training auc: 0.8474, val_auc 0.7789\n",
      "2022-06-30 00:56:36,845 - NCNet pretrain, Epoch [160 / 300]: loss 0.2971, training auc: 0.8573, val_auc 0.7803\n",
      "2022-06-30 00:56:36,962 - NCNet pretrain, Epoch [161 / 300]: loss 0.3000, training auc: 0.8532, val_auc 0.7778\n",
      "2022-06-30 00:56:37,081 - NCNet pretrain, Epoch [162 / 300]: loss 0.2955, training auc: 0.8511, val_auc 0.7791\n",
      "2022-06-30 00:56:37,219 - NCNet pretrain, Epoch [163 / 300]: loss 0.2931, training auc: 0.8561, val_auc 0.7805\n",
      "2022-06-30 00:56:37,337 - NCNet pretrain, Epoch [164 / 300]: loss 0.2954, training auc: 0.8583, val_auc 0.7762\n",
      "2022-06-30 00:56:37,454 - NCNet pretrain, Epoch [165 / 300]: loss 0.3012, training auc: 0.8547, val_auc 0.7808\n",
      "2022-06-30 00:56:37,572 - NCNet pretrain, Epoch [166 / 300]: loss 0.3035, training auc: 0.8631, val_auc 0.7724\n",
      "2022-06-30 00:56:37,690 - NCNet pretrain, Epoch [167 / 300]: loss 0.3078, training auc: 0.8454, val_auc 0.7787\n",
      "2022-06-30 00:56:37,827 - NCNet pretrain, Epoch [168 / 300]: loss 0.2978, training auc: 0.8547, val_auc 0.7798\n",
      "2022-06-30 00:56:37,946 - NCNet pretrain, Epoch [169 / 300]: loss 0.2939, training auc: 0.8667, val_auc 0.7742\n",
      "2022-06-30 00:56:38,063 - NCNet pretrain, Epoch [170 / 300]: loss 0.2972, training auc: 0.8516, val_auc 0.7769\n",
      "2022-06-30 00:56:38,181 - NCNet pretrain, Epoch [171 / 300]: loss 0.2897, training auc: 0.8657, val_auc 0.7782\n",
      "2022-06-30 00:56:38,302 - NCNet pretrain, Epoch [172 / 300]: loss 0.2893, training auc: 0.8683, val_auc 0.7771\n",
      "2022-06-30 00:56:38,420 - NCNet pretrain, Epoch [173 / 300]: loss 0.2876, training auc: 0.8655, val_auc 0.7757\n",
      "2022-06-30 00:56:38,538 - NCNet pretrain, Epoch [174 / 300]: loss 0.2920, training auc: 0.8611, val_auc 0.7783\n",
      "2022-06-30 00:56:38,664 - NCNet pretrain, Epoch [175 / 300]: loss 0.2854, training auc: 0.8728, val_auc 0.7761\n",
      "2022-06-30 00:56:38,781 - NCNet pretrain, Epoch [176 / 300]: loss 0.2955, training auc: 0.8580, val_auc 0.7776\n",
      "2022-06-30 00:56:38,899 - NCNet pretrain, Epoch [177 / 300]: loss 0.2902, training auc: 0.8659, val_auc 0.7746\n",
      "2022-06-30 00:56:39,017 - NCNet pretrain, Epoch [178 / 300]: loss 0.2830, training auc: 0.8697, val_auc 0.7754\n",
      "2022-06-30 00:56:39,135 - NCNet pretrain, Epoch [179 / 300]: loss 0.2841, training auc: 0.8689, val_auc 0.7754\n",
      "2022-06-30 00:56:39,253 - NCNet pretrain, Epoch [180 / 300]: loss 0.2839, training auc: 0.8707, val_auc 0.7749\n",
      "2022-06-30 00:56:39,371 - NCNet pretrain, Epoch [181 / 300]: loss 0.2872, training auc: 0.8650, val_auc 0.7734\n",
      "2022-06-30 00:56:39,488 - NCNet pretrain, Epoch [182 / 300]: loss 0.2937, training auc: 0.8571, val_auc 0.7751\n",
      "2022-06-30 00:56:39,606 - NCNet pretrain, Epoch [183 / 300]: loss 0.2952, training auc: 0.8767, val_auc 0.7695\n",
      "2022-06-30 00:56:39,725 - NCNet pretrain, Epoch [184 / 300]: loss 0.3003, training auc: 0.8586, val_auc 0.7764\n",
      "2022-06-30 00:56:39,844 - NCNet pretrain, Epoch [185 / 300]: loss 0.2897, training auc: 0.8692, val_auc 0.7765\n",
      "2022-06-30 00:56:39,963 - NCNet pretrain, Epoch [186 / 300]: loss 0.2932, training auc: 0.8726, val_auc 0.7685\n",
      "2022-06-30 00:56:40,081 - NCNet pretrain, Epoch [187 / 300]: loss 0.3010, training auc: 0.8603, val_auc 0.7762\n",
      "2022-06-30 00:56:40,199 - NCNet pretrain, Epoch [188 / 300]: loss 0.2937, training auc: 0.8667, val_auc 0.7764\n",
      "2022-06-30 00:56:40,318 - NCNet pretrain, Epoch [189 / 300]: loss 0.2960, training auc: 0.8714, val_auc 0.7692\n",
      "2022-06-30 00:56:40,450 - NCNet pretrain, Epoch [190 / 300]: loss 0.3028, training auc: 0.8604, val_auc 0.7760\n",
      "2022-06-30 00:56:40,590 - NCNet pretrain, Epoch [191 / 300]: loss 0.2888, training auc: 0.8662, val_auc 0.7755\n",
      "2022-06-30 00:56:40,591 - Early stop!\n",
      "2022-06-30 00:56:40,591 - Best Test Results: auc 0.8092, ap 0.4293, f1 0.3774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 336183.83it/s]\n",
      "2022-06-30 00:56:41,552 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:56:41,731 - NCNet pretrain, Epoch [1 / 300]: loss 0.7905, training auc: 0.4781, val_auc 0.4559, test auc 0.4245\n",
      "2022-06-30 00:56:41,861 - NCNet pretrain, Epoch [2 / 300]: loss 0.4985, training auc: 0.4220, val_auc 0.4566, test auc 0.4258\n",
      "2022-06-30 00:56:41,989 - NCNet pretrain, Epoch [3 / 300]: loss 0.4941, training auc: 0.3935, val_auc 0.4568, test auc 0.4262\n",
      "2022-06-30 00:56:42,118 - NCNet pretrain, Epoch [4 / 300]: loss 0.5312, training auc: 0.3918, val_auc 0.4572, test auc 0.4269\n",
      "2022-06-30 00:56:42,266 - NCNet pretrain, Epoch [5 / 300]: loss 0.5118, training auc: 0.4057, val_auc 0.4580, test auc 0.4284\n",
      "2022-06-30 00:56:42,396 - NCNet pretrain, Epoch [6 / 300]: loss 0.4938, training auc: 0.3906, val_auc 0.4593, test auc 0.4314\n",
      "2022-06-30 00:56:42,524 - NCNet pretrain, Epoch [7 / 300]: loss 0.4795, training auc: 0.3916, val_auc 0.4623, test auc 0.4354\n",
      "2022-06-30 00:56:42,674 - NCNet pretrain, Epoch [8 / 300]: loss 0.4737, training auc: 0.4297, val_auc 0.4656, test auc 0.4394\n",
      "2022-06-30 00:56:42,819 - NCNet pretrain, Epoch [9 / 300]: loss 0.4711, training auc: 0.4598, val_auc 0.4677, test auc 0.4418\n",
      "2022-06-30 00:56:42,948 - NCNet pretrain, Epoch [10 / 300]: loss 0.4812, training auc: 0.4045, val_auc 0.4698, test auc 0.4437\n",
      "2022-06-30 00:56:43,078 - NCNet pretrain, Epoch [11 / 300]: loss 0.4737, training auc: 0.4386, val_auc 0.4713, test auc 0.4447\n",
      "2022-06-30 00:56:43,213 - NCNet pretrain, Epoch [12 / 300]: loss 0.4730, training auc: 0.4125, val_auc 0.4718, test auc 0.4448\n",
      "2022-06-30 00:56:43,343 - NCNet pretrain, Epoch [13 / 300]: loss 0.4646, training auc: 0.4416, val_auc 0.4726, test auc 0.4454\n",
      "2022-06-30 00:56:43,471 - NCNet pretrain, Epoch [14 / 300]: loss 0.4636, training auc: 0.4545, val_auc 0.4740, test auc 0.4468\n",
      "2022-06-30 00:56:43,599 - NCNet pretrain, Epoch [15 / 300]: loss 0.4593, training auc: 0.4817, val_auc 0.4754, test auc 0.4485\n",
      "2022-06-30 00:56:43,728 - NCNet pretrain, Epoch [16 / 300]: loss 0.4587, training auc: 0.4626, val_auc 0.4767, test auc 0.4503\n",
      "2022-06-30 00:56:43,858 - NCNet pretrain, Epoch [17 / 300]: loss 0.4676, training auc: 0.4190, val_auc 0.4780, test auc 0.4523\n",
      "2022-06-30 00:56:43,986 - NCNet pretrain, Epoch [18 / 300]: loss 0.4635, training auc: 0.4428, val_auc 0.4799, test auc 0.4549\n",
      "2022-06-30 00:56:44,115 - NCNet pretrain, Epoch [19 / 300]: loss 0.4615, training auc: 0.4560, val_auc 0.4817, test auc 0.4570\n",
      "2022-06-30 00:56:44,243 - NCNet pretrain, Epoch [20 / 300]: loss 0.4603, training auc: 0.4806, val_auc 0.4831, test auc 0.4580\n",
      "2022-06-30 00:56:44,376 - NCNet pretrain, Epoch [21 / 300]: loss 0.4560, training auc: 0.4677, val_auc 0.4839, test auc 0.4585\n",
      "2022-06-30 00:56:44,509 - NCNet pretrain, Epoch [22 / 300]: loss 0.4652, training auc: 0.4129, val_auc 0.4850, test auc 0.4592\n",
      "2022-06-30 00:56:44,637 - NCNet pretrain, Epoch [23 / 300]: loss 0.4561, training auc: 0.4666, val_auc 0.4862, test auc 0.4601\n",
      "2022-06-30 00:56:44,766 - NCNet pretrain, Epoch [24 / 300]: loss 0.4504, training auc: 0.4813, val_auc 0.4872, test auc 0.4610\n",
      "2022-06-30 00:56:44,899 - NCNet pretrain, Epoch [25 / 300]: loss 0.4623, training auc: 0.4243, val_auc 0.4885, test auc 0.4623\n",
      "2022-06-30 00:56:45,048 - NCNet pretrain, Epoch [26 / 300]: loss 0.4493, training auc: 0.4731, val_auc 0.4904, test auc 0.4639\n",
      "2022-06-30 00:56:45,195 - NCNet pretrain, Epoch [27 / 300]: loss 0.4565, training auc: 0.4564, val_auc 0.4927, test auc 0.4664\n",
      "2022-06-30 00:56:45,330 - NCNet pretrain, Epoch [28 / 300]: loss 0.4574, training auc: 0.4528, val_auc 0.4957, test auc 0.4696\n",
      "2022-06-30 00:56:45,463 - NCNet pretrain, Epoch [29 / 300]: loss 0.4538, training auc: 0.4562, val_auc 0.4994, test auc 0.4735\n",
      "2022-06-30 00:56:45,591 - NCNet pretrain, Epoch [30 / 300]: loss 0.4558, training auc: 0.4381, val_auc 0.5037, test auc 0.4783\n",
      "2022-06-30 00:56:45,739 - NCNet pretrain, Epoch [31 / 300]: loss 0.4499, training auc: 0.4613, val_auc 0.5082, test auc 0.4836\n",
      "2022-06-30 00:56:45,888 - NCNet pretrain, Epoch [32 / 300]: loss 0.4502, training auc: 0.4658, val_auc 0.5123, test auc 0.4877\n",
      "2022-06-30 00:56:46,036 - NCNet pretrain, Epoch [33 / 300]: loss 0.4425, training auc: 0.5055, val_auc 0.5154, test auc 0.4905\n",
      "2022-06-30 00:56:46,166 - NCNet pretrain, Epoch [34 / 300]: loss 0.4421, training auc: 0.5305, val_auc 0.5170, test auc 0.4917\n",
      "2022-06-30 00:56:46,313 - NCNet pretrain, Epoch [35 / 300]: loss 0.4455, training auc: 0.4745, val_auc 0.5198, test auc 0.4942\n",
      "2022-06-30 00:56:46,442 - NCNet pretrain, Epoch [36 / 300]: loss 0.4381, training auc: 0.5180, val_auc 0.5230, test auc 0.4974\n",
      "2022-06-30 00:56:46,571 - NCNet pretrain, Epoch [37 / 300]: loss 0.4344, training auc: 0.5265, val_auc 0.5275, test auc 0.5019\n",
      "2022-06-30 00:56:46,700 - NCNet pretrain, Epoch [38 / 300]: loss 0.4335, training auc: 0.5302, val_auc 0.5335, test auc 0.5084\n",
      "2022-06-30 00:56:46,829 - NCNet pretrain, Epoch [39 / 300]: loss 0.4377, training auc: 0.5147, val_auc 0.5421, test auc 0.5175\n",
      "2022-06-30 00:56:46,959 - NCNet pretrain, Epoch [40 / 300]: loss 0.4428, training auc: 0.4818, val_auc 0.5529, test auc 0.5291\n",
      "2022-06-30 00:56:47,088 - NCNet pretrain, Epoch [41 / 300]: loss 0.4440, training auc: 0.4815, val_auc 0.5648, test auc 0.5424\n",
      "2022-06-30 00:56:47,217 - NCNet pretrain, Epoch [42 / 300]: loss 0.4320, training auc: 0.5477, val_auc 0.5743, test auc 0.5524\n",
      "2022-06-30 00:56:47,347 - NCNet pretrain, Epoch [43 / 300]: loss 0.4278, training auc: 0.5696, val_auc 0.5773, test auc 0.5550\n",
      "2022-06-30 00:56:47,473 - NCNet pretrain, Epoch [44 / 300]: loss 0.4236, training auc: 0.5803, val_auc 0.5760\n",
      "2022-06-30 00:56:47,602 - NCNet pretrain, Epoch [45 / 300]: loss 0.4317, training auc: 0.5218, val_auc 0.5815, test auc 0.5575\n",
      "2022-06-30 00:56:47,730 - NCNet pretrain, Epoch [46 / 300]: loss 0.4216, training auc: 0.5687, val_auc 0.5948, test auc 0.5715\n",
      "2022-06-30 00:56:47,868 - NCNet pretrain, Epoch [47 / 300]: loss 0.4218, training auc: 0.5720, val_auc 0.6141, test auc 0.5914\n",
      "2022-06-30 00:56:47,996 - NCNet pretrain, Epoch [48 / 300]: loss 0.4240, training auc: 0.5688, val_auc 0.6363, test auc 0.6138\n",
      "2022-06-30 00:56:48,124 - NCNet pretrain, Epoch [49 / 300]: loss 0.4151, training auc: 0.6280, val_auc 0.6460, test auc 0.6228\n",
      "2022-06-30 00:56:48,264 - NCNet pretrain, Epoch [50 / 300]: loss 0.4120, training auc: 0.6358, val_auc 0.6475, test auc 0.6219\n",
      "2022-06-30 00:56:48,388 - NCNet pretrain, Epoch [51 / 300]: loss 0.4115, training auc: 0.6185, val_auc 0.6492, test auc 0.6221\n",
      "2022-06-30 00:56:48,518 - NCNet pretrain, Epoch [52 / 300]: loss 0.4044, training auc: 0.6466, val_auc 0.6572, test auc 0.6291\n",
      "2022-06-30 00:56:48,648 - NCNet pretrain, Epoch [53 / 300]: loss 0.4032, training auc: 0.6492, val_auc 0.6767, test auc 0.6492\n",
      "2022-06-30 00:56:48,792 - NCNet pretrain, Epoch [54 / 300]: loss 0.4018, training auc: 0.6479, val_auc 0.7028, test auc 0.6819\n",
      "2022-06-30 00:56:48,928 - NCNet pretrain, Epoch [55 / 300]: loss 0.4008, training auc: 0.6790, val_auc 0.7044, test auc 0.6829\n",
      "2022-06-30 00:56:49,047 - NCNet pretrain, Epoch [56 / 300]: loss 0.3922, training auc: 0.7053, val_auc 0.6932\n",
      "2022-06-30 00:56:49,168 - NCNet pretrain, Epoch [57 / 300]: loss 0.3946, training auc: 0.6710, val_auc 0.6932\n",
      "2022-06-30 00:56:49,297 - NCNet pretrain, Epoch [58 / 300]: loss 0.3886, training auc: 0.6775, val_auc 0.7162, test auc 0.6977\n",
      "2022-06-30 00:56:49,427 - NCNet pretrain, Epoch [59 / 300]: loss 0.3849, training auc: 0.7082, val_auc 0.7277, test auc 0.7148\n",
      "2022-06-30 00:56:49,552 - NCNet pretrain, Epoch [60 / 300]: loss 0.3873, training auc: 0.7133, val_auc 0.7227\n",
      "2022-06-30 00:56:49,674 - NCNet pretrain, Epoch [61 / 300]: loss 0.3792, training auc: 0.7184, val_auc 0.7227\n",
      "2022-06-30 00:56:49,792 - NCNet pretrain, Epoch [62 / 300]: loss 0.3809, training auc: 0.7110, val_auc 0.7254\n",
      "2022-06-30 00:56:49,920 - NCNet pretrain, Epoch [63 / 300]: loss 0.3758, training auc: 0.7127, val_auc 0.7325, test auc 0.7209\n",
      "2022-06-30 00:56:50,048 - NCNet pretrain, Epoch [64 / 300]: loss 0.3746, training auc: 0.7295, val_auc 0.7351, test auc 0.7247\n",
      "2022-06-30 00:56:50,166 - NCNet pretrain, Epoch [65 / 300]: loss 0.3740, training auc: 0.7308, val_auc 0.7348\n",
      "2022-06-30 00:56:50,294 - NCNet pretrain, Epoch [66 / 300]: loss 0.3763, training auc: 0.7210, val_auc 0.7384, test auc 0.7291\n",
      "2022-06-30 00:56:50,424 - NCNet pretrain, Epoch [67 / 300]: loss 0.3716, training auc: 0.7287, val_auc 0.7409, test auc 0.7322\n",
      "2022-06-30 00:56:50,547 - NCNet pretrain, Epoch [68 / 300]: loss 0.3656, training auc: 0.7446, val_auc 0.7411, test auc 0.7318\n",
      "2022-06-30 00:56:50,676 - NCNet pretrain, Epoch [69 / 300]: loss 0.3690, training auc: 0.7371, val_auc 0.7494, test auc 0.7431\n",
      "2022-06-30 00:56:50,801 - NCNet pretrain, Epoch [70 / 300]: loss 0.3636, training auc: 0.7613, val_auc 0.7498, test auc 0.7428\n",
      "2022-06-30 00:56:50,920 - NCNet pretrain, Epoch [71 / 300]: loss 0.3564, training auc: 0.7657, val_auc 0.7458\n",
      "2022-06-30 00:56:51,049 - NCNet pretrain, Epoch [72 / 300]: loss 0.3719, training auc: 0.7360, val_auc 0.7554, test auc 0.7505\n",
      "2022-06-30 00:56:51,200 - NCNet pretrain, Epoch [73 / 300]: loss 0.3550, training auc: 0.7739, val_auc 0.7569, test auc 0.7525\n",
      "2022-06-30 00:56:51,321 - NCNet pretrain, Epoch [74 / 300]: loss 0.3611, training auc: 0.7619, val_auc 0.7543\n",
      "2022-06-30 00:56:51,450 - NCNet pretrain, Epoch [75 / 300]: loss 0.3548, training auc: 0.7597, val_auc 0.7601, test auc 0.7563\n",
      "2022-06-30 00:56:51,579 - NCNet pretrain, Epoch [76 / 300]: loss 0.3587, training auc: 0.7588, val_auc 0.7657, test auc 0.7655\n",
      "2022-06-30 00:56:51,724 - NCNet pretrain, Epoch [77 / 300]: loss 0.3477, training auc: 0.7909, val_auc 0.7515\n",
      "2022-06-30 00:56:51,846 - NCNet pretrain, Epoch [78 / 300]: loss 0.3518, training auc: 0.7714, val_auc 0.7621\n",
      "2022-06-30 00:56:51,974 - NCNet pretrain, Epoch [79 / 300]: loss 0.3425, training auc: 0.7827, val_auc 0.7717, test auc 0.7764\n",
      "2022-06-30 00:56:52,092 - NCNet pretrain, Epoch [80 / 300]: loss 0.3524, training auc: 0.7935, val_auc 0.7597\n",
      "2022-06-30 00:56:52,209 - NCNet pretrain, Epoch [81 / 300]: loss 0.3463, training auc: 0.7760, val_auc 0.7551\n",
      "2022-06-30 00:56:52,332 - NCNet pretrain, Epoch [82 / 300]: loss 0.3497, training auc: 0.7685, val_auc 0.7720, test auc 0.7761\n",
      "2022-06-30 00:56:52,450 - NCNet pretrain, Epoch [83 / 300]: loss 0.3493, training auc: 0.7941, val_auc 0.7719\n",
      "2022-06-30 00:56:52,568 - NCNet pretrain, Epoch [84 / 300]: loss 0.3468, training auc: 0.7948, val_auc 0.7566\n",
      "2022-06-30 00:56:52,688 - NCNet pretrain, Epoch [85 / 300]: loss 0.3504, training auc: 0.7745, val_auc 0.7649\n",
      "2022-06-30 00:56:52,817 - NCNet pretrain, Epoch [86 / 300]: loss 0.3484, training auc: 0.7853, val_auc 0.7771, test auc 0.7852\n",
      "2022-06-30 00:56:52,936 - NCNet pretrain, Epoch [87 / 300]: loss 0.3572, training auc: 0.8022, val_auc 0.7710\n",
      "2022-06-30 00:56:53,054 - NCNet pretrain, Epoch [88 / 300]: loss 0.3491, training auc: 0.7832, val_auc 0.7600\n",
      "2022-06-30 00:56:53,172 - NCNet pretrain, Epoch [89 / 300]: loss 0.3374, training auc: 0.7964, val_auc 0.7651\n",
      "2022-06-30 00:56:53,290 - NCNet pretrain, Epoch [90 / 300]: loss 0.3389, training auc: 0.7941, val_auc 0.7764\n",
      "2022-06-30 00:56:53,429 - NCNet pretrain, Epoch [91 / 300]: loss 0.3431, training auc: 0.8088, val_auc 0.7748\n",
      "2022-06-30 00:56:53,550 - NCNet pretrain, Epoch [92 / 300]: loss 0.3357, training auc: 0.8160, val_auc 0.7612\n",
      "2022-06-30 00:56:53,668 - NCNet pretrain, Epoch [93 / 300]: loss 0.3409, training auc: 0.7946, val_auc 0.7647\n",
      "2022-06-30 00:56:53,806 - NCNet pretrain, Epoch [94 / 300]: loss 0.3389, training auc: 0.7936, val_auc 0.7776, test auc 0.7853\n",
      "2022-06-30 00:56:53,924 - NCNet pretrain, Epoch [95 / 300]: loss 0.3404, training auc: 0.8101, val_auc 0.7772\n",
      "2022-06-30 00:56:54,041 - NCNet pretrain, Epoch [96 / 300]: loss 0.3359, training auc: 0.8190, val_auc 0.7653\n",
      "2022-06-30 00:56:54,159 - NCNet pretrain, Epoch [97 / 300]: loss 0.3459, training auc: 0.7916, val_auc 0.7651\n",
      "2022-06-30 00:56:54,276 - NCNet pretrain, Epoch [98 / 300]: loss 0.3304, training auc: 0.8086, val_auc 0.7742\n",
      "2022-06-30 00:56:54,423 - NCNet pretrain, Epoch [99 / 300]: loss 0.3330, training auc: 0.8043, val_auc 0.7795, test auc 0.7893\n",
      "2022-06-30 00:56:54,560 - NCNet pretrain, Epoch [100 / 300]: loss 0.3410, training auc: 0.8118, val_auc 0.7742\n",
      "2022-06-30 00:56:54,691 - NCNet pretrain, Epoch [101 / 300]: loss 0.3340, training auc: 0.8016, val_auc 0.7677\n",
      "2022-06-30 00:56:54,810 - NCNet pretrain, Epoch [102 / 300]: loss 0.3329, training auc: 0.8047, val_auc 0.7735\n",
      "2022-06-30 00:56:54,929 - NCNet pretrain, Epoch [103 / 300]: loss 0.3349, training auc: 0.8009, val_auc 0.7786\n",
      "2022-06-30 00:56:55,048 - NCNet pretrain, Epoch [104 / 300]: loss 0.3208, training auc: 0.8334, val_auc 0.7753\n",
      "2022-06-30 00:56:55,166 - NCNet pretrain, Epoch [105 / 300]: loss 0.3272, training auc: 0.8169, val_auc 0.7669\n",
      "2022-06-30 00:56:55,304 - NCNet pretrain, Epoch [106 / 300]: loss 0.3323, training auc: 0.7978, val_auc 0.7717\n",
      "2022-06-30 00:56:55,423 - NCNet pretrain, Epoch [107 / 300]: loss 0.3325, training auc: 0.8097, val_auc 0.7790\n",
      "2022-06-30 00:56:55,545 - NCNet pretrain, Epoch [108 / 300]: loss 0.3285, training auc: 0.8286, val_auc 0.7774\n",
      "2022-06-30 00:56:55,663 - NCNet pretrain, Epoch [109 / 300]: loss 0.3295, training auc: 0.8138, val_auc 0.7738\n",
      "2022-06-30 00:56:55,782 - NCNet pretrain, Epoch [110 / 300]: loss 0.3168, training auc: 0.8240, val_auc 0.7755\n",
      "2022-06-30 00:56:55,914 - NCNet pretrain, Epoch [111 / 300]: loss 0.3266, training auc: 0.8179, val_auc 0.7767\n",
      "2022-06-30 00:56:56,032 - NCNet pretrain, Epoch [112 / 300]: loss 0.3363, training auc: 0.8030, val_auc 0.7744\n",
      "2022-06-30 00:56:56,150 - NCNet pretrain, Epoch [113 / 300]: loss 0.3311, training auc: 0.8087, val_auc 0.7764\n",
      "2022-06-30 00:56:56,267 - NCNet pretrain, Epoch [114 / 300]: loss 0.3269, training auc: 0.8223, val_auc 0.7775\n",
      "2022-06-30 00:56:56,385 - NCNet pretrain, Epoch [115 / 300]: loss 0.3132, training auc: 0.8368, val_auc 0.7745\n",
      "2022-06-30 00:56:56,503 - NCNet pretrain, Epoch [116 / 300]: loss 0.3321, training auc: 0.8168, val_auc 0.7743\n",
      "2022-06-30 00:56:56,621 - NCNet pretrain, Epoch [117 / 300]: loss 0.3314, training auc: 0.8089, val_auc 0.7772\n",
      "2022-06-30 00:56:56,738 - NCNet pretrain, Epoch [118 / 300]: loss 0.3173, training auc: 0.8284, val_auc 0.7777\n",
      "2022-06-30 00:56:56,875 - NCNet pretrain, Epoch [119 / 300]: loss 0.3221, training auc: 0.8269, val_auc 0.7731\n",
      "2022-06-30 00:56:56,993 - NCNet pretrain, Epoch [120 / 300]: loss 0.3201, training auc: 0.8253, val_auc 0.7761\n",
      "2022-06-30 00:56:57,112 - NCNet pretrain, Epoch [121 / 300]: loss 0.3215, training auc: 0.8191, val_auc 0.7786\n",
      "2022-06-30 00:56:57,230 - NCNet pretrain, Epoch [122 / 300]: loss 0.3235, training auc: 0.8264, val_auc 0.7749\n",
      "2022-06-30 00:56:57,350 - NCNet pretrain, Epoch [123 / 300]: loss 0.3159, training auc: 0.8309, val_auc 0.7761\n",
      "2022-06-30 00:56:57,468 - NCNet pretrain, Epoch [124 / 300]: loss 0.3194, training auc: 0.8293, val_auc 0.7786\n",
      "2022-06-30 00:56:57,591 - NCNet pretrain, Epoch [125 / 300]: loss 0.3194, training auc: 0.8297, val_auc 0.7772\n",
      "2022-06-30 00:56:57,710 - NCNet pretrain, Epoch [126 / 300]: loss 0.3215, training auc: 0.8252, val_auc 0.7767\n",
      "2022-06-30 00:56:57,829 - NCNet pretrain, Epoch [127 / 300]: loss 0.3196, training auc: 0.8302, val_auc 0.7793\n",
      "2022-06-30 00:56:57,953 - NCNet pretrain, Epoch [128 / 300]: loss 0.3163, training auc: 0.8249, val_auc 0.7783\n",
      "2022-06-30 00:56:58,094 - NCNet pretrain, Epoch [129 / 300]: loss 0.3075, training auc: 0.8371, val_auc 0.7708\n",
      "2022-06-30 00:56:58,222 - NCNet pretrain, Epoch [130 / 300]: loss 0.3252, training auc: 0.8209, val_auc 0.7815, test auc 0.7970\n",
      "2022-06-30 00:56:58,339 - NCNet pretrain, Epoch [131 / 300]: loss 0.3178, training auc: 0.8384, val_auc 0.7790\n",
      "2022-06-30 00:56:58,457 - NCNet pretrain, Epoch [132 / 300]: loss 0.3143, training auc: 0.8315, val_auc 0.7667\n",
      "2022-06-30 00:56:58,575 - NCNet pretrain, Epoch [133 / 300]: loss 0.3309, training auc: 0.8155, val_auc 0.7810\n",
      "2022-06-30 00:56:58,711 - NCNet pretrain, Epoch [134 / 300]: loss 0.3253, training auc: 0.8294, val_auc 0.7783\n",
      "2022-06-30 00:56:58,836 - NCNet pretrain, Epoch [135 / 300]: loss 0.3096, training auc: 0.8482, val_auc 0.7692\n",
      "2022-06-30 00:56:58,974 - NCNet pretrain, Epoch [136 / 300]: loss 0.3160, training auc: 0.8321, val_auc 0.7781\n",
      "2022-06-30 00:56:59,093 - NCNet pretrain, Epoch [137 / 300]: loss 0.3132, training auc: 0.8456, val_auc 0.7791\n",
      "2022-06-30 00:56:59,231 - NCNet pretrain, Epoch [138 / 300]: loss 0.3182, training auc: 0.8400, val_auc 0.7729\n",
      "2022-06-30 00:56:59,349 - NCNet pretrain, Epoch [139 / 300]: loss 0.3153, training auc: 0.8335, val_auc 0.7735\n",
      "2022-06-30 00:56:59,467 - NCNet pretrain, Epoch [140 / 300]: loss 0.3102, training auc: 0.8458, val_auc 0.7790\n",
      "2022-06-30 00:56:59,587 - NCNet pretrain, Epoch [141 / 300]: loss 0.3159, training auc: 0.8462, val_auc 0.7748\n",
      "2022-06-30 00:56:59,708 - NCNet pretrain, Epoch [142 / 300]: loss 0.3118, training auc: 0.8308, val_auc 0.7760\n",
      "2022-06-30 00:56:59,827 - NCNet pretrain, Epoch [143 / 300]: loss 0.3064, training auc: 0.8370, val_auc 0.7801\n",
      "2022-06-30 00:56:59,947 - NCNet pretrain, Epoch [144 / 300]: loss 0.3193, training auc: 0.8344, val_auc 0.7730\n",
      "2022-06-30 00:57:00,086 - NCNet pretrain, Epoch [145 / 300]: loss 0.3122, training auc: 0.8345, val_auc 0.7745\n",
      "2022-06-30 00:57:00,207 - NCNet pretrain, Epoch [146 / 300]: loss 0.3137, training auc: 0.8300, val_auc 0.7799\n",
      "2022-06-30 00:57:00,325 - NCNet pretrain, Epoch [147 / 300]: loss 0.3125, training auc: 0.8487, val_auc 0.7763\n",
      "2022-06-30 00:57:00,443 - NCNet pretrain, Epoch [148 / 300]: loss 0.3060, training auc: 0.8436, val_auc 0.7712\n",
      "2022-06-30 00:57:00,565 - NCNet pretrain, Epoch [149 / 300]: loss 0.3117, training auc: 0.8373, val_auc 0.7801\n",
      "2022-06-30 00:57:00,683 - NCNet pretrain, Epoch [150 / 300]: loss 0.3102, training auc: 0.8458, val_auc 0.7768\n",
      "2022-06-30 00:57:00,802 - NCNet pretrain, Epoch [151 / 300]: loss 0.3160, training auc: 0.8338, val_auc 0.7721\n",
      "2022-06-30 00:57:00,922 - NCNet pretrain, Epoch [152 / 300]: loss 0.3109, training auc: 0.8394, val_auc 0.7802\n",
      "2022-06-30 00:57:01,045 - NCNet pretrain, Epoch [153 / 300]: loss 0.3194, training auc: 0.8395, val_auc 0.7754\n",
      "2022-06-30 00:57:01,165 - NCNet pretrain, Epoch [154 / 300]: loss 0.3094, training auc: 0.8421, val_auc 0.7709\n",
      "2022-06-30 00:57:01,284 - NCNet pretrain, Epoch [155 / 300]: loss 0.3118, training auc: 0.8407, val_auc 0.7798\n",
      "2022-06-30 00:57:01,407 - NCNet pretrain, Epoch [156 / 300]: loss 0.3136, training auc: 0.8477, val_auc 0.7756\n",
      "2022-06-30 00:57:01,547 - NCNet pretrain, Epoch [157 / 300]: loss 0.3016, training auc: 0.8472, val_auc 0.7717\n",
      "2022-06-30 00:57:01,688 - NCNet pretrain, Epoch [158 / 300]: loss 0.3133, training auc: 0.8405, val_auc 0.7787\n",
      "2022-06-30 00:57:01,810 - NCNet pretrain, Epoch [159 / 300]: loss 0.3013, training auc: 0.8551, val_auc 0.7784\n",
      "2022-06-30 00:57:01,932 - NCNet pretrain, Epoch [160 / 300]: loss 0.3042, training auc: 0.8494, val_auc 0.7659\n",
      "2022-06-30 00:57:02,054 - NCNet pretrain, Epoch [161 / 300]: loss 0.3079, training auc: 0.8461, val_auc 0.7790\n",
      "2022-06-30 00:57:02,195 - NCNet pretrain, Epoch [162 / 300]: loss 0.2953, training auc: 0.8674, val_auc 0.7773\n",
      "2022-06-30 00:57:02,339 - NCNet pretrain, Epoch [163 / 300]: loss 0.2999, training auc: 0.8529, val_auc 0.7705\n",
      "2022-06-30 00:57:02,458 - NCNet pretrain, Epoch [164 / 300]: loss 0.3088, training auc: 0.8424, val_auc 0.7778\n",
      "2022-06-30 00:57:02,580 - NCNet pretrain, Epoch [165 / 300]: loss 0.3004, training auc: 0.8665, val_auc 0.7729\n",
      "2022-06-30 00:57:02,719 - NCNet pretrain, Epoch [166 / 300]: loss 0.3056, training auc: 0.8410, val_auc 0.7741\n",
      "2022-06-30 00:57:02,839 - NCNet pretrain, Epoch [167 / 300]: loss 0.3017, training auc: 0.8538, val_auc 0.7760\n",
      "2022-06-30 00:57:02,959 - NCNet pretrain, Epoch [168 / 300]: loss 0.3004, training auc: 0.8600, val_auc 0.7722\n",
      "2022-06-30 00:57:03,078 - NCNet pretrain, Epoch [169 / 300]: loss 0.2965, training auc: 0.8556, val_auc 0.7734\n",
      "2022-06-30 00:57:03,197 - NCNet pretrain, Epoch [170 / 300]: loss 0.2904, training auc: 0.8597, val_auc 0.7732\n",
      "2022-06-30 00:57:03,316 - NCNet pretrain, Epoch [171 / 300]: loss 0.2922, training auc: 0.8641, val_auc 0.7755\n",
      "2022-06-30 00:57:03,435 - NCNet pretrain, Epoch [172 / 300]: loss 0.2968, training auc: 0.8676, val_auc 0.7694\n",
      "2022-06-30 00:57:03,554 - NCNet pretrain, Epoch [173 / 300]: loss 0.2990, training auc: 0.8544, val_auc 0.7734\n",
      "2022-06-30 00:57:03,673 - NCNet pretrain, Epoch [174 / 300]: loss 0.2965, training auc: 0.8588, val_auc 0.7741\n",
      "2022-06-30 00:57:03,792 - NCNet pretrain, Epoch [175 / 300]: loss 0.2981, training auc: 0.8598, val_auc 0.7647\n",
      "2022-06-30 00:57:03,921 - NCNet pretrain, Epoch [176 / 300]: loss 0.2957, training auc: 0.8584, val_auc 0.7746\n",
      "2022-06-30 00:57:04,040 - NCNet pretrain, Epoch [177 / 300]: loss 0.3035, training auc: 0.8595, val_auc 0.7721\n",
      "2022-06-30 00:57:04,159 - NCNet pretrain, Epoch [178 / 300]: loss 0.2857, training auc: 0.8739, val_auc 0.7687\n",
      "2022-06-30 00:57:04,276 - NCNet pretrain, Epoch [179 / 300]: loss 0.3021, training auc: 0.8485, val_auc 0.7744\n",
      "2022-06-30 00:57:04,398 - NCNet pretrain, Epoch [180 / 300]: loss 0.3027, training auc: 0.8733, val_auc 0.7647\n",
      "2022-06-30 00:57:04,399 - Early stop!\n",
      "2022-06-30 00:57:04,400 - Best Test Results: auc 0.7970, ap 0.4099, f1 0.2793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 246345.69it/s]\n",
      "2022-06-30 00:57:05,554 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:57:05,736 - NCNet pretrain, Epoch [1 / 300]: loss 0.5609, training auc: 0.4623, val_auc 0.4630, test auc 0.4332\n",
      "2022-06-30 00:57:05,855 - NCNet pretrain, Epoch [2 / 300]: loss 0.4440, training auc: 0.4526, val_auc 0.4624\n",
      "2022-06-30 00:57:05,974 - NCNet pretrain, Epoch [3 / 300]: loss 0.4883, training auc: 0.3998, val_auc 0.4614\n",
      "2022-06-30 00:57:06,103 - NCNet pretrain, Epoch [4 / 300]: loss 0.4630, training auc: 0.4380, val_auc 0.4643, test auc 0.4381\n",
      "2022-06-30 00:57:06,250 - NCNet pretrain, Epoch [5 / 300]: loss 0.4442, training auc: 0.4666, val_auc 0.4676, test auc 0.4430\n",
      "2022-06-30 00:57:06,379 - NCNet pretrain, Epoch [6 / 300]: loss 0.4498, training auc: 0.4236, val_auc 0.4721, test auc 0.4491\n",
      "2022-06-30 00:57:06,508 - NCNet pretrain, Epoch [7 / 300]: loss 0.4417, training auc: 0.4837, val_auc 0.4754, test auc 0.4542\n",
      "2022-06-30 00:57:06,647 - NCNet pretrain, Epoch [8 / 300]: loss 0.4442, training auc: 0.4971, val_auc 0.4769, test auc 0.4551\n",
      "2022-06-30 00:57:06,765 - NCNet pretrain, Epoch [9 / 300]: loss 0.4454, training auc: 0.4653, val_auc 0.4766\n",
      "2022-06-30 00:57:06,883 - NCNet pretrain, Epoch [10 / 300]: loss 0.4400, training auc: 0.4846, val_auc 0.4766\n",
      "2022-06-30 00:57:07,019 - NCNet pretrain, Epoch [11 / 300]: loss 0.4384, training auc: 0.4911, val_auc 0.4761\n",
      "2022-06-30 00:57:07,155 - NCNet pretrain, Epoch [12 / 300]: loss 0.4473, training auc: 0.4242, val_auc 0.4767\n",
      "2022-06-30 00:57:07,277 - NCNet pretrain, Epoch [13 / 300]: loss 0.4320, training auc: 0.5083, val_auc 0.4775, test auc 0.4522\n",
      "2022-06-30 00:57:07,399 - NCNet pretrain, Epoch [14 / 300]: loss 0.4416, training auc: 0.4701, val_auc 0.4782, test auc 0.4530\n",
      "2022-06-30 00:57:07,522 - NCNet pretrain, Epoch [15 / 300]: loss 0.4476, training auc: 0.4188, val_auc 0.4801, test auc 0.4551\n",
      "2022-06-30 00:57:07,669 - NCNet pretrain, Epoch [16 / 300]: loss 0.4417, training auc: 0.4362, val_auc 0.4820, test auc 0.4569\n",
      "2022-06-30 00:57:07,806 - NCNet pretrain, Epoch [17 / 300]: loss 0.4402, training auc: 0.4454, val_auc 0.4840, test auc 0.4588\n",
      "2022-06-30 00:57:07,934 - NCNet pretrain, Epoch [18 / 300]: loss 0.4381, training auc: 0.4646, val_auc 0.4863, test auc 0.4609\n",
      "2022-06-30 00:57:08,063 - NCNet pretrain, Epoch [19 / 300]: loss 0.4307, training auc: 0.5140, val_auc 0.4885, test auc 0.4627\n",
      "2022-06-30 00:57:08,193 - NCNet pretrain, Epoch [20 / 300]: loss 0.4345, training auc: 0.4746, val_auc 0.4907, test auc 0.4645\n",
      "2022-06-30 00:57:08,323 - NCNet pretrain, Epoch [21 / 300]: loss 0.4384, training auc: 0.4472, val_auc 0.4927, test auc 0.4664\n",
      "2022-06-30 00:57:08,453 - NCNet pretrain, Epoch [22 / 300]: loss 0.4379, training auc: 0.4309, val_auc 0.4957, test auc 0.4689\n",
      "2022-06-30 00:57:08,601 - NCNet pretrain, Epoch [23 / 300]: loss 0.4314, training auc: 0.4795, val_auc 0.4991, test auc 0.4723\n",
      "2022-06-30 00:57:08,753 - NCNet pretrain, Epoch [24 / 300]: loss 0.4301, training auc: 0.4827, val_auc 0.5026, test auc 0.4755\n",
      "2022-06-30 00:57:08,881 - NCNet pretrain, Epoch [25 / 300]: loss 0.4220, training auc: 0.5379, val_auc 0.5075, test auc 0.4801\n",
      "2022-06-30 00:57:09,010 - NCNet pretrain, Epoch [26 / 300]: loss 0.4245, training auc: 0.5249, val_auc 0.5140, test auc 0.4865\n",
      "2022-06-30 00:57:09,138 - NCNet pretrain, Epoch [27 / 300]: loss 0.4319, training auc: 0.4563, val_auc 0.5256, test auc 0.4979\n",
      "2022-06-30 00:57:09,267 - NCNet pretrain, Epoch [28 / 300]: loss 0.4284, training auc: 0.4940, val_auc 0.5410, test auc 0.5143\n",
      "2022-06-30 00:57:09,396 - NCNet pretrain, Epoch [29 / 300]: loss 0.4217, training auc: 0.5602, val_auc 0.5485, test auc 0.5208\n",
      "2022-06-30 00:57:09,525 - NCNet pretrain, Epoch [30 / 300]: loss 0.4228, training auc: 0.5427, val_auc 0.5528, test auc 0.5246\n",
      "2022-06-30 00:57:09,654 - NCNet pretrain, Epoch [31 / 300]: loss 0.4209, training auc: 0.5348, val_auc 0.5604, test auc 0.5319\n",
      "2022-06-30 00:57:09,803 - NCNet pretrain, Epoch [32 / 300]: loss 0.4204, training auc: 0.5197, val_auc 0.5763, test auc 0.5472\n",
      "2022-06-30 00:57:09,952 - NCNet pretrain, Epoch [33 / 300]: loss 0.4200, training auc: 0.5275, val_auc 0.6042, test auc 0.5738\n",
      "2022-06-30 00:57:10,082 - NCNet pretrain, Epoch [34 / 300]: loss 0.4079, training auc: 0.6274, val_auc 0.6258, test auc 0.5938\n",
      "2022-06-30 00:57:10,212 - NCNet pretrain, Epoch [35 / 300]: loss 0.4123, training auc: 0.5959, val_auc 0.6421, test auc 0.6089\n",
      "2022-06-30 00:57:10,342 - NCNet pretrain, Epoch [36 / 300]: loss 0.4157, training auc: 0.5508, val_auc 0.6606, test auc 0.6266\n",
      "2022-06-30 00:57:10,473 - NCNet pretrain, Epoch [37 / 300]: loss 0.4046, training auc: 0.6260, val_auc 0.6791, test auc 0.6469\n",
      "2022-06-30 00:57:10,603 - NCNet pretrain, Epoch [38 / 300]: loss 0.4088, training auc: 0.5966, val_auc 0.6985, test auc 0.6693\n",
      "2022-06-30 00:57:10,734 - NCNet pretrain, Epoch [39 / 300]: loss 0.4044, training auc: 0.6020, val_auc 0.7074, test auc 0.6801\n",
      "2022-06-30 00:57:10,855 - NCNet pretrain, Epoch [40 / 300]: loss 0.3965, training auc: 0.6758, val_auc 0.7017\n",
      "2022-06-30 00:57:11,008 - NCNet pretrain, Epoch [41 / 300]: loss 0.3923, training auc: 0.6578, val_auc 0.7225, test auc 0.7043\n",
      "2022-06-30 00:57:11,136 - NCNet pretrain, Epoch [42 / 300]: loss 0.3893, training auc: 0.6928, val_auc 0.7282, test auc 0.7127\n",
      "2022-06-30 00:57:11,254 - NCNet pretrain, Epoch [43 / 300]: loss 0.3910, training auc: 0.6947, val_auc 0.7251\n",
      "2022-06-30 00:57:11,382 - NCNet pretrain, Epoch [44 / 300]: loss 0.3793, training auc: 0.7201, val_auc 0.7328, test auc 0.7203\n",
      "2022-06-30 00:57:11,506 - NCNet pretrain, Epoch [45 / 300]: loss 0.3757, training auc: 0.7307, val_auc 0.7331, test auc 0.7200\n",
      "2022-06-30 00:57:11,655 - NCNet pretrain, Epoch [46 / 300]: loss 0.3747, training auc: 0.7232, val_auc 0.7424, test auc 0.7353\n",
      "2022-06-30 00:57:11,773 - NCNet pretrain, Epoch [47 / 300]: loss 0.3689, training auc: 0.7482, val_auc 0.7377\n",
      "2022-06-30 00:57:11,901 - NCNet pretrain, Epoch [48 / 300]: loss 0.3681, training auc: 0.7283, val_auc 0.7514, test auc 0.7472\n",
      "2022-06-30 00:57:12,019 - NCNet pretrain, Epoch [49 / 300]: loss 0.3613, training auc: 0.7686, val_auc 0.7379\n",
      "2022-06-30 00:57:12,148 - NCNet pretrain, Epoch [50 / 300]: loss 0.3662, training auc: 0.7401, val_auc 0.7546, test auc 0.7495\n",
      "2022-06-30 00:57:12,277 - NCNet pretrain, Epoch [51 / 300]: loss 0.3650, training auc: 0.7506, val_auc 0.7658, test auc 0.7663\n",
      "2022-06-30 00:57:12,395 - NCNet pretrain, Epoch [52 / 300]: loss 0.3679, training auc: 0.7686, val_auc 0.7471\n",
      "2022-06-30 00:57:12,514 - NCNet pretrain, Epoch [53 / 300]: loss 0.3654, training auc: 0.7464, val_auc 0.7577\n",
      "2022-06-30 00:57:12,668 - NCNet pretrain, Epoch [54 / 300]: loss 0.3597, training auc: 0.7515, val_auc 0.7725, test auc 0.7782\n",
      "2022-06-30 00:57:12,787 - NCNet pretrain, Epoch [55 / 300]: loss 0.3698, training auc: 0.7800, val_auc 0.7568\n",
      "2022-06-30 00:57:12,905 - NCNet pretrain, Epoch [56 / 300]: loss 0.3650, training auc: 0.7458, val_auc 0.7518\n",
      "2022-06-30 00:57:13,031 - NCNet pretrain, Epoch [57 / 300]: loss 0.3630, training auc: 0.7503, val_auc 0.7712\n",
      "2022-06-30 00:57:13,160 - NCNet pretrain, Epoch [58 / 300]: loss 0.3567, training auc: 0.7775, val_auc 0.7753, test auc 0.7817\n",
      "2022-06-30 00:57:13,278 - NCNet pretrain, Epoch [59 / 300]: loss 0.3576, training auc: 0.7856, val_auc 0.7696\n",
      "2022-06-30 00:57:13,395 - NCNet pretrain, Epoch [60 / 300]: loss 0.3389, training auc: 0.7937, val_auc 0.7620\n",
      "2022-06-30 00:57:13,530 - NCNet pretrain, Epoch [61 / 300]: loss 0.3511, training auc: 0.7679, val_auc 0.7692\n",
      "2022-06-30 00:57:13,658 - NCNet pretrain, Epoch [62 / 300]: loss 0.3411, training auc: 0.7863, val_auc 0.7770, test auc 0.7828\n",
      "2022-06-30 00:57:13,795 - NCNet pretrain, Epoch [63 / 300]: loss 0.3542, training auc: 0.7888, val_auc 0.7751\n",
      "2022-06-30 00:57:13,912 - NCNet pretrain, Epoch [64 / 300]: loss 0.3413, training auc: 0.7924, val_auc 0.7708\n",
      "2022-06-30 00:57:14,031 - NCNet pretrain, Epoch [65 / 300]: loss 0.3447, training auc: 0.7809, val_auc 0.7736\n",
      "2022-06-30 00:57:14,174 - NCNet pretrain, Epoch [66 / 300]: loss 0.3400, training auc: 0.7909, val_auc 0.7778, test auc 0.7806\n",
      "2022-06-30 00:57:14,294 - NCNet pretrain, Epoch [67 / 300]: loss 0.3383, training auc: 0.8004, val_auc 0.7776\n",
      "2022-06-30 00:57:14,418 - NCNet pretrain, Epoch [68 / 300]: loss 0.3453, training auc: 0.7851, val_auc 0.7793, test auc 0.7818\n",
      "2022-06-30 00:57:14,535 - NCNet pretrain, Epoch [69 / 300]: loss 0.3327, training auc: 0.8073, val_auc 0.7767\n",
      "2022-06-30 00:57:14,682 - NCNet pretrain, Epoch [70 / 300]: loss 0.3388, training auc: 0.7943, val_auc 0.7798, test auc 0.7823\n",
      "2022-06-30 00:57:14,832 - NCNet pretrain, Epoch [71 / 300]: loss 0.3427, training auc: 0.7935, val_auc 0.7822, test auc 0.7866\n",
      "2022-06-30 00:57:14,954 - NCNet pretrain, Epoch [72 / 300]: loss 0.3295, training auc: 0.8123, val_auc 0.7786\n",
      "2022-06-30 00:57:15,073 - NCNet pretrain, Epoch [73 / 300]: loss 0.3416, training auc: 0.7941, val_auc 0.7773\n",
      "2022-06-30 00:57:15,209 - NCNet pretrain, Epoch [74 / 300]: loss 0.3381, training auc: 0.7950, val_auc 0.7850, test auc 0.7911\n",
      "2022-06-30 00:57:15,326 - NCNet pretrain, Epoch [75 / 300]: loss 0.3367, training auc: 0.8091, val_auc 0.7831\n",
      "2022-06-30 00:57:15,463 - NCNet pretrain, Epoch [76 / 300]: loss 0.3287, training auc: 0.8126, val_auc 0.7796\n",
      "2022-06-30 00:57:15,598 - NCNet pretrain, Epoch [77 / 300]: loss 0.3290, training auc: 0.8032, val_auc 0.7834\n",
      "2022-06-30 00:57:15,716 - NCNet pretrain, Epoch [78 / 300]: loss 0.3315, training auc: 0.8128, val_auc 0.7821\n",
      "2022-06-30 00:57:15,833 - NCNet pretrain, Epoch [79 / 300]: loss 0.3260, training auc: 0.8166, val_auc 0.7823\n",
      "2022-06-30 00:57:15,958 - NCNet pretrain, Epoch [80 / 300]: loss 0.3312, training auc: 0.8184, val_auc 0.7820\n",
      "2022-06-30 00:57:16,076 - NCNet pretrain, Epoch [81 / 300]: loss 0.3210, training auc: 0.8227, val_auc 0.7828\n",
      "2022-06-30 00:57:16,194 - NCNet pretrain, Epoch [82 / 300]: loss 0.3240, training auc: 0.8180, val_auc 0.7794\n",
      "2022-06-30 00:57:16,323 - NCNet pretrain, Epoch [83 / 300]: loss 0.3301, training auc: 0.8020, val_auc 0.7855, test auc 0.7960\n",
      "2022-06-30 00:57:16,452 - NCNet pretrain, Epoch [84 / 300]: loss 0.3234, training auc: 0.8236, val_auc 0.7860, test auc 0.7966\n",
      "2022-06-30 00:57:16,570 - NCNet pretrain, Epoch [85 / 300]: loss 0.3213, training auc: 0.8309, val_auc 0.7780\n",
      "2022-06-30 00:57:16,687 - NCNet pretrain, Epoch [86 / 300]: loss 0.3244, training auc: 0.8143, val_auc 0.7856\n",
      "2022-06-30 00:57:16,819 - NCNet pretrain, Epoch [87 / 300]: loss 0.3196, training auc: 0.8280, val_auc 0.7869, test auc 0.7991\n",
      "2022-06-30 00:57:16,940 - NCNet pretrain, Epoch [88 / 300]: loss 0.3168, training auc: 0.8283, val_auc 0.7823\n",
      "2022-06-30 00:57:17,071 - NCNet pretrain, Epoch [89 / 300]: loss 0.3201, training auc: 0.8245, val_auc 0.7882, test auc 0.8013\n",
      "2022-06-30 00:57:17,189 - NCNet pretrain, Epoch [90 / 300]: loss 0.3196, training auc: 0.8370, val_auc 0.7881\n",
      "2022-06-30 00:57:17,311 - NCNet pretrain, Epoch [91 / 300]: loss 0.3208, training auc: 0.8348, val_auc 0.7769\n",
      "2022-06-30 00:57:17,466 - NCNet pretrain, Epoch [92 / 300]: loss 0.3272, training auc: 0.8176, val_auc 0.7906, test auc 0.8050\n",
      "2022-06-30 00:57:17,583 - NCNet pretrain, Epoch [93 / 300]: loss 0.3178, training auc: 0.8405, val_auc 0.7896\n",
      "2022-06-30 00:57:17,700 - NCNet pretrain, Epoch [94 / 300]: loss 0.3171, training auc: 0.8392, val_auc 0.7776\n",
      "2022-06-30 00:57:17,822 - NCNet pretrain, Epoch [95 / 300]: loss 0.3315, training auc: 0.8185, val_auc 0.7898\n",
      "2022-06-30 00:57:17,939 - NCNet pretrain, Epoch [96 / 300]: loss 0.3202, training auc: 0.8336, val_auc 0.7897\n",
      "2022-06-30 00:57:18,057 - NCNet pretrain, Epoch [97 / 300]: loss 0.3238, training auc: 0.8358, val_auc 0.7812\n",
      "2022-06-30 00:57:18,175 - NCNet pretrain, Epoch [98 / 300]: loss 0.3191, training auc: 0.8246, val_auc 0.7855\n",
      "2022-06-30 00:57:18,292 - NCNet pretrain, Epoch [99 / 300]: loss 0.3152, training auc: 0.8310, val_auc 0.7900\n",
      "2022-06-30 00:57:18,409 - NCNet pretrain, Epoch [100 / 300]: loss 0.3170, training auc: 0.8475, val_auc 0.7816\n",
      "2022-06-30 00:57:18,527 - NCNet pretrain, Epoch [101 / 300]: loss 0.3115, training auc: 0.8355, val_auc 0.7840\n",
      "2022-06-30 00:57:18,656 - NCNet pretrain, Epoch [102 / 300]: loss 0.3150, training auc: 0.8298, val_auc 0.7910, test auc 0.8087\n",
      "2022-06-30 00:57:18,793 - NCNet pretrain, Epoch [103 / 300]: loss 0.3164, training auc: 0.8462, val_auc 0.7883\n",
      "2022-06-30 00:57:18,911 - NCNet pretrain, Epoch [104 / 300]: loss 0.3097, training auc: 0.8413, val_auc 0.7820\n",
      "2022-06-30 00:57:19,039 - NCNet pretrain, Epoch [105 / 300]: loss 0.3116, training auc: 0.8365, val_auc 0.7891\n",
      "2022-06-30 00:57:19,159 - NCNet pretrain, Epoch [106 / 300]: loss 0.3102, training auc: 0.8399, val_auc 0.7901\n",
      "2022-06-30 00:57:19,277 - NCNet pretrain, Epoch [107 / 300]: loss 0.3119, training auc: 0.8421, val_auc 0.7816\n",
      "2022-06-30 00:57:19,396 - NCNet pretrain, Epoch [108 / 300]: loss 0.3142, training auc: 0.8323, val_auc 0.7884\n",
      "2022-06-30 00:57:19,529 - NCNet pretrain, Epoch [109 / 300]: loss 0.3021, training auc: 0.8503, val_auc 0.7915, test auc 0.8122\n",
      "2022-06-30 00:57:19,668 - NCNet pretrain, Epoch [110 / 300]: loss 0.3179, training auc: 0.8532, val_auc 0.7823\n",
      "2022-06-30 00:57:19,793 - NCNet pretrain, Epoch [111 / 300]: loss 0.3105, training auc: 0.8394, val_auc 0.7851\n",
      "2022-06-30 00:57:19,929 - NCNet pretrain, Epoch [112 / 300]: loss 0.3052, training auc: 0.8418, val_auc 0.7907\n",
      "2022-06-30 00:57:20,067 - NCNet pretrain, Epoch [113 / 300]: loss 0.3124, training auc: 0.8534, val_auc 0.7880\n",
      "2022-06-30 00:57:20,198 - NCNet pretrain, Epoch [114 / 300]: loss 0.3066, training auc: 0.8408, val_auc 0.7854\n",
      "2022-06-30 00:57:20,315 - NCNet pretrain, Epoch [115 / 300]: loss 0.3050, training auc: 0.8441, val_auc 0.7898\n",
      "2022-06-30 00:57:20,432 - NCNet pretrain, Epoch [116 / 300]: loss 0.3065, training auc: 0.8488, val_auc 0.7905\n",
      "2022-06-30 00:57:20,570 - NCNet pretrain, Epoch [117 / 300]: loss 0.3043, training auc: 0.8521, val_auc 0.7857\n",
      "2022-06-30 00:57:20,687 - NCNet pretrain, Epoch [118 / 300]: loss 0.3014, training auc: 0.8452, val_auc 0.7880\n",
      "2022-06-30 00:57:20,814 - NCNet pretrain, Epoch [119 / 300]: loss 0.3014, training auc: 0.8478, val_auc 0.7916, test auc 0.8102\n",
      "2022-06-30 00:57:20,939 - NCNet pretrain, Epoch [120 / 300]: loss 0.3108, training auc: 0.8455, val_auc 0.7884\n",
      "2022-06-30 00:57:21,056 - NCNet pretrain, Epoch [121 / 300]: loss 0.3013, training auc: 0.8446, val_auc 0.7870\n",
      "2022-06-30 00:57:21,180 - NCNet pretrain, Epoch [122 / 300]: loss 0.3054, training auc: 0.8380, val_auc 0.7920, test auc 0.8107\n",
      "2022-06-30 00:57:21,315 - NCNet pretrain, Epoch [123 / 300]: loss 0.2966, training auc: 0.8615, val_auc 0.7885\n",
      "2022-06-30 00:57:21,434 - NCNet pretrain, Epoch [124 / 300]: loss 0.2975, training auc: 0.8525, val_auc 0.7852\n",
      "2022-06-30 00:57:21,553 - NCNet pretrain, Epoch [125 / 300]: loss 0.3065, training auc: 0.8384, val_auc 0.7906\n",
      "2022-06-30 00:57:21,675 - NCNet pretrain, Epoch [126 / 300]: loss 0.2988, training auc: 0.8522, val_auc 0.7904\n",
      "2022-06-30 00:57:21,800 - NCNet pretrain, Epoch [127 / 300]: loss 0.2947, training auc: 0.8555, val_auc 0.7881\n",
      "2022-06-30 00:57:21,917 - NCNet pretrain, Epoch [128 / 300]: loss 0.2942, training auc: 0.8570, val_auc 0.7885\n",
      "2022-06-30 00:57:22,036 - NCNet pretrain, Epoch [129 / 300]: loss 0.2911, training auc: 0.8597, val_auc 0.7908\n",
      "2022-06-30 00:57:22,160 - NCNet pretrain, Epoch [130 / 300]: loss 0.3050, training auc: 0.8514, val_auc 0.7864\n",
      "2022-06-30 00:57:22,276 - NCNet pretrain, Epoch [131 / 300]: loss 0.2918, training auc: 0.8589, val_auc 0.7881\n",
      "2022-06-30 00:57:22,393 - NCNet pretrain, Epoch [132 / 300]: loss 0.2887, training auc: 0.8603, val_auc 0.7897\n",
      "2022-06-30 00:57:22,511 - NCNet pretrain, Epoch [133 / 300]: loss 0.2967, training auc: 0.8604, val_auc 0.7869\n",
      "2022-06-30 00:57:22,628 - NCNet pretrain, Epoch [134 / 300]: loss 0.2869, training auc: 0.8658, val_auc 0.7884\n",
      "2022-06-30 00:57:22,745 - NCNet pretrain, Epoch [135 / 300]: loss 0.2882, training auc: 0.8610, val_auc 0.7882\n",
      "2022-06-30 00:57:22,864 - NCNet pretrain, Epoch [136 / 300]: loss 0.2956, training auc: 0.8641, val_auc 0.7852\n",
      "2022-06-30 00:57:22,983 - NCNet pretrain, Epoch [137 / 300]: loss 0.2960, training auc: 0.8465, val_auc 0.7871\n",
      "2022-06-30 00:57:23,102 - NCNet pretrain, Epoch [138 / 300]: loss 0.2902, training auc: 0.8593, val_auc 0.7888\n",
      "2022-06-30 00:57:23,221 - NCNet pretrain, Epoch [139 / 300]: loss 0.2935, training auc: 0.8631, val_auc 0.7839\n",
      "2022-06-30 00:57:23,359 - NCNet pretrain, Epoch [140 / 300]: loss 0.2893, training auc: 0.8584, val_auc 0.7855\n",
      "2022-06-30 00:57:23,488 - NCNet pretrain, Epoch [141 / 300]: loss 0.2967, training auc: 0.8567, val_auc 0.7881\n",
      "2022-06-30 00:57:23,606 - NCNet pretrain, Epoch [142 / 300]: loss 0.2975, training auc: 0.8627, val_auc 0.7824\n",
      "2022-06-30 00:57:23,725 - NCNet pretrain, Epoch [143 / 300]: loss 0.2846, training auc: 0.8625, val_auc 0.7847\n",
      "2022-06-30 00:57:23,870 - NCNet pretrain, Epoch [144 / 300]: loss 0.2891, training auc: 0.8600, val_auc 0.7874\n",
      "2022-06-30 00:57:24,003 - NCNet pretrain, Epoch [145 / 300]: loss 0.2947, training auc: 0.8739, val_auc 0.7729\n",
      "2022-06-30 00:57:24,120 - NCNet pretrain, Epoch [146 / 300]: loss 0.2974, training auc: 0.8605, val_auc 0.7867\n",
      "2022-06-30 00:57:24,238 - NCNet pretrain, Epoch [147 / 300]: loss 0.2921, training auc: 0.8675, val_auc 0.7864\n",
      "2022-06-30 00:57:24,375 - NCNet pretrain, Epoch [148 / 300]: loss 0.2895, training auc: 0.8738, val_auc 0.7690\n",
      "2022-06-30 00:57:24,492 - NCNet pretrain, Epoch [149 / 300]: loss 0.3159, training auc: 0.8498, val_auc 0.7857\n",
      "2022-06-30 00:57:24,610 - NCNet pretrain, Epoch [150 / 300]: loss 0.2907, training auc: 0.8694, val_auc 0.7851\n",
      "2022-06-30 00:57:24,747 - NCNet pretrain, Epoch [151 / 300]: loss 0.2879, training auc: 0.8759, val_auc 0.7778\n",
      "2022-06-30 00:57:24,875 - NCNet pretrain, Epoch [152 / 300]: loss 0.2898, training auc: 0.8580, val_auc 0.7815\n",
      "2022-06-30 00:57:24,993 - NCNet pretrain, Epoch [153 / 300]: loss 0.2876, training auc: 0.8607, val_auc 0.7853\n",
      "2022-06-30 00:57:25,112 - NCNet pretrain, Epoch [154 / 300]: loss 0.2895, training auc: 0.8776, val_auc 0.7811\n",
      "2022-06-30 00:57:25,237 - NCNet pretrain, Epoch [155 / 300]: loss 0.2923, training auc: 0.8615, val_auc 0.7838\n",
      "2022-06-30 00:57:25,355 - NCNet pretrain, Epoch [156 / 300]: loss 0.2843, training auc: 0.8684, val_auc 0.7856\n",
      "2022-06-30 00:57:25,474 - NCNet pretrain, Epoch [157 / 300]: loss 0.2842, training auc: 0.8760, val_auc 0.7825\n",
      "2022-06-30 00:57:25,593 - NCNet pretrain, Epoch [158 / 300]: loss 0.2838, training auc: 0.8680, val_auc 0.7847\n",
      "2022-06-30 00:57:25,711 - NCNet pretrain, Epoch [159 / 300]: loss 0.2842, training auc: 0.8699, val_auc 0.7853\n",
      "2022-06-30 00:57:25,832 - NCNet pretrain, Epoch [160 / 300]: loss 0.2815, training auc: 0.8813, val_auc 0.7812\n",
      "2022-06-30 00:57:25,954 - NCNet pretrain, Epoch [161 / 300]: loss 0.2776, training auc: 0.8719, val_auc 0.7841\n",
      "2022-06-30 00:57:26,092 - NCNet pretrain, Epoch [162 / 300]: loss 0.2820, training auc: 0.8720, val_auc 0.7841\n",
      "2022-06-30 00:57:26,210 - NCNet pretrain, Epoch [163 / 300]: loss 0.2909, training auc: 0.8880, val_auc 0.7706\n",
      "2022-06-30 00:57:26,327 - NCNet pretrain, Epoch [164 / 300]: loss 0.3256, training auc: 0.8472, val_auc 0.7838\n",
      "2022-06-30 00:57:26,463 - NCNet pretrain, Epoch [165 / 300]: loss 0.2805, training auc: 0.8797, val_auc 0.7814\n",
      "2022-06-30 00:57:26,581 - NCNet pretrain, Epoch [166 / 300]: loss 0.3185, training auc: 0.8736, val_auc 0.7628\n",
      "2022-06-30 00:57:26,698 - NCNet pretrain, Epoch [167 / 300]: loss 0.3326, training auc: 0.8479, val_auc 0.7733\n",
      "2022-06-30 00:57:26,816 - NCNet pretrain, Epoch [168 / 300]: loss 0.3083, training auc: 0.8603, val_auc 0.7792\n",
      "2022-06-30 00:57:26,936 - NCNet pretrain, Epoch [169 / 300]: loss 0.3462, training auc: 0.8748, val_auc 0.7770\n",
      "2022-06-30 00:57:27,054 - NCNet pretrain, Epoch [170 / 300]: loss 0.2858, training auc: 0.8712, val_auc 0.7666\n",
      "2022-06-30 00:57:27,174 - NCNet pretrain, Epoch [171 / 300]: loss 0.3189, training auc: 0.8525, val_auc 0.7836\n",
      "2022-06-30 00:57:27,292 - NCNet pretrain, Epoch [172 / 300]: loss 0.2859, training auc: 0.8709, val_auc 0.7818\n",
      "2022-06-30 00:57:27,293 - Early stop!\n",
      "2022-06-30 00:57:27,294 - Best Test Results: auc 0.8122, ap 0.4368, f1 0.3378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 236638.95it/s]\n",
      "2022-06-30 00:57:28,483 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:57:28,699 - NCNet pretrain, Epoch [1 / 300]: loss 1.0035, training auc: 0.5127, val_auc 0.4554, test auc 0.4253\n",
      "2022-06-30 00:57:28,828 - NCNet pretrain, Epoch [2 / 300]: loss 0.6707, training auc: 0.4599, val_auc 0.4595, test auc 0.4297\n",
      "2022-06-30 00:57:28,977 - NCNet pretrain, Epoch [3 / 300]: loss 0.5129, training auc: 0.4224, val_auc 0.4607, test auc 0.4310\n",
      "2022-06-30 00:57:29,124 - NCNet pretrain, Epoch [4 / 300]: loss 0.4953, training auc: 0.4495, val_auc 0.4610, test auc 0.4312\n",
      "2022-06-30 00:57:29,271 - NCNet pretrain, Epoch [5 / 300]: loss 0.5631, training auc: 0.4212, val_auc 0.4612, test auc 0.4317\n",
      "2022-06-30 00:57:29,400 - NCNet pretrain, Epoch [6 / 300]: loss 0.5521, training auc: 0.4472, val_auc 0.4616, test auc 0.4324\n",
      "2022-06-30 00:57:29,528 - NCNet pretrain, Epoch [7 / 300]: loss 0.5388, training auc: 0.4294, val_auc 0.4629, test auc 0.4345\n",
      "2022-06-30 00:57:29,660 - NCNet pretrain, Epoch [8 / 300]: loss 0.5175, training auc: 0.4052, val_auc 0.4648, test auc 0.4374\n",
      "2022-06-30 00:57:29,793 - NCNet pretrain, Epoch [9 / 300]: loss 0.4958, training auc: 0.4215, val_auc 0.4671, test auc 0.4409\n",
      "2022-06-30 00:57:29,946 - NCNet pretrain, Epoch [10 / 300]: loss 0.4743, training auc: 0.5285, val_auc 0.4690, test auc 0.4437\n",
      "2022-06-30 00:57:30,080 - NCNet pretrain, Epoch [11 / 300]: loss 0.5044, training auc: 0.4542, val_auc 0.4701, test auc 0.4449\n",
      "2022-06-30 00:57:30,211 - NCNet pretrain, Epoch [12 / 300]: loss 0.5116, training auc: 0.4074, val_auc 0.4708, test auc 0.4448\n",
      "2022-06-30 00:57:30,338 - NCNet pretrain, Epoch [13 / 300]: loss 0.5024, training auc: 0.3988, val_auc 0.4706\n",
      "2022-06-30 00:57:30,464 - NCNet pretrain, Epoch [14 / 300]: loss 0.5082, training auc: 0.3769, val_auc 0.4714, test auc 0.4438\n",
      "2022-06-30 00:57:30,592 - NCNet pretrain, Epoch [15 / 300]: loss 0.4911, training auc: 0.4261, val_auc 0.4725, test auc 0.4449\n",
      "2022-06-30 00:57:30,720 - NCNet pretrain, Epoch [16 / 300]: loss 0.4857, training auc: 0.4467, val_auc 0.4740, test auc 0.4466\n",
      "2022-06-30 00:57:30,850 - NCNet pretrain, Epoch [17 / 300]: loss 0.4994, training auc: 0.4082, val_auc 0.4755, test auc 0.4487\n",
      "2022-06-30 00:57:30,983 - NCNet pretrain, Epoch [18 / 300]: loss 0.4837, training auc: 0.4407, val_auc 0.4773, test auc 0.4511\n",
      "2022-06-30 00:57:31,113 - NCNet pretrain, Epoch [19 / 300]: loss 0.4832, training auc: 0.4382, val_auc 0.4789, test auc 0.4533\n",
      "2022-06-30 00:57:31,243 - NCNet pretrain, Epoch [20 / 300]: loss 0.4781, training auc: 0.4625, val_auc 0.4802, test auc 0.4549\n",
      "2022-06-30 00:57:31,373 - NCNet pretrain, Epoch [21 / 300]: loss 0.4773, training auc: 0.4732, val_auc 0.4814, test auc 0.4560\n",
      "2022-06-30 00:57:31,522 - NCNet pretrain, Epoch [22 / 300]: loss 0.4889, training auc: 0.4106, val_auc 0.4829, test auc 0.4571\n",
      "2022-06-30 00:57:31,670 - NCNet pretrain, Epoch [23 / 300]: loss 0.4789, training auc: 0.4444, val_auc 0.4843, test auc 0.4581\n",
      "2022-06-30 00:57:31,801 - NCNet pretrain, Epoch [24 / 300]: loss 0.4714, training auc: 0.4588, val_auc 0.4856, test auc 0.4591\n",
      "2022-06-30 00:57:31,937 - NCNet pretrain, Epoch [25 / 300]: loss 0.4654, training auc: 0.4966, val_auc 0.4868, test auc 0.4603\n",
      "2022-06-30 00:57:32,069 - NCNet pretrain, Epoch [26 / 300]: loss 0.4596, training auc: 0.5255, val_auc 0.4882, test auc 0.4614\n",
      "2022-06-30 00:57:32,199 - NCNet pretrain, Epoch [27 / 300]: loss 0.4730, training auc: 0.4726, val_auc 0.4900, test auc 0.4633\n",
      "2022-06-30 00:57:32,330 - NCNet pretrain, Epoch [28 / 300]: loss 0.4690, training auc: 0.4784, val_auc 0.4926, test auc 0.4662\n",
      "2022-06-30 00:57:32,464 - NCNet pretrain, Epoch [29 / 300]: loss 0.4634, training auc: 0.4963, val_auc 0.4955, test auc 0.4695\n",
      "2022-06-30 00:57:32,593 - NCNet pretrain, Epoch [30 / 300]: loss 0.4666, training auc: 0.4716, val_auc 0.4990, test auc 0.4733\n",
      "2022-06-30 00:57:32,721 - NCNet pretrain, Epoch [31 / 300]: loss 0.4740, training auc: 0.4429, val_auc 0.5022, test auc 0.4769\n",
      "2022-06-30 00:57:32,850 - NCNet pretrain, Epoch [32 / 300]: loss 0.4663, training auc: 0.4684, val_auc 0.5058, test auc 0.4807\n",
      "2022-06-30 00:57:32,997 - NCNet pretrain, Epoch [33 / 300]: loss 0.4596, training auc: 0.4936, val_auc 0.5089, test auc 0.4835\n",
      "2022-06-30 00:57:33,126 - NCNet pretrain, Epoch [34 / 300]: loss 0.4635, training auc: 0.4759, val_auc 0.5116, test auc 0.4860\n",
      "2022-06-30 00:57:33,256 - NCNet pretrain, Epoch [35 / 300]: loss 0.4450, training auc: 0.5445, val_auc 0.5140, test auc 0.4879\n",
      "2022-06-30 00:57:33,386 - NCNet pretrain, Epoch [36 / 300]: loss 0.4702, training auc: 0.4623, val_auc 0.5186, test auc 0.4926\n",
      "2022-06-30 00:57:33,535 - NCNet pretrain, Epoch [37 / 300]: loss 0.4566, training auc: 0.5026, val_auc 0.5257, test auc 0.4999\n",
      "2022-06-30 00:57:33,669 - NCNet pretrain, Epoch [38 / 300]: loss 0.4650, training auc: 0.4712, val_auc 0.5344, test auc 0.5097\n",
      "2022-06-30 00:57:33,799 - NCNet pretrain, Epoch [39 / 300]: loss 0.4436, training auc: 0.5520, val_auc 0.5413, test auc 0.5173\n",
      "2022-06-30 00:57:33,929 - NCNet pretrain, Epoch [40 / 300]: loss 0.4485, training auc: 0.5334, val_auc 0.5460, test auc 0.5219\n",
      "2022-06-30 00:57:34,059 - NCNet pretrain, Epoch [41 / 300]: loss 0.4485, training auc: 0.5201, val_auc 0.5503, test auc 0.5260\n",
      "2022-06-30 00:57:34,189 - NCNet pretrain, Epoch [42 / 300]: loss 0.4469, training auc: 0.5288, val_auc 0.5598, test auc 0.5355\n",
      "2022-06-30 00:57:34,319 - NCNet pretrain, Epoch [43 / 300]: loss 0.4446, training auc: 0.5360, val_auc 0.5725, test auc 0.5499\n",
      "2022-06-30 00:57:34,451 - NCNet pretrain, Epoch [44 / 300]: loss 0.4381, training auc: 0.5612, val_auc 0.5846, test auc 0.5631\n",
      "2022-06-30 00:57:34,585 - NCNet pretrain, Epoch [45 / 300]: loss 0.4354, training auc: 0.5714, val_auc 0.5915, test auc 0.5696\n",
      "2022-06-30 00:57:34,733 - NCNet pretrain, Epoch [46 / 300]: loss 0.4316, training auc: 0.5906, val_auc 0.5943, test auc 0.5716\n",
      "2022-06-30 00:57:34,864 - NCNet pretrain, Epoch [47 / 300]: loss 0.4331, training auc: 0.5722, val_auc 0.6059, test auc 0.5834\n",
      "2022-06-30 00:57:34,992 - NCNet pretrain, Epoch [48 / 300]: loss 0.4382, training auc: 0.5508, val_auc 0.6293, test auc 0.6081\n",
      "2022-06-30 00:57:35,121 - NCNet pretrain, Epoch [49 / 300]: loss 0.4241, training auc: 0.6118, val_auc 0.6475, test auc 0.6271\n",
      "2022-06-30 00:57:35,239 - NCNet pretrain, Epoch [50 / 300]: loss 0.4187, training auc: 0.6577, val_auc 0.6395\n",
      "2022-06-30 00:57:35,376 - NCNet pretrain, Epoch [51 / 300]: loss 0.4124, training auc: 0.6399, val_auc 0.6375\n",
      "2022-06-30 00:57:35,523 - NCNet pretrain, Epoch [52 / 300]: loss 0.4272, training auc: 0.6042, val_auc 0.6626, test auc 0.6401\n",
      "2022-06-30 00:57:35,673 - NCNet pretrain, Epoch [53 / 300]: loss 0.4074, training auc: 0.6624, val_auc 0.6821, test auc 0.6646\n",
      "2022-06-30 00:57:35,792 - NCNet pretrain, Epoch [54 / 300]: loss 0.4131, training auc: 0.6840, val_auc 0.6781\n",
      "2022-06-30 00:57:35,911 - NCNet pretrain, Epoch [55 / 300]: loss 0.4059, training auc: 0.6743, val_auc 0.6681\n",
      "2022-06-30 00:57:36,049 - NCNet pretrain, Epoch [56 / 300]: loss 0.4125, training auc: 0.6542, val_auc 0.6776\n",
      "2022-06-30 00:57:36,198 - NCNet pretrain, Epoch [57 / 300]: loss 0.4110, training auc: 0.6487, val_auc 0.6973, test auc 0.6813\n",
      "2022-06-30 00:57:36,330 - NCNet pretrain, Epoch [58 / 300]: loss 0.3989, training auc: 0.7026, val_auc 0.7037, test auc 0.6900\n",
      "2022-06-30 00:57:36,448 - NCNet pretrain, Epoch [59 / 300]: loss 0.4054, training auc: 0.6945, val_auc 0.6957\n",
      "2022-06-30 00:57:36,567 - NCNet pretrain, Epoch [60 / 300]: loss 0.3974, training auc: 0.6886, val_auc 0.6913\n",
      "2022-06-30 00:57:36,694 - NCNet pretrain, Epoch [61 / 300]: loss 0.3987, training auc: 0.6828, val_auc 0.6995\n",
      "2022-06-30 00:57:36,823 - NCNet pretrain, Epoch [62 / 300]: loss 0.3899, training auc: 0.6982, val_auc 0.7139, test auc 0.7032\n",
      "2022-06-30 00:57:36,952 - NCNet pretrain, Epoch [63 / 300]: loss 0.3968, training auc: 0.7042, val_auc 0.7171, test auc 0.7071\n",
      "2022-06-30 00:57:37,070 - NCNet pretrain, Epoch [64 / 300]: loss 0.3904, training auc: 0.7170, val_auc 0.7094\n",
      "2022-06-30 00:57:37,189 - NCNet pretrain, Epoch [65 / 300]: loss 0.3873, training auc: 0.7125, val_auc 0.7088\n",
      "2022-06-30 00:57:37,318 - NCNet pretrain, Epoch [66 / 300]: loss 0.3898, training auc: 0.6988, val_auc 0.7210, test auc 0.7114\n",
      "2022-06-30 00:57:37,456 - NCNet pretrain, Epoch [67 / 300]: loss 0.3766, training auc: 0.7307, val_auc 0.7265, test auc 0.7190\n",
      "2022-06-30 00:57:37,574 - NCNet pretrain, Epoch [68 / 300]: loss 0.3911, training auc: 0.7168, val_auc 0.7241\n",
      "2022-06-30 00:57:37,692 - NCNet pretrain, Epoch [69 / 300]: loss 0.3823, training auc: 0.7253, val_auc 0.7206\n",
      "2022-06-30 00:57:37,810 - NCNet pretrain, Epoch [70 / 300]: loss 0.3865, training auc: 0.7085, val_auc 0.7230\n",
      "2022-06-30 00:57:37,960 - NCNet pretrain, Epoch [71 / 300]: loss 0.3802, training auc: 0.7196, val_auc 0.7308, test auc 0.7255\n",
      "2022-06-30 00:57:38,091 - NCNet pretrain, Epoch [72 / 300]: loss 0.3798, training auc: 0.7300, val_auc 0.7355, test auc 0.7310\n",
      "2022-06-30 00:57:38,229 - NCNet pretrain, Epoch [73 / 300]: loss 0.3825, training auc: 0.7283, val_auc 0.7303\n",
      "2022-06-30 00:57:38,348 - NCNet pretrain, Epoch [74 / 300]: loss 0.3714, training auc: 0.7414, val_auc 0.7246\n",
      "2022-06-30 00:57:38,471 - NCNet pretrain, Epoch [75 / 300]: loss 0.3795, training auc: 0.7265, val_auc 0.7340\n",
      "2022-06-30 00:57:38,600 - NCNet pretrain, Epoch [76 / 300]: loss 0.3682, training auc: 0.7518, val_auc 0.7416, test auc 0.7403\n",
      "2022-06-30 00:57:38,719 - NCNet pretrain, Epoch [77 / 300]: loss 0.3705, training auc: 0.7576, val_auc 0.7347\n",
      "2022-06-30 00:57:38,845 - NCNet pretrain, Epoch [78 / 300]: loss 0.3780, training auc: 0.7365, val_auc 0.7305\n",
      "2022-06-30 00:57:38,962 - NCNet pretrain, Epoch [79 / 300]: loss 0.3667, training auc: 0.7461, val_auc 0.7365\n",
      "2022-06-30 00:57:39,109 - NCNet pretrain, Epoch [80 / 300]: loss 0.3687, training auc: 0.7419, val_auc 0.7476, test auc 0.7484\n",
      "2022-06-30 00:57:39,245 - NCNet pretrain, Epoch [81 / 300]: loss 0.3660, training auc: 0.7703, val_auc 0.7403\n",
      "2022-06-30 00:57:39,381 - NCNet pretrain, Epoch [82 / 300]: loss 0.3750, training auc: 0.7328, val_auc 0.7358\n",
      "2022-06-30 00:57:39,497 - NCNet pretrain, Epoch [83 / 300]: loss 0.3600, training auc: 0.7580, val_auc 0.7439\n",
      "2022-06-30 00:57:39,614 - NCNet pretrain, Epoch [84 / 300]: loss 0.3559, training auc: 0.7719, val_auc 0.7471\n",
      "2022-06-30 00:57:39,731 - NCNet pretrain, Epoch [85 / 300]: loss 0.3605, training auc: 0.7703, val_auc 0.7437\n",
      "2022-06-30 00:57:39,864 - NCNet pretrain, Epoch [86 / 300]: loss 0.3572, training auc: 0.7678, val_auc 0.7418\n",
      "2022-06-30 00:57:39,994 - NCNet pretrain, Epoch [87 / 300]: loss 0.3637, training auc: 0.7579, val_auc 0.7486, test auc 0.7511\n",
      "2022-06-30 00:57:40,122 - NCNet pretrain, Epoch [88 / 300]: loss 0.3602, training auc: 0.7736, val_auc 0.7489, test auc 0.7517\n",
      "2022-06-30 00:57:40,239 - NCNet pretrain, Epoch [89 / 300]: loss 0.3608, training auc: 0.7645, val_auc 0.7439\n",
      "2022-06-30 00:57:40,357 - NCNet pretrain, Epoch [90 / 300]: loss 0.3478, training auc: 0.7817, val_auc 0.7458\n",
      "2022-06-30 00:57:40,486 - NCNet pretrain, Epoch [91 / 300]: loss 0.3506, training auc: 0.7762, val_auc 0.7502, test auc 0.7550\n",
      "2022-06-30 00:57:40,605 - NCNet pretrain, Epoch [92 / 300]: loss 0.3417, training auc: 0.7939, val_auc 0.7491\n",
      "2022-06-30 00:57:40,729 - NCNet pretrain, Epoch [93 / 300]: loss 0.3555, training auc: 0.7719, val_auc 0.7474\n",
      "2022-06-30 00:57:40,861 - NCNet pretrain, Epoch [94 / 300]: loss 0.3471, training auc: 0.7831, val_auc 0.7499\n",
      "2022-06-30 00:57:40,993 - NCNet pretrain, Epoch [95 / 300]: loss 0.3537, training auc: 0.7759, val_auc 0.7541, test auc 0.7627\n",
      "2022-06-30 00:57:41,133 - NCNet pretrain, Epoch [96 / 300]: loss 0.3493, training auc: 0.7908, val_auc 0.7482\n",
      "2022-06-30 00:57:41,263 - NCNet pretrain, Epoch [97 / 300]: loss 0.3538, training auc: 0.7826, val_auc 0.7567, test auc 0.7675\n",
      "2022-06-30 00:57:41,380 - NCNet pretrain, Epoch [98 / 300]: loss 0.3443, training auc: 0.7962, val_auc 0.7546\n",
      "2022-06-30 00:57:41,496 - NCNet pretrain, Epoch [99 / 300]: loss 0.3476, training auc: 0.7845, val_auc 0.7472\n",
      "2022-06-30 00:57:41,613 - NCNet pretrain, Epoch [100 / 300]: loss 0.3453, training auc: 0.7901, val_auc 0.7538\n",
      "2022-06-30 00:57:41,740 - NCNet pretrain, Epoch [101 / 300]: loss 0.3428, training auc: 0.7939, val_auc 0.7595, test auc 0.7715\n",
      "2022-06-30 00:57:41,856 - NCNet pretrain, Epoch [102 / 300]: loss 0.3478, training auc: 0.8061, val_auc 0.7487\n",
      "2022-06-30 00:57:41,973 - NCNet pretrain, Epoch [103 / 300]: loss 0.3408, training auc: 0.7974, val_auc 0.7542\n",
      "2022-06-30 00:57:42,103 - NCNet pretrain, Epoch [104 / 300]: loss 0.3400, training auc: 0.8015, val_auc 0.7612, test auc 0.7735\n",
      "2022-06-30 00:57:42,240 - NCNet pretrain, Epoch [105 / 300]: loss 0.3413, training auc: 0.8095, val_auc 0.7482\n",
      "2022-06-30 00:57:42,377 - NCNet pretrain, Epoch [106 / 300]: loss 0.3412, training auc: 0.7921, val_auc 0.7532\n",
      "2022-06-30 00:57:42,495 - NCNet pretrain, Epoch [107 / 300]: loss 0.3465, training auc: 0.7887, val_auc 0.7606\n",
      "2022-06-30 00:57:42,612 - NCNet pretrain, Epoch [108 / 300]: loss 0.3448, training auc: 0.8051, val_auc 0.7540\n",
      "2022-06-30 00:57:42,730 - NCNet pretrain, Epoch [109 / 300]: loss 0.3374, training auc: 0.7984, val_auc 0.7504\n",
      "2022-06-30 00:57:42,852 - NCNet pretrain, Epoch [110 / 300]: loss 0.3447, training auc: 0.7904, val_auc 0.7601\n",
      "2022-06-30 00:57:42,981 - NCNet pretrain, Epoch [111 / 300]: loss 0.3320, training auc: 0.8161, val_auc 0.7625, test auc 0.7761\n",
      "2022-06-30 00:57:43,101 - NCNet pretrain, Epoch [112 / 300]: loss 0.3332, training auc: 0.8205, val_auc 0.7511\n",
      "2022-06-30 00:57:43,226 - NCNet pretrain, Epoch [113 / 300]: loss 0.3384, training auc: 0.8022, val_auc 0.7510\n",
      "2022-06-30 00:57:43,362 - NCNet pretrain, Epoch [114 / 300]: loss 0.3356, training auc: 0.8016, val_auc 0.7665, test auc 0.7818\n",
      "2022-06-30 00:57:43,478 - NCNet pretrain, Epoch [115 / 300]: loss 0.3410, training auc: 0.8253, val_auc 0.7559\n",
      "2022-06-30 00:57:43,595 - NCNet pretrain, Epoch [116 / 300]: loss 0.3375, training auc: 0.8000, val_auc 0.7458\n",
      "2022-06-30 00:57:43,731 - NCNet pretrain, Epoch [117 / 300]: loss 0.3439, training auc: 0.7967, val_auc 0.7623\n",
      "2022-06-30 00:57:43,869 - NCNet pretrain, Epoch [118 / 300]: loss 0.3448, training auc: 0.7993, val_auc 0.7648\n",
      "2022-06-30 00:57:43,986 - NCNet pretrain, Epoch [119 / 300]: loss 0.3332, training auc: 0.8274, val_auc 0.7533\n",
      "2022-06-30 00:57:44,105 - NCNet pretrain, Epoch [120 / 300]: loss 0.3276, training auc: 0.8113, val_auc 0.7457\n",
      "2022-06-30 00:57:44,222 - NCNet pretrain, Epoch [121 / 300]: loss 0.3411, training auc: 0.8006, val_auc 0.7607\n",
      "2022-06-30 00:57:44,350 - NCNet pretrain, Epoch [122 / 300]: loss 0.3278, training auc: 0.8175, val_auc 0.7685, test auc 0.7852\n",
      "2022-06-30 00:57:44,467 - NCNet pretrain, Epoch [123 / 300]: loss 0.3331, training auc: 0.8390, val_auc 0.7544\n",
      "2022-06-30 00:57:44,584 - NCNet pretrain, Epoch [124 / 300]: loss 0.3282, training auc: 0.8133, val_auc 0.7466\n",
      "2022-06-30 00:57:44,700 - NCNet pretrain, Epoch [125 / 300]: loss 0.3360, training auc: 0.8136, val_auc 0.7605\n",
      "2022-06-30 00:57:44,828 - NCNet pretrain, Epoch [126 / 300]: loss 0.3221, training auc: 0.8198, val_auc 0.7686, test auc 0.7855\n",
      "2022-06-30 00:57:44,946 - NCNet pretrain, Epoch [127 / 300]: loss 0.3510, training auc: 0.8240, val_auc 0.7528\n",
      "2022-06-30 00:57:45,066 - NCNet pretrain, Epoch [128 / 300]: loss 0.3301, training auc: 0.8163, val_auc 0.7475\n",
      "2022-06-30 00:57:45,184 - NCNet pretrain, Epoch [129 / 300]: loss 0.3404, training auc: 0.8081, val_auc 0.7629\n",
      "2022-06-30 00:57:45,305 - NCNet pretrain, Epoch [130 / 300]: loss 0.3200, training auc: 0.8393, val_auc 0.7663\n",
      "2022-06-30 00:57:45,425 - NCNet pretrain, Epoch [131 / 300]: loss 0.3266, training auc: 0.8416, val_auc 0.7554\n",
      "2022-06-30 00:57:45,542 - NCNet pretrain, Epoch [132 / 300]: loss 0.3168, training auc: 0.8274, val_auc 0.7476\n",
      "2022-06-30 00:57:45,658 - NCNet pretrain, Epoch [133 / 300]: loss 0.3321, training auc: 0.8194, val_auc 0.7588\n",
      "2022-06-30 00:57:45,775 - NCNet pretrain, Epoch [134 / 300]: loss 0.3201, training auc: 0.8286, val_auc 0.7676\n",
      "2022-06-30 00:57:45,891 - NCNet pretrain, Epoch [135 / 300]: loss 0.3381, training auc: 0.8372, val_auc 0.7616\n",
      "2022-06-30 00:57:46,008 - NCNet pretrain, Epoch [136 / 300]: loss 0.3242, training auc: 0.8235, val_auc 0.7523\n",
      "2022-06-30 00:57:46,136 - NCNet pretrain, Epoch [137 / 300]: loss 0.3299, training auc: 0.8169, val_auc 0.7576\n",
      "2022-06-30 00:57:46,259 - NCNet pretrain, Epoch [138 / 300]: loss 0.3231, training auc: 0.8239, val_auc 0.7665\n",
      "2022-06-30 00:57:46,377 - NCNet pretrain, Epoch [139 / 300]: loss 0.3269, training auc: 0.8362, val_auc 0.7648\n",
      "2022-06-30 00:57:46,494 - NCNet pretrain, Epoch [140 / 300]: loss 0.3191, training auc: 0.8334, val_auc 0.7542\n",
      "2022-06-30 00:57:46,611 - NCNet pretrain, Epoch [141 / 300]: loss 0.3291, training auc: 0.8158, val_auc 0.7526\n",
      "2022-06-30 00:57:46,747 - NCNet pretrain, Epoch [142 / 300]: loss 0.3187, training auc: 0.8290, val_auc 0.7624\n",
      "2022-06-30 00:57:46,864 - NCNet pretrain, Epoch [143 / 300]: loss 0.3169, training auc: 0.8334, val_auc 0.7676\n",
      "2022-06-30 00:57:46,982 - NCNet pretrain, Epoch [144 / 300]: loss 0.3274, training auc: 0.8436, val_auc 0.7583\n",
      "2022-06-30 00:57:47,113 - NCNet pretrain, Epoch [145 / 300]: loss 0.3237, training auc: 0.8221, val_auc 0.7546\n",
      "2022-06-30 00:57:47,249 - NCNet pretrain, Epoch [146 / 300]: loss 0.3248, training auc: 0.8225, val_auc 0.7605\n",
      "2022-06-30 00:57:47,367 - NCNet pretrain, Epoch [147 / 300]: loss 0.3247, training auc: 0.8210, val_auc 0.7670\n",
      "2022-06-30 00:57:47,506 - NCNet pretrain, Epoch [148 / 300]: loss 0.3205, training auc: 0.8389, val_auc 0.7635\n",
      "2022-06-30 00:57:47,643 - NCNet pretrain, Epoch [149 / 300]: loss 0.3139, training auc: 0.8428, val_auc 0.7511\n",
      "2022-06-30 00:57:47,770 - NCNet pretrain, Epoch [150 / 300]: loss 0.3237, training auc: 0.8273, val_auc 0.7572\n",
      "2022-06-30 00:57:47,886 - NCNet pretrain, Epoch [151 / 300]: loss 0.3166, training auc: 0.8342, val_auc 0.7664\n",
      "2022-06-30 00:57:48,002 - NCNet pretrain, Epoch [152 / 300]: loss 0.3284, training auc: 0.8367, val_auc 0.7627\n",
      "2022-06-30 00:57:48,119 - NCNet pretrain, Epoch [153 / 300]: loss 0.3166, training auc: 0.8323, val_auc 0.7546\n",
      "2022-06-30 00:57:48,236 - NCNet pretrain, Epoch [154 / 300]: loss 0.3212, training auc: 0.8253, val_auc 0.7572\n",
      "2022-06-30 00:57:48,352 - NCNet pretrain, Epoch [155 / 300]: loss 0.3100, training auc: 0.8401, val_auc 0.7615\n",
      "2022-06-30 00:57:48,468 - NCNet pretrain, Epoch [156 / 300]: loss 0.3268, training auc: 0.8215, val_auc 0.7605\n",
      "2022-06-30 00:57:48,585 - NCNet pretrain, Epoch [157 / 300]: loss 0.3235, training auc: 0.8286, val_auc 0.7549\n",
      "2022-06-30 00:57:48,701 - NCNet pretrain, Epoch [158 / 300]: loss 0.3226, training auc: 0.8231, val_auc 0.7576\n",
      "2022-06-30 00:57:48,817 - NCNet pretrain, Epoch [159 / 300]: loss 0.3128, training auc: 0.8445, val_auc 0.7629\n",
      "2022-06-30 00:57:48,934 - NCNet pretrain, Epoch [160 / 300]: loss 0.3171, training auc: 0.8400, val_auc 0.7627\n",
      "2022-06-30 00:57:49,051 - NCNet pretrain, Epoch [161 / 300]: loss 0.3150, training auc: 0.8365, val_auc 0.7570\n",
      "2022-06-30 00:57:49,172 - NCNet pretrain, Epoch [162 / 300]: loss 0.3101, training auc: 0.8388, val_auc 0.7579\n",
      "2022-06-30 00:57:49,289 - NCNet pretrain, Epoch [163 / 300]: loss 0.3104, training auc: 0.8384, val_auc 0.7625\n",
      "2022-06-30 00:57:49,406 - NCNet pretrain, Epoch [164 / 300]: loss 0.3113, training auc: 0.8434, val_auc 0.7616\n",
      "2022-06-30 00:57:49,524 - NCNet pretrain, Epoch [165 / 300]: loss 0.3115, training auc: 0.8335, val_auc 0.7594\n",
      "2022-06-30 00:57:49,647 - NCNet pretrain, Epoch [166 / 300]: loss 0.3191, training auc: 0.8294, val_auc 0.7578\n",
      "2022-06-30 00:57:49,763 - NCNet pretrain, Epoch [167 / 300]: loss 0.3085, training auc: 0.8421, val_auc 0.7603\n",
      "2022-06-30 00:57:49,880 - NCNet pretrain, Epoch [168 / 300]: loss 0.3160, training auc: 0.8342, val_auc 0.7598\n",
      "2022-06-30 00:57:49,996 - NCNet pretrain, Epoch [169 / 300]: loss 0.3132, training auc: 0.8385, val_auc 0.7618\n",
      "2022-06-30 00:57:50,113 - NCNet pretrain, Epoch [170 / 300]: loss 0.3119, training auc: 0.8406, val_auc 0.7611\n",
      "2022-06-30 00:57:50,229 - NCNet pretrain, Epoch [171 / 300]: loss 0.3023, training auc: 0.8512, val_auc 0.7562\n",
      "2022-06-30 00:57:50,345 - NCNet pretrain, Epoch [172 / 300]: loss 0.3113, training auc: 0.8354, val_auc 0.7607\n",
      "2022-06-30 00:57:50,461 - NCNet pretrain, Epoch [173 / 300]: loss 0.3149, training auc: 0.8358, val_auc 0.7614\n",
      "2022-06-30 00:57:50,577 - NCNet pretrain, Epoch [174 / 300]: loss 0.3113, training auc: 0.8428, val_auc 0.7566\n",
      "2022-06-30 00:57:50,698 - NCNet pretrain, Epoch [175 / 300]: loss 0.3144, training auc: 0.8369, val_auc 0.7604\n",
      "2022-06-30 00:57:50,814 - NCNet pretrain, Epoch [176 / 300]: loss 0.3080, training auc: 0.8427, val_auc 0.7606\n",
      "2022-06-30 00:57:50,815 - Early stop!\n",
      "2022-06-30 00:57:50,816 - Best Test Results: auc 0.7855, ap 0.4022, f1 0.3396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 319447.77it/s]\n",
      "2022-06-30 00:57:51,804 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:57:52,019 - NCNet pretrain, Epoch [1 / 300]: loss 0.7632, training auc: 0.5274, val_auc 0.4700, test auc 0.4449\n",
      "2022-06-30 00:57:52,146 - NCNet pretrain, Epoch [2 / 300]: loss 0.5063, training auc: 0.4643, val_auc 0.4651\n",
      "2022-06-30 00:57:52,263 - NCNet pretrain, Epoch [3 / 300]: loss 0.4980, training auc: 0.4446, val_auc 0.4642\n",
      "2022-06-30 00:57:52,379 - NCNet pretrain, Epoch [4 / 300]: loss 0.5501, training auc: 0.4212, val_auc 0.4650\n",
      "2022-06-30 00:57:52,498 - NCNet pretrain, Epoch [5 / 300]: loss 0.5361, training auc: 0.4166, val_auc 0.4674\n",
      "2022-06-30 00:57:52,625 - NCNet pretrain, Epoch [6 / 300]: loss 0.5066, training auc: 0.4174, val_auc 0.4703, test auc 0.4460\n",
      "2022-06-30 00:57:52,759 - NCNet pretrain, Epoch [7 / 300]: loss 0.4862, training auc: 0.4357, val_auc 0.4752, test auc 0.4534\n",
      "2022-06-30 00:57:52,901 - NCNet pretrain, Epoch [8 / 300]: loss 0.4986, training auc: 0.4113, val_auc 0.4789, test auc 0.4590\n",
      "2022-06-30 00:57:53,049 - NCNet pretrain, Epoch [9 / 300]: loss 0.4933, training auc: 0.4826, val_auc 0.4800, test auc 0.4605\n",
      "2022-06-30 00:57:53,186 - NCNet pretrain, Epoch [10 / 300]: loss 0.5007, training auc: 0.4292, val_auc 0.4799\n",
      "2022-06-30 00:57:53,304 - NCNet pretrain, Epoch [11 / 300]: loss 0.4970, training auc: 0.4139, val_auc 0.4793\n",
      "2022-06-30 00:57:53,441 - NCNet pretrain, Epoch [12 / 300]: loss 0.4920, training auc: 0.4155, val_auc 0.4793\n",
      "2022-06-30 00:57:53,562 - NCNet pretrain, Epoch [13 / 300]: loss 0.4875, training auc: 0.4222, val_auc 0.4795\n",
      "2022-06-30 00:57:53,686 - NCNet pretrain, Epoch [14 / 300]: loss 0.4835, training auc: 0.4428, val_auc 0.4805, test auc 0.4576\n",
      "2022-06-30 00:57:53,809 - NCNet pretrain, Epoch [15 / 300]: loss 0.4776, training auc: 0.4612, val_auc 0.4821, test auc 0.4594\n",
      "2022-06-30 00:57:53,963 - NCNet pretrain, Epoch [16 / 300]: loss 0.4789, training auc: 0.4458, val_auc 0.4836, test auc 0.4616\n",
      "2022-06-30 00:57:54,094 - NCNet pretrain, Epoch [17 / 300]: loss 0.4847, training auc: 0.4240, val_auc 0.4853, test auc 0.4637\n",
      "2022-06-30 00:57:54,224 - NCNet pretrain, Epoch [18 / 300]: loss 0.4790, training auc: 0.4424, val_auc 0.4869, test auc 0.4658\n",
      "2022-06-30 00:57:54,351 - NCNet pretrain, Epoch [19 / 300]: loss 0.4738, training auc: 0.4572, val_auc 0.4884, test auc 0.4674\n",
      "2022-06-30 00:57:54,478 - NCNet pretrain, Epoch [20 / 300]: loss 0.4671, training auc: 0.4968, val_auc 0.4892, test auc 0.4680\n",
      "2022-06-30 00:57:54,604 - NCNet pretrain, Epoch [21 / 300]: loss 0.4688, training auc: 0.4938, val_auc 0.4898, test auc 0.4682\n",
      "2022-06-30 00:57:54,731 - NCNet pretrain, Epoch [22 / 300]: loss 0.4780, training auc: 0.4390, val_auc 0.4911, test auc 0.4687\n",
      "2022-06-30 00:57:54,858 - NCNet pretrain, Epoch [23 / 300]: loss 0.4695, training auc: 0.4654, val_auc 0.4922, test auc 0.4692\n",
      "2022-06-30 00:57:54,986 - NCNet pretrain, Epoch [24 / 300]: loss 0.4632, training auc: 0.4794, val_auc 0.4936, test auc 0.4703\n",
      "2022-06-30 00:57:55,114 - NCNet pretrain, Epoch [25 / 300]: loss 0.4632, training auc: 0.4750, val_auc 0.4949, test auc 0.4715\n",
      "2022-06-30 00:57:55,243 - NCNet pretrain, Epoch [26 / 300]: loss 0.4638, training auc: 0.4769, val_auc 0.4967, test auc 0.4732\n",
      "2022-06-30 00:57:55,372 - NCNet pretrain, Epoch [27 / 300]: loss 0.4616, training auc: 0.4788, val_auc 0.4989, test auc 0.4757\n",
      "2022-06-30 00:57:55,517 - NCNet pretrain, Epoch [28 / 300]: loss 0.4601, training auc: 0.4813, val_auc 0.5016, test auc 0.4786\n",
      "2022-06-30 00:57:55,645 - NCNet pretrain, Epoch [29 / 300]: loss 0.4585, training auc: 0.4763, val_auc 0.5048, test auc 0.4821\n",
      "2022-06-30 00:57:55,775 - NCNet pretrain, Epoch [30 / 300]: loss 0.4541, training auc: 0.5092, val_auc 0.5083, test auc 0.4854\n",
      "2022-06-30 00:57:55,903 - NCNet pretrain, Epoch [31 / 300]: loss 0.4604, training auc: 0.4792, val_auc 0.5114, test auc 0.4885\n",
      "2022-06-30 00:57:56,030 - NCNet pretrain, Epoch [32 / 300]: loss 0.4609, training auc: 0.4512, val_auc 0.5150, test auc 0.4923\n",
      "2022-06-30 00:57:56,170 - NCNet pretrain, Epoch [33 / 300]: loss 0.4576, training auc: 0.4738, val_auc 0.5191, test auc 0.4961\n",
      "2022-06-30 00:57:56,297 - NCNet pretrain, Epoch [34 / 300]: loss 0.4452, training auc: 0.5251, val_auc 0.5232, test auc 0.4998\n",
      "2022-06-30 00:57:56,425 - NCNet pretrain, Epoch [35 / 300]: loss 0.4491, training auc: 0.4958, val_auc 0.5268, test auc 0.5036\n",
      "2022-06-30 00:57:56,553 - NCNet pretrain, Epoch [36 / 300]: loss 0.4504, training auc: 0.4879, val_auc 0.5321, test auc 0.5095\n",
      "2022-06-30 00:57:56,684 - NCNet pretrain, Epoch [37 / 300]: loss 0.4413, training auc: 0.5211, val_auc 0.5395, test auc 0.5169\n",
      "2022-06-30 00:57:56,831 - NCNet pretrain, Epoch [38 / 300]: loss 0.4444, training auc: 0.5054, val_auc 0.5481, test auc 0.5256\n",
      "2022-06-30 00:57:56,958 - NCNet pretrain, Epoch [39 / 300]: loss 0.4463, training auc: 0.4964, val_auc 0.5576, test auc 0.5357\n",
      "2022-06-30 00:57:57,085 - NCNet pretrain, Epoch [40 / 300]: loss 0.4359, training auc: 0.5440, val_auc 0.5661, test auc 0.5443\n",
      "2022-06-30 00:57:57,216 - NCNet pretrain, Epoch [41 / 300]: loss 0.4321, training auc: 0.5700, val_auc 0.5723, test auc 0.5506\n",
      "2022-06-30 00:57:57,344 - NCNet pretrain, Epoch [42 / 300]: loss 0.4439, training auc: 0.5045, val_auc 0.5827, test auc 0.5616\n",
      "2022-06-30 00:57:57,477 - NCNet pretrain, Epoch [43 / 300]: loss 0.4358, training auc: 0.5385, val_auc 0.5932, test auc 0.5731\n",
      "2022-06-30 00:57:57,604 - NCNet pretrain, Epoch [44 / 300]: loss 0.4335, training auc: 0.5512, val_auc 0.6057, test auc 0.5865\n",
      "2022-06-30 00:57:57,733 - NCNet pretrain, Epoch [45 / 300]: loss 0.4217, training auc: 0.6002, val_auc 0.6131, test auc 0.5934\n",
      "2022-06-30 00:57:57,862 - NCNet pretrain, Epoch [46 / 300]: loss 0.4194, training auc: 0.6033, val_auc 0.6196, test auc 0.5988\n",
      "2022-06-30 00:57:57,990 - NCNet pretrain, Epoch [47 / 300]: loss 0.4227, training auc: 0.5850, val_auc 0.6392, test auc 0.6187\n",
      "2022-06-30 00:57:58,119 - NCNet pretrain, Epoch [48 / 300]: loss 0.4195, training auc: 0.6045, val_auc 0.6612, test auc 0.6441\n",
      "2022-06-30 00:57:58,254 - NCNet pretrain, Epoch [49 / 300]: loss 0.4095, training auc: 0.6607, val_auc 0.6672, test auc 0.6477\n",
      "2022-06-30 00:57:58,376 - NCNet pretrain, Epoch [50 / 300]: loss 0.4059, training auc: 0.6542, val_auc 0.6678, test auc 0.6462\n",
      "2022-06-30 00:57:58,504 - NCNet pretrain, Epoch [51 / 300]: loss 0.4066, training auc: 0.6437, val_auc 0.6788, test auc 0.6571\n",
      "2022-06-30 00:57:58,651 - NCNet pretrain, Epoch [52 / 300]: loss 0.3891, training auc: 0.6807, val_auc 0.6923, test auc 0.6713\n",
      "2022-06-30 00:57:58,778 - NCNet pretrain, Epoch [53 / 300]: loss 0.3982, training auc: 0.6676, val_auc 0.7035, test auc 0.6842\n",
      "2022-06-30 00:57:58,925 - NCNet pretrain, Epoch [54 / 300]: loss 0.3948, training auc: 0.6873, val_auc 0.7102, test auc 0.6923\n",
      "2022-06-30 00:57:59,066 - NCNet pretrain, Epoch [55 / 300]: loss 0.3875, training auc: 0.7095, val_auc 0.7113, test auc 0.6920\n",
      "2022-06-30 00:57:59,212 - NCNet pretrain, Epoch [56 / 300]: loss 0.3841, training auc: 0.7085, val_auc 0.7188, test auc 0.7013\n",
      "2022-06-30 00:57:59,340 - NCNet pretrain, Epoch [57 / 300]: loss 0.3874, training auc: 0.7059, val_auc 0.7279, test auc 0.7134\n",
      "2022-06-30 00:57:59,468 - NCNet pretrain, Epoch [58 / 300]: loss 0.3780, training auc: 0.7319, val_auc 0.7295, test auc 0.7150\n",
      "2022-06-30 00:57:59,616 - NCNet pretrain, Epoch [59 / 300]: loss 0.3826, training auc: 0.7158, val_auc 0.7305, test auc 0.7164\n",
      "2022-06-30 00:57:59,764 - NCNet pretrain, Epoch [60 / 300]: loss 0.3737, training auc: 0.7307, val_auc 0.7387, test auc 0.7275\n",
      "2022-06-30 00:57:59,881 - NCNet pretrain, Epoch [61 / 300]: loss 0.3726, training auc: 0.7403, val_auc 0.7361\n",
      "2022-06-30 00:58:00,001 - NCNet pretrain, Epoch [62 / 300]: loss 0.3666, training auc: 0.7339, val_auc 0.7375\n",
      "2022-06-30 00:58:00,130 - NCNet pretrain, Epoch [63 / 300]: loss 0.3661, training auc: 0.7461, val_auc 0.7522, test auc 0.7496\n",
      "2022-06-30 00:58:00,250 - NCNet pretrain, Epoch [64 / 300]: loss 0.3681, training auc: 0.7665, val_auc 0.7401\n",
      "2022-06-30 00:58:00,372 - NCNet pretrain, Epoch [65 / 300]: loss 0.3687, training auc: 0.7319, val_auc 0.7417\n",
      "2022-06-30 00:58:00,502 - NCNet pretrain, Epoch [66 / 300]: loss 0.3695, training auc: 0.7367, val_auc 0.7565, test auc 0.7573\n",
      "2022-06-30 00:58:00,630 - NCNet pretrain, Epoch [67 / 300]: loss 0.3626, training auc: 0.7709, val_auc 0.7569, test auc 0.7577\n",
      "2022-06-30 00:58:00,748 - NCNet pretrain, Epoch [68 / 300]: loss 0.3749, training auc: 0.7366, val_auc 0.7466\n",
      "2022-06-30 00:58:00,865 - NCNet pretrain, Epoch [69 / 300]: loss 0.3618, training auc: 0.7536, val_auc 0.7474\n",
      "2022-06-30 00:58:00,994 - NCNet pretrain, Epoch [70 / 300]: loss 0.3622, training auc: 0.7531, val_auc 0.7603, test auc 0.7639\n",
      "2022-06-30 00:58:01,123 - NCNet pretrain, Epoch [71 / 300]: loss 0.3658, training auc: 0.7558, val_auc 0.7622, test auc 0.7685\n",
      "2022-06-30 00:58:01,241 - NCNet pretrain, Epoch [72 / 300]: loss 0.3619, training auc: 0.7837, val_auc 0.7540\n",
      "2022-06-30 00:58:01,358 - NCNet pretrain, Epoch [73 / 300]: loss 0.3521, training auc: 0.7726, val_auc 0.7536\n",
      "2022-06-30 00:58:01,487 - NCNet pretrain, Epoch [74 / 300]: loss 0.3725, training auc: 0.7372, val_auc 0.7633, test auc 0.7692\n",
      "2022-06-30 00:58:01,643 - NCNet pretrain, Epoch [75 / 300]: loss 0.3601, training auc: 0.7635, val_auc 0.7656, test auc 0.7740\n",
      "2022-06-30 00:58:01,782 - NCNet pretrain, Epoch [76 / 300]: loss 0.3623, training auc: 0.7838, val_auc 0.7582\n",
      "2022-06-30 00:58:01,902 - NCNet pretrain, Epoch [77 / 300]: loss 0.3578, training auc: 0.7688, val_auc 0.7555\n",
      "2022-06-30 00:58:02,021 - NCNet pretrain, Epoch [78 / 300]: loss 0.3463, training auc: 0.7853, val_auc 0.7628\n",
      "2022-06-30 00:58:02,175 - NCNet pretrain, Epoch [79 / 300]: loss 0.3505, training auc: 0.7780, val_auc 0.7677, test auc 0.7752\n",
      "2022-06-30 00:58:02,315 - NCNet pretrain, Epoch [80 / 300]: loss 0.3532, training auc: 0.7942, val_auc 0.7637\n",
      "2022-06-30 00:58:02,456 - NCNet pretrain, Epoch [81 / 300]: loss 0.3450, training auc: 0.7878, val_auc 0.7594\n",
      "2022-06-30 00:58:02,579 - NCNet pretrain, Epoch [82 / 300]: loss 0.3504, training auc: 0.7781, val_auc 0.7629\n",
      "2022-06-30 00:58:02,708 - NCNet pretrain, Epoch [83 / 300]: loss 0.3534, training auc: 0.7719, val_auc 0.7683, test auc 0.7771\n",
      "2022-06-30 00:58:02,826 - NCNet pretrain, Epoch [84 / 300]: loss 0.3425, training auc: 0.7942, val_auc 0.7682\n",
      "2022-06-30 00:58:02,943 - NCNet pretrain, Epoch [85 / 300]: loss 0.3422, training auc: 0.7972, val_auc 0.7610\n",
      "2022-06-30 00:58:03,061 - NCNet pretrain, Epoch [86 / 300]: loss 0.3540, training auc: 0.7698, val_auc 0.7608\n",
      "2022-06-30 00:58:03,178 - NCNet pretrain, Epoch [87 / 300]: loss 0.3407, training auc: 0.7922, val_auc 0.7676\n",
      "2022-06-30 00:58:03,318 - NCNet pretrain, Epoch [88 / 300]: loss 0.3466, training auc: 0.7926, val_auc 0.7708, test auc 0.7835\n",
      "2022-06-30 00:58:03,455 - NCNet pretrain, Epoch [89 / 300]: loss 0.3420, training auc: 0.8096, val_auc 0.7653\n",
      "2022-06-30 00:58:03,572 - NCNet pretrain, Epoch [90 / 300]: loss 0.3357, training auc: 0.7976, val_auc 0.7602\n",
      "2022-06-30 00:58:03,689 - NCNet pretrain, Epoch [91 / 300]: loss 0.3465, training auc: 0.7865, val_auc 0.7666\n",
      "2022-06-30 00:58:03,807 - NCNet pretrain, Epoch [92 / 300]: loss 0.3415, training auc: 0.7975, val_auc 0.7706\n",
      "2022-06-30 00:58:03,924 - NCNet pretrain, Epoch [93 / 300]: loss 0.3389, training auc: 0.8089, val_auc 0.7692\n",
      "2022-06-30 00:58:04,042 - NCNet pretrain, Epoch [94 / 300]: loss 0.3266, training auc: 0.8111, val_auc 0.7636\n",
      "2022-06-30 00:58:04,159 - NCNet pretrain, Epoch [95 / 300]: loss 0.3379, training auc: 0.7970, val_auc 0.7655\n",
      "2022-06-30 00:58:04,281 - NCNet pretrain, Epoch [96 / 300]: loss 0.3314, training auc: 0.8042, val_auc 0.7707\n",
      "2022-06-30 00:58:04,410 - NCNet pretrain, Epoch [97 / 300]: loss 0.3381, training auc: 0.8065, val_auc 0.7712, test auc 0.7867\n",
      "2022-06-30 00:58:04,527 - NCNet pretrain, Epoch [98 / 300]: loss 0.3330, training auc: 0.8161, val_auc 0.7673\n",
      "2022-06-30 00:58:04,651 - NCNet pretrain, Epoch [99 / 300]: loss 0.3247, training auc: 0.8163, val_auc 0.7673\n",
      "2022-06-30 00:58:04,778 - NCNet pretrain, Epoch [100 / 300]: loss 0.3374, training auc: 0.8026, val_auc 0.7713, test auc 0.7877\n",
      "2022-06-30 00:58:04,895 - NCNet pretrain, Epoch [101 / 300]: loss 0.3270, training auc: 0.8249, val_auc 0.7713\n",
      "2022-06-30 00:58:05,031 - NCNet pretrain, Epoch [102 / 300]: loss 0.3281, training auc: 0.8171, val_auc 0.7658\n",
      "2022-06-30 00:58:05,148 - NCNet pretrain, Epoch [103 / 300]: loss 0.3301, training auc: 0.8114, val_auc 0.7690\n",
      "2022-06-30 00:58:05,278 - NCNet pretrain, Epoch [104 / 300]: loss 0.3293, training auc: 0.8058, val_auc 0.7728, test auc 0.7916\n",
      "2022-06-30 00:58:05,414 - NCNet pretrain, Epoch [105 / 300]: loss 0.3394, training auc: 0.8078, val_auc 0.7724\n",
      "2022-06-30 00:58:05,535 - NCNet pretrain, Epoch [106 / 300]: loss 0.3371, training auc: 0.8016, val_auc 0.7676\n",
      "2022-06-30 00:58:05,652 - NCNet pretrain, Epoch [107 / 300]: loss 0.3343, training auc: 0.8146, val_auc 0.7707\n",
      "2022-06-30 00:58:05,769 - NCNet pretrain, Epoch [108 / 300]: loss 0.3361, training auc: 0.8026, val_auc 0.7725\n",
      "2022-06-30 00:58:05,905 - NCNet pretrain, Epoch [109 / 300]: loss 0.3194, training auc: 0.8340, val_auc 0.7707\n",
      "2022-06-30 00:58:06,022 - NCNet pretrain, Epoch [110 / 300]: loss 0.3254, training auc: 0.8166, val_auc 0.7688\n",
      "2022-06-30 00:58:06,139 - NCNet pretrain, Epoch [111 / 300]: loss 0.3270, training auc: 0.8129, val_auc 0.7709\n",
      "2022-06-30 00:58:06,269 - NCNet pretrain, Epoch [112 / 300]: loss 0.3271, training auc: 0.8184, val_auc 0.7732, test auc 0.7929\n",
      "2022-06-30 00:58:06,409 - NCNet pretrain, Epoch [113 / 300]: loss 0.3220, training auc: 0.8292, val_auc 0.7729\n",
      "2022-06-30 00:58:06,541 - NCNet pretrain, Epoch [114 / 300]: loss 0.3253, training auc: 0.8184, val_auc 0.7700\n",
      "2022-06-30 00:58:06,659 - NCNet pretrain, Epoch [115 / 300]: loss 0.3285, training auc: 0.8169, val_auc 0.7728\n",
      "2022-06-30 00:58:06,787 - NCNet pretrain, Epoch [116 / 300]: loss 0.3254, training auc: 0.8183, val_auc 0.7743, test auc 0.7968\n",
      "2022-06-30 00:58:06,909 - NCNet pretrain, Epoch [117 / 300]: loss 0.3275, training auc: 0.8287, val_auc 0.7696\n",
      "2022-06-30 00:58:07,025 - NCNet pretrain, Epoch [118 / 300]: loss 0.3221, training auc: 0.8207, val_auc 0.7673\n",
      "2022-06-30 00:58:07,153 - NCNet pretrain, Epoch [119 / 300]: loss 0.3274, training auc: 0.8124, val_auc 0.7751, test auc 0.7973\n",
      "2022-06-30 00:58:07,289 - NCNet pretrain, Epoch [120 / 300]: loss 0.3276, training auc: 0.8300, val_auc 0.7747\n",
      "2022-06-30 00:58:07,424 - NCNet pretrain, Epoch [121 / 300]: loss 0.3187, training auc: 0.8315, val_auc 0.7709\n",
      "2022-06-30 00:58:07,554 - NCNet pretrain, Epoch [122 / 300]: loss 0.3174, training auc: 0.8257, val_auc 0.7722\n",
      "2022-06-30 00:58:07,686 - NCNet pretrain, Epoch [123 / 300]: loss 0.3191, training auc: 0.8254, val_auc 0.7752, test auc 0.7970\n",
      "2022-06-30 00:58:07,802 - NCNet pretrain, Epoch [124 / 300]: loss 0.3175, training auc: 0.8417, val_auc 0.7734\n",
      "2022-06-30 00:58:07,938 - NCNet pretrain, Epoch [125 / 300]: loss 0.3153, training auc: 0.8297, val_auc 0.7685\n",
      "2022-06-30 00:58:08,055 - NCNet pretrain, Epoch [126 / 300]: loss 0.3324, training auc: 0.8173, val_auc 0.7747\n",
      "2022-06-30 00:58:08,173 - NCNet pretrain, Epoch [127 / 300]: loss 0.3202, training auc: 0.8293, val_auc 0.7747\n",
      "2022-06-30 00:58:08,310 - NCNet pretrain, Epoch [128 / 300]: loss 0.3186, training auc: 0.8369, val_auc 0.7714\n",
      "2022-06-30 00:58:08,427 - NCNet pretrain, Epoch [129 / 300]: loss 0.3235, training auc: 0.8227, val_auc 0.7696\n",
      "2022-06-30 00:58:08,545 - NCNet pretrain, Epoch [130 / 300]: loss 0.3180, training auc: 0.8305, val_auc 0.7745\n",
      "2022-06-30 00:58:08,682 - NCNet pretrain, Epoch [131 / 300]: loss 0.3160, training auc: 0.8342, val_auc 0.7743\n",
      "2022-06-30 00:58:08,799 - NCNet pretrain, Epoch [132 / 300]: loss 0.3160, training auc: 0.8363, val_auc 0.7734\n",
      "2022-06-30 00:58:08,939 - NCNet pretrain, Epoch [133 / 300]: loss 0.3122, training auc: 0.8346, val_auc 0.7733\n",
      "2022-06-30 00:58:09,078 - NCNet pretrain, Epoch [134 / 300]: loss 0.3176, training auc: 0.8323, val_auc 0.7743\n",
      "2022-06-30 00:58:09,224 - NCNet pretrain, Epoch [135 / 300]: loss 0.3167, training auc: 0.8293, val_auc 0.7752, test auc 0.7980\n",
      "2022-06-30 00:58:09,360 - NCNet pretrain, Epoch [136 / 300]: loss 0.3210, training auc: 0.8312, val_auc 0.7742\n",
      "2022-06-30 00:58:09,497 - NCNet pretrain, Epoch [137 / 300]: loss 0.3134, training auc: 0.8357, val_auc 0.7747\n",
      "2022-06-30 00:58:09,614 - NCNet pretrain, Epoch [138 / 300]: loss 0.3062, training auc: 0.8442, val_auc 0.7752\n",
      "2022-06-30 00:58:09,731 - NCNet pretrain, Epoch [139 / 300]: loss 0.3046, training auc: 0.8491, val_auc 0.7731\n",
      "2022-06-30 00:58:09,860 - NCNet pretrain, Epoch [140 / 300]: loss 0.3073, training auc: 0.8393, val_auc 0.7769, test auc 0.7998\n",
      "2022-06-30 00:58:09,977 - NCNet pretrain, Epoch [141 / 300]: loss 0.3172, training auc: 0.8328, val_auc 0.7754\n",
      "2022-06-30 00:58:10,116 - NCNet pretrain, Epoch [142 / 300]: loss 0.3038, training auc: 0.8482, val_auc 0.7735\n",
      "2022-06-30 00:58:10,234 - NCNet pretrain, Epoch [143 / 300]: loss 0.3101, training auc: 0.8388, val_auc 0.7767\n",
      "2022-06-30 00:58:10,354 - NCNet pretrain, Epoch [144 / 300]: loss 0.3033, training auc: 0.8465, val_auc 0.7745\n",
      "2022-06-30 00:58:10,490 - NCNet pretrain, Epoch [145 / 300]: loss 0.3000, training auc: 0.8505, val_auc 0.7748\n",
      "2022-06-30 00:58:10,628 - NCNet pretrain, Epoch [146 / 300]: loss 0.3038, training auc: 0.8485, val_auc 0.7759\n",
      "2022-06-30 00:58:10,747 - NCNet pretrain, Epoch [147 / 300]: loss 0.3079, training auc: 0.8406, val_auc 0.7751\n",
      "2022-06-30 00:58:10,866 - NCNet pretrain, Epoch [148 / 300]: loss 0.3084, training auc: 0.8357, val_auc 0.7704\n",
      "2022-06-30 00:58:10,984 - NCNet pretrain, Epoch [149 / 300]: loss 0.2992, training auc: 0.8520, val_auc 0.7752\n",
      "2022-06-30 00:58:11,101 - NCNet pretrain, Epoch [150 / 300]: loss 0.3001, training auc: 0.8513, val_auc 0.7756\n",
      "2022-06-30 00:58:11,227 - NCNet pretrain, Epoch [151 / 300]: loss 0.3035, training auc: 0.8536, val_auc 0.7694\n",
      "2022-06-30 00:58:11,344 - NCNet pretrain, Epoch [152 / 300]: loss 0.3116, training auc: 0.8404, val_auc 0.7754\n",
      "2022-06-30 00:58:11,461 - NCNet pretrain, Epoch [153 / 300]: loss 0.3081, training auc: 0.8507, val_auc 0.7719\n",
      "2022-06-30 00:58:11,578 - NCNet pretrain, Epoch [154 / 300]: loss 0.3042, training auc: 0.8427, val_auc 0.7705\n",
      "2022-06-30 00:58:11,695 - NCNet pretrain, Epoch [155 / 300]: loss 0.3070, training auc: 0.8410, val_auc 0.7751\n",
      "2022-06-30 00:58:11,813 - NCNet pretrain, Epoch [156 / 300]: loss 0.3039, training auc: 0.8507, val_auc 0.7739\n",
      "2022-06-30 00:58:11,929 - NCNet pretrain, Epoch [157 / 300]: loss 0.2979, training auc: 0.8576, val_auc 0.7713\n",
      "2022-06-30 00:58:12,047 - NCNet pretrain, Epoch [158 / 300]: loss 0.3044, training auc: 0.8461, val_auc 0.7737\n",
      "2022-06-30 00:58:12,176 - NCNet pretrain, Epoch [159 / 300]: loss 0.3084, training auc: 0.8425, val_auc 0.7726\n",
      "2022-06-30 00:58:12,294 - NCNet pretrain, Epoch [160 / 300]: loss 0.3027, training auc: 0.8492, val_auc 0.7728\n",
      "2022-06-30 00:58:12,425 - NCNet pretrain, Epoch [161 / 300]: loss 0.3030, training auc: 0.8473, val_auc 0.7726\n",
      "2022-06-30 00:58:12,562 - NCNet pretrain, Epoch [162 / 300]: loss 0.2974, training auc: 0.8572, val_auc 0.7735\n",
      "2022-06-30 00:58:12,699 - NCNet pretrain, Epoch [163 / 300]: loss 0.2961, training auc: 0.8597, val_auc 0.7714\n",
      "2022-06-30 00:58:12,821 - NCNet pretrain, Epoch [164 / 300]: loss 0.3050, training auc: 0.8480, val_auc 0.7729\n",
      "2022-06-30 00:58:12,941 - NCNet pretrain, Epoch [165 / 300]: loss 0.2966, training auc: 0.8600, val_auc 0.7715\n",
      "2022-06-30 00:58:13,059 - NCNet pretrain, Epoch [166 / 300]: loss 0.2969, training auc: 0.8580, val_auc 0.7731\n",
      "2022-06-30 00:58:13,177 - NCNet pretrain, Epoch [167 / 300]: loss 0.3007, training auc: 0.8584, val_auc 0.7697\n",
      "2022-06-30 00:58:13,302 - NCNet pretrain, Epoch [168 / 300]: loss 0.2980, training auc: 0.8506, val_auc 0.7718\n",
      "2022-06-30 00:58:13,438 - NCNet pretrain, Epoch [169 / 300]: loss 0.3005, training auc: 0.8506, val_auc 0.7721\n",
      "2022-06-30 00:58:13,574 - NCNet pretrain, Epoch [170 / 300]: loss 0.2960, training auc: 0.8618, val_auc 0.7679\n",
      "2022-06-30 00:58:13,711 - NCNet pretrain, Epoch [171 / 300]: loss 0.2916, training auc: 0.8629, val_auc 0.7721\n",
      "2022-06-30 00:58:13,838 - NCNet pretrain, Epoch [172 / 300]: loss 0.3025, training auc: 0.8604, val_auc 0.7671\n",
      "2022-06-30 00:58:13,955 - NCNet pretrain, Epoch [173 / 300]: loss 0.2985, training auc: 0.8562, val_auc 0.7713\n",
      "2022-06-30 00:58:14,071 - NCNet pretrain, Epoch [174 / 300]: loss 0.2968, training auc: 0.8647, val_auc 0.7702\n",
      "2022-06-30 00:58:14,207 - NCNet pretrain, Epoch [175 / 300]: loss 0.2871, training auc: 0.8682, val_auc 0.7679\n",
      "2022-06-30 00:58:14,342 - NCNet pretrain, Epoch [176 / 300]: loss 0.2943, training auc: 0.8571, val_auc 0.7714\n",
      "2022-06-30 00:58:14,461 - NCNet pretrain, Epoch [177 / 300]: loss 0.2996, training auc: 0.8684, val_auc 0.7626\n",
      "2022-06-30 00:58:14,577 - NCNet pretrain, Epoch [178 / 300]: loss 0.3134, training auc: 0.8482, val_auc 0.7692\n",
      "2022-06-30 00:58:14,694 - NCNet pretrain, Epoch [179 / 300]: loss 0.3083, training auc: 0.8729, val_auc 0.7644\n",
      "2022-06-30 00:58:14,811 - NCNet pretrain, Epoch [180 / 300]: loss 0.3014, training auc: 0.8469, val_auc 0.7659\n",
      "2022-06-30 00:58:14,943 - NCNet pretrain, Epoch [181 / 300]: loss 0.2977, training auc: 0.8625, val_auc 0.7658\n",
      "2022-06-30 00:58:15,068 - NCNet pretrain, Epoch [182 / 300]: loss 0.3454, training auc: 0.8743, val_auc 0.7501\n",
      "2022-06-30 00:58:15,201 - NCNet pretrain, Epoch [183 / 300]: loss 0.3710, training auc: 0.8359, val_auc 0.7645\n",
      "2022-06-30 00:58:15,339 - NCNet pretrain, Epoch [184 / 300]: loss 0.3012, training auc: 0.8550, val_auc 0.7622\n",
      "2022-06-30 00:58:15,461 - NCNet pretrain, Epoch [185 / 300]: loss 0.3605, training auc: 0.8646, val_auc 0.7657\n",
      "2022-06-30 00:58:15,577 - NCNet pretrain, Epoch [186 / 300]: loss 0.2947, training auc: 0.8567, val_auc 0.7551\n",
      "2022-06-30 00:58:15,694 - NCNet pretrain, Epoch [187 / 300]: loss 0.3298, training auc: 0.8515, val_auc 0.7693\n",
      "2022-06-30 00:58:15,830 - NCNet pretrain, Epoch [188 / 300]: loss 0.2957, training auc: 0.8565, val_auc 0.7680\n",
      "2022-06-30 00:58:15,947 - NCNet pretrain, Epoch [189 / 300]: loss 0.3410, training auc: 0.8649, val_auc 0.7713\n",
      "2022-06-30 00:58:16,085 - NCNet pretrain, Epoch [190 / 300]: loss 0.2935, training auc: 0.8621, val_auc 0.7595\n",
      "2022-06-30 00:58:16,086 - Early stop!\n",
      "2022-06-30 00:58:16,087 - Best Test Results: auc 0.7998, ap 0.4143, f1 0.2857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 240770.58it/s]\n",
      "2022-06-30 00:58:17,243 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:58:17,448 - NCNet pretrain, Epoch [1 / 300]: loss 0.8110, training auc: 0.5922, val_auc 0.4603, test auc 0.4327\n",
      "2022-06-30 00:58:17,588 - NCNet pretrain, Epoch [2 / 300]: loss 0.4925, training auc: 0.4512, val_auc 0.4589\n",
      "2022-06-30 00:58:17,723 - NCNet pretrain, Epoch [3 / 300]: loss 0.5111, training auc: 0.3994, val_auc 0.4585\n",
      "2022-06-30 00:58:17,843 - NCNet pretrain, Epoch [4 / 300]: loss 0.5132, training auc: 0.4288, val_auc 0.4580\n",
      "2022-06-30 00:58:17,960 - NCNet pretrain, Epoch [5 / 300]: loss 0.5011, training auc: 0.4247, val_auc 0.4600\n",
      "2022-06-30 00:58:18,089 - NCNet pretrain, Epoch [6 / 300]: loss 0.4787, training auc: 0.4312, val_auc 0.4613, test auc 0.4372\n",
      "2022-06-30 00:58:18,236 - NCNet pretrain, Epoch [7 / 300]: loss 0.4766, training auc: 0.4084, val_auc 0.4650, test auc 0.4424\n",
      "2022-06-30 00:58:18,363 - NCNet pretrain, Epoch [8 / 300]: loss 0.4795, training auc: 0.4166, val_auc 0.4684, test auc 0.4474\n",
      "2022-06-30 00:58:18,492 - NCNet pretrain, Epoch [9 / 300]: loss 0.4877, training auc: 0.4307, val_auc 0.4704, test auc 0.4496\n",
      "2022-06-30 00:58:18,619 - NCNet pretrain, Epoch [10 / 300]: loss 0.4755, training auc: 0.4860, val_auc 0.4716, test auc 0.4499\n",
      "2022-06-30 00:58:18,742 - NCNet pretrain, Epoch [11 / 300]: loss 0.4754, training auc: 0.4614, val_auc 0.4724, test auc 0.4493\n",
      "2022-06-30 00:58:18,871 - NCNet pretrain, Epoch [12 / 300]: loss 0.4739, training auc: 0.4272, val_auc 0.4735, test auc 0.4497\n",
      "2022-06-30 00:58:18,999 - NCNet pretrain, Epoch [13 / 300]: loss 0.4727, training auc: 0.4274, val_auc 0.4739, test auc 0.4502\n",
      "2022-06-30 00:58:19,127 - NCNet pretrain, Epoch [14 / 300]: loss 0.4740, training auc: 0.4342, val_auc 0.4745, test auc 0.4509\n",
      "2022-06-30 00:58:19,257 - NCNet pretrain, Epoch [15 / 300]: loss 0.4735, training auc: 0.4320, val_auc 0.4751, test auc 0.4518\n",
      "2022-06-30 00:58:19,385 - NCNet pretrain, Epoch [16 / 300]: loss 0.4644, training auc: 0.4538, val_auc 0.4759, test auc 0.4529\n",
      "2022-06-30 00:58:19,514 - NCNet pretrain, Epoch [17 / 300]: loss 0.4698, training auc: 0.4291, val_auc 0.4770, test auc 0.4544\n",
      "2022-06-30 00:58:19,649 - NCNet pretrain, Epoch [18 / 300]: loss 0.4620, training auc: 0.4698, val_auc 0.4780, test auc 0.4555\n",
      "2022-06-30 00:58:19,776 - NCNet pretrain, Epoch [19 / 300]: loss 0.4667, training auc: 0.4365, val_auc 0.4788, test auc 0.4564\n",
      "2022-06-30 00:58:19,905 - NCNet pretrain, Epoch [20 / 300]: loss 0.4672, training auc: 0.4226, val_auc 0.4801, test auc 0.4573\n",
      "2022-06-30 00:58:20,051 - NCNet pretrain, Epoch [21 / 300]: loss 0.4628, training auc: 0.4506, val_auc 0.4813, test auc 0.4582\n",
      "2022-06-30 00:58:20,179 - NCNet pretrain, Epoch [22 / 300]: loss 0.4574, training auc: 0.4695, val_auc 0.4827, test auc 0.4588\n",
      "2022-06-30 00:58:20,306 - NCNet pretrain, Epoch [23 / 300]: loss 0.4586, training auc: 0.4531, val_auc 0.4841, test auc 0.4596\n",
      "2022-06-30 00:58:20,438 - NCNet pretrain, Epoch [24 / 300]: loss 0.4692, training auc: 0.4101, val_auc 0.4858, test auc 0.4610\n",
      "2022-06-30 00:58:20,566 - NCNet pretrain, Epoch [25 / 300]: loss 0.4537, training auc: 0.4698, val_auc 0.4878, test auc 0.4629\n",
      "2022-06-30 00:58:20,693 - NCNet pretrain, Epoch [26 / 300]: loss 0.4615, training auc: 0.4358, val_auc 0.4901, test auc 0.4654\n",
      "2022-06-30 00:58:20,822 - NCNet pretrain, Epoch [27 / 300]: loss 0.4469, training auc: 0.4998, val_auc 0.4926, test auc 0.4677\n",
      "2022-06-30 00:58:20,950 - NCNet pretrain, Epoch [28 / 300]: loss 0.4539, training auc: 0.4528, val_auc 0.4954, test auc 0.4704\n",
      "2022-06-30 00:58:21,078 - NCNet pretrain, Epoch [29 / 300]: loss 0.4579, training auc: 0.4350, val_auc 0.4989, test auc 0.4739\n",
      "2022-06-30 00:58:21,207 - NCNet pretrain, Epoch [30 / 300]: loss 0.4496, training auc: 0.4909, val_auc 0.5026, test auc 0.4777\n",
      "2022-06-30 00:58:21,354 - NCNet pretrain, Epoch [31 / 300]: loss 0.4474, training auc: 0.4857, val_auc 0.5068, test auc 0.4816\n",
      "2022-06-30 00:58:21,484 - NCNet pretrain, Epoch [32 / 300]: loss 0.4493, training auc: 0.4674, val_auc 0.5113, test auc 0.4858\n",
      "2022-06-30 00:58:21,631 - NCNet pretrain, Epoch [33 / 300]: loss 0.4420, training auc: 0.5002, val_auc 0.5158, test auc 0.4899\n",
      "2022-06-30 00:58:21,781 - NCNet pretrain, Epoch [34 / 300]: loss 0.4363, training auc: 0.5305, val_auc 0.5201, test auc 0.4939\n",
      "2022-06-30 00:58:21,917 - NCNet pretrain, Epoch [35 / 300]: loss 0.4420, training auc: 0.4983, val_auc 0.5258, test auc 0.4997\n",
      "2022-06-30 00:58:22,044 - NCNet pretrain, Epoch [36 / 300]: loss 0.4337, training auc: 0.5314, val_auc 0.5325, test auc 0.5067\n",
      "2022-06-30 00:58:22,171 - NCNet pretrain, Epoch [37 / 300]: loss 0.4367, training auc: 0.5099, val_auc 0.5418, test auc 0.5161\n",
      "2022-06-30 00:58:22,303 - NCNet pretrain, Epoch [38 / 300]: loss 0.4404, training auc: 0.4982, val_auc 0.5524, test auc 0.5279\n",
      "2022-06-30 00:58:22,431 - NCNet pretrain, Epoch [39 / 300]: loss 0.4375, training auc: 0.5106, val_auc 0.5637, test auc 0.5400\n",
      "2022-06-30 00:58:22,562 - NCNet pretrain, Epoch [40 / 300]: loss 0.4233, training auc: 0.5925, val_auc 0.5710, test auc 0.5468\n",
      "2022-06-30 00:58:22,689 - NCNet pretrain, Epoch [41 / 300]: loss 0.4298, training auc: 0.5487, val_auc 0.5794, test auc 0.5550\n",
      "2022-06-30 00:58:22,817 - NCNet pretrain, Epoch [42 / 300]: loss 0.4223, training auc: 0.5671, val_auc 0.5935, test auc 0.5700\n",
      "2022-06-30 00:58:22,944 - NCNet pretrain, Epoch [43 / 300]: loss 0.4241, training auc: 0.5622, val_auc 0.6184, test auc 0.5948\n",
      "2022-06-30 00:58:23,073 - NCNet pretrain, Epoch [44 / 300]: loss 0.4208, training auc: 0.5918, val_auc 0.6406, test auc 0.6173\n",
      "2022-06-30 00:58:23,199 - NCNet pretrain, Epoch [45 / 300]: loss 0.4161, training auc: 0.6100, val_auc 0.6419, test auc 0.6166\n",
      "2022-06-30 00:58:23,337 - NCNet pretrain, Epoch [46 / 300]: loss 0.4049, training auc: 0.6333, val_auc 0.6414\n",
      "2022-06-30 00:58:23,487 - NCNet pretrain, Epoch [47 / 300]: loss 0.4069, training auc: 0.6287, val_auc 0.6645, test auc 0.6379\n",
      "2022-06-30 00:58:23,638 - NCNet pretrain, Epoch [48 / 300]: loss 0.4094, training auc: 0.6188, val_auc 0.6930, test auc 0.6688\n",
      "2022-06-30 00:58:23,758 - NCNet pretrain, Epoch [49 / 300]: loss 0.3946, training auc: 0.7056, val_auc 0.6888\n",
      "2022-06-30 00:58:23,879 - NCNet pretrain, Epoch [50 / 300]: loss 0.3982, training auc: 0.6621, val_auc 0.6826\n",
      "2022-06-30 00:58:24,007 - NCNet pretrain, Epoch [51 / 300]: loss 0.3995, training auc: 0.6598, val_auc 0.6933, test auc 0.6679\n",
      "2022-06-30 00:58:24,137 - NCNet pretrain, Epoch [52 / 300]: loss 0.3915, training auc: 0.6790, val_auc 0.7155, test auc 0.6961\n",
      "2022-06-30 00:58:24,267 - NCNet pretrain, Epoch [53 / 300]: loss 0.3993, training auc: 0.6739, val_auc 0.7249, test auc 0.7105\n",
      "2022-06-30 00:58:24,385 - NCNet pretrain, Epoch [54 / 300]: loss 0.3922, training auc: 0.7106, val_auc 0.7152\n",
      "2022-06-30 00:58:24,513 - NCNet pretrain, Epoch [55 / 300]: loss 0.3869, training auc: 0.6957, val_auc 0.7106\n",
      "2022-06-30 00:58:24,642 - NCNet pretrain, Epoch [56 / 300]: loss 0.3915, training auc: 0.6796, val_auc 0.7262, test auc 0.7121\n",
      "2022-06-30 00:58:24,773 - NCNet pretrain, Epoch [57 / 300]: loss 0.3793, training auc: 0.7083, val_auc 0.7394, test auc 0.7317\n",
      "2022-06-30 00:58:24,895 - NCNet pretrain, Epoch [58 / 300]: loss 0.3858, training auc: 0.7264, val_auc 0.7346\n",
      "2022-06-30 00:58:25,018 - NCNet pretrain, Epoch [59 / 300]: loss 0.3744, training auc: 0.7306, val_auc 0.7245\n",
      "2022-06-30 00:58:25,138 - NCNet pretrain, Epoch [60 / 300]: loss 0.3917, training auc: 0.6981, val_auc 0.7354\n",
      "2022-06-30 00:58:25,297 - NCNet pretrain, Epoch [61 / 300]: loss 0.3679, training auc: 0.7402, val_auc 0.7466, test auc 0.7424\n",
      "2022-06-30 00:58:25,450 - NCNet pretrain, Epoch [62 / 300]: loss 0.3739, training auc: 0.7575, val_auc 0.7470, test auc 0.7422\n",
      "2022-06-30 00:58:25,595 - NCNet pretrain, Epoch [63 / 300]: loss 0.3783, training auc: 0.7454, val_auc 0.7387\n",
      "2022-06-30 00:58:25,739 - NCNet pretrain, Epoch [64 / 300]: loss 0.3777, training auc: 0.7211, val_auc 0.7400\n",
      "2022-06-30 00:58:25,887 - NCNet pretrain, Epoch [65 / 300]: loss 0.3741, training auc: 0.7316, val_auc 0.7515, test auc 0.7488\n",
      "2022-06-30 00:58:26,040 - NCNet pretrain, Epoch [66 / 300]: loss 0.3757, training auc: 0.7393, val_auc 0.7553, test auc 0.7555\n",
      "2022-06-30 00:58:26,183 - NCNet pretrain, Epoch [67 / 300]: loss 0.3718, training auc: 0.7478, val_auc 0.7517\n",
      "2022-06-30 00:58:26,321 - NCNet pretrain, Epoch [68 / 300]: loss 0.3614, training auc: 0.7508, val_auc 0.7470\n",
      "2022-06-30 00:58:26,466 - NCNet pretrain, Epoch [69 / 300]: loss 0.3642, training auc: 0.7445, val_auc 0.7525\n",
      "2022-06-30 00:58:26,608 - NCNet pretrain, Epoch [70 / 300]: loss 0.3566, training auc: 0.7630, val_auc 0.7603, test auc 0.7630\n",
      "2022-06-30 00:58:26,748 - NCNet pretrain, Epoch [71 / 300]: loss 0.3613, training auc: 0.7712, val_auc 0.7615, test auc 0.7647\n",
      "2022-06-30 00:58:26,879 - NCNet pretrain, Epoch [72 / 300]: loss 0.3577, training auc: 0.7688, val_auc 0.7573\n",
      "2022-06-30 00:58:27,004 - NCNet pretrain, Epoch [73 / 300]: loss 0.3575, training auc: 0.7605, val_auc 0.7561\n",
      "2022-06-30 00:58:27,148 - NCNet pretrain, Epoch [74 / 300]: loss 0.3575, training auc: 0.7608, val_auc 0.7642, test auc 0.7682\n",
      "2022-06-30 00:58:27,298 - NCNet pretrain, Epoch [75 / 300]: loss 0.3487, training auc: 0.7887, val_auc 0.7679, test auc 0.7740\n",
      "2022-06-30 00:58:27,437 - NCNet pretrain, Epoch [76 / 300]: loss 0.3492, training auc: 0.7917, val_auc 0.7639\n",
      "2022-06-30 00:58:27,577 - NCNet pretrain, Epoch [77 / 300]: loss 0.3518, training auc: 0.7775, val_auc 0.7618\n",
      "2022-06-30 00:58:27,716 - NCNet pretrain, Epoch [78 / 300]: loss 0.3444, training auc: 0.7847, val_auc 0.7672\n",
      "2022-06-30 00:58:27,868 - NCNet pretrain, Epoch [79 / 300]: loss 0.3459, training auc: 0.7856, val_auc 0.7707, test auc 0.7768\n",
      "2022-06-30 00:58:28,014 - NCNet pretrain, Epoch [80 / 300]: loss 0.3536, training auc: 0.7745, val_auc 0.7692\n",
      "2022-06-30 00:58:28,183 - NCNet pretrain, Epoch [81 / 300]: loss 0.3481, training auc: 0.7789, val_auc 0.7663\n",
      "2022-06-30 00:58:28,326 - NCNet pretrain, Epoch [82 / 300]: loss 0.3350, training auc: 0.7946, val_auc 0.7700\n",
      "2022-06-30 00:58:28,472 - NCNet pretrain, Epoch [83 / 300]: loss 0.3466, training auc: 0.7833, val_auc 0.7731, test auc 0.7797\n",
      "2022-06-30 00:58:28,590 - NCNet pretrain, Epoch [84 / 300]: loss 0.3380, training auc: 0.8011, val_auc 0.7715\n",
      "2022-06-30 00:58:28,708 - NCNet pretrain, Epoch [85 / 300]: loss 0.3424, training auc: 0.7974, val_auc 0.7717\n",
      "2022-06-30 00:58:28,827 - NCNet pretrain, Epoch [86 / 300]: loss 0.3313, training auc: 0.8019, val_auc 0.7717\n",
      "2022-06-30 00:58:28,977 - NCNet pretrain, Epoch [87 / 300]: loss 0.3369, training auc: 0.8033, val_auc 0.7751, test auc 0.7839\n",
      "2022-06-30 00:58:29,095 - NCNet pretrain, Epoch [88 / 300]: loss 0.3351, training auc: 0.8013, val_auc 0.7720\n",
      "2022-06-30 00:58:29,214 - NCNet pretrain, Epoch [89 / 300]: loss 0.3282, training auc: 0.8113, val_auc 0.7714\n",
      "2022-06-30 00:58:29,344 - NCNet pretrain, Epoch [90 / 300]: loss 0.3396, training auc: 0.7985, val_auc 0.7754, test auc 0.7867\n",
      "2022-06-30 00:58:29,463 - NCNet pretrain, Epoch [91 / 300]: loss 0.3389, training auc: 0.8010, val_auc 0.7747\n",
      "2022-06-30 00:58:29,584 - NCNet pretrain, Epoch [92 / 300]: loss 0.3197, training auc: 0.8257, val_auc 0.7700\n",
      "2022-06-30 00:58:29,714 - NCNet pretrain, Epoch [93 / 300]: loss 0.3388, training auc: 0.8045, val_auc 0.7770, test auc 0.7922\n",
      "2022-06-30 00:58:29,836 - NCNet pretrain, Epoch [94 / 300]: loss 0.3256, training auc: 0.8232, val_auc 0.7745\n",
      "2022-06-30 00:58:29,959 - NCNet pretrain, Epoch [95 / 300]: loss 0.3317, training auc: 0.8073, val_auc 0.7694\n",
      "2022-06-30 00:58:30,094 - NCNet pretrain, Epoch [96 / 300]: loss 0.3360, training auc: 0.8051, val_auc 0.7778, test auc 0.7965\n",
      "2022-06-30 00:58:30,217 - NCNet pretrain, Epoch [97 / 300]: loss 0.3318, training auc: 0.8225, val_auc 0.7748\n",
      "2022-06-30 00:58:30,341 - NCNet pretrain, Epoch [98 / 300]: loss 0.3270, training auc: 0.8169, val_auc 0.7656\n",
      "2022-06-30 00:58:30,467 - NCNet pretrain, Epoch [99 / 300]: loss 0.3559, training auc: 0.7847, val_auc 0.7777\n",
      "2022-06-30 00:58:30,592 - NCNet pretrain, Epoch [100 / 300]: loss 0.3286, training auc: 0.8184, val_auc 0.7774\n",
      "2022-06-30 00:58:30,728 - NCNet pretrain, Epoch [101 / 300]: loss 0.3323, training auc: 0.8263, val_auc 0.7705\n",
      "2022-06-30 00:58:30,858 - NCNet pretrain, Epoch [102 / 300]: loss 0.3279, training auc: 0.8175, val_auc 0.7720\n",
      "2022-06-30 00:58:31,010 - NCNet pretrain, Epoch [103 / 300]: loss 0.3319, training auc: 0.8113, val_auc 0.7782, test auc 0.7974\n",
      "2022-06-30 00:58:31,130 - NCNet pretrain, Epoch [104 / 300]: loss 0.3262, training auc: 0.8363, val_auc 0.7769\n",
      "2022-06-30 00:58:31,250 - NCNet pretrain, Epoch [105 / 300]: loss 0.3186, training auc: 0.8308, val_auc 0.7720\n",
      "2022-06-30 00:58:31,370 - NCNet pretrain, Epoch [106 / 300]: loss 0.3227, training auc: 0.8141, val_auc 0.7742\n",
      "2022-06-30 00:58:31,503 - NCNet pretrain, Epoch [107 / 300]: loss 0.3183, training auc: 0.8203, val_auc 0.7793, test auc 0.7970\n",
      "2022-06-30 00:58:31,623 - NCNet pretrain, Epoch [108 / 300]: loss 0.3321, training auc: 0.8166, val_auc 0.7775\n",
      "2022-06-30 00:58:31,757 - NCNet pretrain, Epoch [109 / 300]: loss 0.3100, training auc: 0.8438, val_auc 0.7749\n",
      "2022-06-30 00:58:31,877 - NCNet pretrain, Epoch [110 / 300]: loss 0.3158, training auc: 0.8281, val_auc 0.7781\n",
      "2022-06-30 00:58:32,009 - NCNet pretrain, Epoch [111 / 300]: loss 0.3174, training auc: 0.8300, val_auc 0.7793, test auc 0.7974\n",
      "2022-06-30 00:58:32,128 - NCNet pretrain, Epoch [112 / 300]: loss 0.3149, training auc: 0.8335, val_auc 0.7788\n",
      "2022-06-30 00:58:32,247 - NCNet pretrain, Epoch [113 / 300]: loss 0.3230, training auc: 0.8174, val_auc 0.7751\n",
      "2022-06-30 00:58:32,376 - NCNet pretrain, Epoch [114 / 300]: loss 0.3291, training auc: 0.8178, val_auc 0.7819, test auc 0.8032\n",
      "2022-06-30 00:58:32,498 - NCNet pretrain, Epoch [115 / 300]: loss 0.3200, training auc: 0.8433, val_auc 0.7807\n",
      "2022-06-30 00:58:32,624 - NCNet pretrain, Epoch [116 / 300]: loss 0.3241, training auc: 0.8243, val_auc 0.7739\n",
      "2022-06-30 00:58:32,741 - NCNet pretrain, Epoch [117 / 300]: loss 0.3188, training auc: 0.8247, val_auc 0.7796\n",
      "2022-06-30 00:58:32,859 - NCNet pretrain, Epoch [118 / 300]: loss 0.3118, training auc: 0.8344, val_auc 0.7818\n",
      "2022-06-30 00:58:32,997 - NCNet pretrain, Epoch [119 / 300]: loss 0.3240, training auc: 0.8382, val_auc 0.7778\n",
      "2022-06-30 00:58:33,134 - NCNet pretrain, Epoch [120 / 300]: loss 0.3197, training auc: 0.8292, val_auc 0.7758\n",
      "2022-06-30 00:58:33,269 - NCNet pretrain, Epoch [121 / 300]: loss 0.3222, training auc: 0.8244, val_auc 0.7814\n",
      "2022-06-30 00:58:33,490 - NCNet pretrain, Epoch [122 / 300]: loss 0.3107, training auc: 0.8393, val_auc 0.7804\n",
      "2022-06-30 00:58:33,609 - NCNet pretrain, Epoch [123 / 300]: loss 0.3111, training auc: 0.8398, val_auc 0.7760\n",
      "2022-06-30 00:58:33,727 - NCNet pretrain, Epoch [124 / 300]: loss 0.3250, training auc: 0.8189, val_auc 0.7798\n",
      "2022-06-30 00:58:33,845 - NCNet pretrain, Epoch [125 / 300]: loss 0.3187, training auc: 0.8275, val_auc 0.7816\n",
      "2022-06-30 00:58:33,964 - NCNet pretrain, Epoch [126 / 300]: loss 0.3187, training auc: 0.8343, val_auc 0.7789\n",
      "2022-06-30 00:58:34,099 - NCNet pretrain, Epoch [127 / 300]: loss 0.3152, training auc: 0.8326, val_auc 0.7755\n",
      "2022-06-30 00:58:34,232 - NCNet pretrain, Epoch [128 / 300]: loss 0.3108, training auc: 0.8350, val_auc 0.7791\n",
      "2022-06-30 00:58:34,352 - NCNet pretrain, Epoch [129 / 300]: loss 0.3069, training auc: 0.8414, val_auc 0.7805\n",
      "2022-06-30 00:58:34,480 - NCNet pretrain, Epoch [130 / 300]: loss 0.3065, training auc: 0.8500, val_auc 0.7761\n",
      "2022-06-30 00:58:34,601 - NCNet pretrain, Epoch [131 / 300]: loss 0.3044, training auc: 0.8446, val_auc 0.7761\n",
      "2022-06-30 00:58:34,723 - NCNet pretrain, Epoch [132 / 300]: loss 0.3123, training auc: 0.8424, val_auc 0.7798\n",
      "2022-06-30 00:58:34,860 - NCNet pretrain, Epoch [133 / 300]: loss 0.3208, training auc: 0.8379, val_auc 0.7769\n",
      "2022-06-30 00:58:34,978 - NCNet pretrain, Epoch [134 / 300]: loss 0.3110, training auc: 0.8384, val_auc 0.7742\n",
      "2022-06-30 00:58:35,095 - NCNet pretrain, Epoch [135 / 300]: loss 0.3150, training auc: 0.8308, val_auc 0.7786\n",
      "2022-06-30 00:58:35,212 - NCNet pretrain, Epoch [136 / 300]: loss 0.2964, training auc: 0.8604, val_auc 0.7787\n",
      "2022-06-30 00:58:35,336 - NCNet pretrain, Epoch [137 / 300]: loss 0.3104, training auc: 0.8400, val_auc 0.7741\n",
      "2022-06-30 00:58:35,453 - NCNet pretrain, Epoch [138 / 300]: loss 0.3204, training auc: 0.8188, val_auc 0.7772\n",
      "2022-06-30 00:58:35,571 - NCNet pretrain, Epoch [139 / 300]: loss 0.3106, training auc: 0.8459, val_auc 0.7783\n",
      "2022-06-30 00:58:35,690 - NCNet pretrain, Epoch [140 / 300]: loss 0.3025, training auc: 0.8505, val_auc 0.7766\n",
      "2022-06-30 00:58:35,826 - NCNet pretrain, Epoch [141 / 300]: loss 0.2969, training auc: 0.8604, val_auc 0.7772\n",
      "2022-06-30 00:58:35,943 - NCNet pretrain, Epoch [142 / 300]: loss 0.2908, training auc: 0.8673, val_auc 0.7783\n",
      "2022-06-30 00:58:36,061 - NCNet pretrain, Epoch [143 / 300]: loss 0.3036, training auc: 0.8489, val_auc 0.7757\n",
      "2022-06-30 00:58:36,179 - NCNet pretrain, Epoch [144 / 300]: loss 0.3098, training auc: 0.8375, val_auc 0.7765\n",
      "2022-06-30 00:58:36,298 - NCNet pretrain, Epoch [145 / 300]: loss 0.3129, training auc: 0.8317, val_auc 0.7790\n",
      "2022-06-30 00:58:36,420 - NCNet pretrain, Epoch [146 / 300]: loss 0.3060, training auc: 0.8453, val_auc 0.7794\n",
      "2022-06-30 00:58:36,539 - NCNet pretrain, Epoch [147 / 300]: loss 0.3062, training auc: 0.8541, val_auc 0.7760\n",
      "2022-06-30 00:58:36,661 - NCNet pretrain, Epoch [148 / 300]: loss 0.3018, training auc: 0.8526, val_auc 0.7778\n",
      "2022-06-30 00:58:36,787 - NCNet pretrain, Epoch [149 / 300]: loss 0.2996, training auc: 0.8533, val_auc 0.7804\n",
      "2022-06-30 00:58:36,905 - NCNet pretrain, Epoch [150 / 300]: loss 0.3025, training auc: 0.8527, val_auc 0.7778\n",
      "2022-06-30 00:58:37,023 - NCNet pretrain, Epoch [151 / 300]: loss 0.2989, training auc: 0.8541, val_auc 0.7796\n",
      "2022-06-30 00:58:37,141 - NCNet pretrain, Epoch [152 / 300]: loss 0.2986, training auc: 0.8529, val_auc 0.7767\n",
      "2022-06-30 00:58:37,258 - NCNet pretrain, Epoch [153 / 300]: loss 0.2945, training auc: 0.8595, val_auc 0.7771\n",
      "2022-06-30 00:58:37,395 - NCNet pretrain, Epoch [154 / 300]: loss 0.2971, training auc: 0.8564, val_auc 0.7795\n",
      "2022-06-30 00:58:37,512 - NCNet pretrain, Epoch [155 / 300]: loss 0.3171, training auc: 0.8523, val_auc 0.7649\n",
      "2022-06-30 00:58:37,630 - NCNet pretrain, Epoch [156 / 300]: loss 0.3175, training auc: 0.8427, val_auc 0.7785\n",
      "2022-06-30 00:58:37,747 - NCNet pretrain, Epoch [157 / 300]: loss 0.3002, training auc: 0.8598, val_auc 0.7786\n",
      "2022-06-30 00:58:37,865 - NCNet pretrain, Epoch [158 / 300]: loss 0.3014, training auc: 0.8634, val_auc 0.7707\n",
      "2022-06-30 00:58:37,983 - NCNet pretrain, Epoch [159 / 300]: loss 0.2998, training auc: 0.8507, val_auc 0.7750\n",
      "2022-06-30 00:58:38,103 - NCNet pretrain, Epoch [160 / 300]: loss 0.2933, training auc: 0.8561, val_auc 0.7763\n",
      "2022-06-30 00:58:38,222 - NCNet pretrain, Epoch [161 / 300]: loss 0.3093, training auc: 0.8539, val_auc 0.7673\n",
      "2022-06-30 00:58:38,343 - NCNet pretrain, Epoch [162 / 300]: loss 0.3225, training auc: 0.8420, val_auc 0.7741\n",
      "2022-06-30 00:58:38,464 - NCNet pretrain, Epoch [163 / 300]: loss 0.2927, training auc: 0.8601, val_auc 0.7745\n",
      "2022-06-30 00:58:38,583 - NCNet pretrain, Epoch [164 / 300]: loss 0.3052, training auc: 0.8695, val_auc 0.7626\n",
      "2022-06-30 00:58:38,584 - Early stop!\n",
      "2022-06-30 00:58:38,586 - Best Test Results: auc 0.8032, ap 0.4132, f1 0.2749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 333650.56it/s]\n",
      "2022-06-30 00:58:39,556 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:58:39,735 - NCNet pretrain, Epoch [1 / 300]: loss 0.4274, training auc: 0.5366, val_auc 0.4643, test auc 0.4350\n",
      "2022-06-30 00:58:39,863 - NCNet pretrain, Epoch [2 / 300]: loss 0.4499, training auc: 0.4310, val_auc 0.4689, test auc 0.4407\n",
      "2022-06-30 00:58:39,992 - NCNet pretrain, Epoch [3 / 300]: loss 0.4253, training auc: 0.4573, val_auc 0.4788, test auc 0.4510\n",
      "2022-06-30 00:58:40,141 - NCNet pretrain, Epoch [4 / 300]: loss 0.4192, training auc: 0.4958, val_auc 0.5365, test auc 0.5039\n",
      "2022-06-30 00:58:40,289 - NCNet pretrain, Epoch [5 / 300]: loss 0.4249, training auc: 0.4176, val_auc 0.5852, test auc 0.5463\n",
      "2022-06-30 00:58:40,412 - NCNet pretrain, Epoch [6 / 300]: loss 0.4247, training auc: 0.4687, val_auc 0.5276\n",
      "2022-06-30 00:58:40,531 - NCNet pretrain, Epoch [7 / 300]: loss 0.4193, training auc: 0.5565, val_auc 0.5061\n",
      "2022-06-30 00:58:40,670 - NCNet pretrain, Epoch [8 / 300]: loss 0.4205, training auc: 0.4339, val_auc 0.5030\n",
      "2022-06-30 00:58:40,789 - NCNet pretrain, Epoch [9 / 300]: loss 0.4183, training auc: 0.4583, val_auc 0.5043\n",
      "2022-06-30 00:58:40,907 - NCNet pretrain, Epoch [10 / 300]: loss 0.4171, training auc: 0.4915, val_auc 0.5095\n",
      "2022-06-30 00:58:41,030 - NCNet pretrain, Epoch [11 / 300]: loss 0.4204, training auc: 0.4271, val_auc 0.5187\n",
      "2022-06-30 00:58:41,151 - NCNet pretrain, Epoch [12 / 300]: loss 0.4170, training auc: 0.4937, val_auc 0.5379\n",
      "2022-06-30 00:58:41,269 - NCNet pretrain, Epoch [13 / 300]: loss 0.4151, training auc: 0.5251, val_auc 0.5799\n",
      "2022-06-30 00:58:41,397 - NCNet pretrain, Epoch [14 / 300]: loss 0.4162, training auc: 0.5116, val_auc 0.6237, test auc 0.5906\n",
      "2022-06-30 00:58:41,525 - NCNet pretrain, Epoch [15 / 300]: loss 0.4151, training auc: 0.5320, val_auc 0.6360, test auc 0.6010\n",
      "2022-06-30 00:58:41,642 - NCNet pretrain, Epoch [16 / 300]: loss 0.4136, training auc: 0.5710, val_auc 0.6219\n",
      "2022-06-30 00:58:41,762 - NCNet pretrain, Epoch [17 / 300]: loss 0.4120, training auc: 0.5806, val_auc 0.6203\n",
      "2022-06-30 00:58:41,892 - NCNet pretrain, Epoch [18 / 300]: loss 0.4145, training auc: 0.5171, val_auc 0.6443, test auc 0.6058\n",
      "2022-06-30 00:58:42,020 - NCNet pretrain, Epoch [19 / 300]: loss 0.4091, training auc: 0.6332, val_auc 0.6731, test auc 0.6342\n",
      "2022-06-30 00:58:42,170 - NCNet pretrain, Epoch [20 / 300]: loss 0.4111, training auc: 0.5653, val_auc 0.7024, test auc 0.6620\n",
      "2022-06-30 00:58:42,304 - NCNet pretrain, Epoch [21 / 300]: loss 0.4079, training auc: 0.6487, val_auc 0.7178, test auc 0.6771\n",
      "2022-06-30 00:58:42,455 - NCNet pretrain, Epoch [22 / 300]: loss 0.4071, training auc: 0.6318, val_auc 0.7263, test auc 0.6871\n",
      "2022-06-30 00:58:42,583 - NCNet pretrain, Epoch [23 / 300]: loss 0.4044, training auc: 0.6723, val_auc 0.7342, test auc 0.6954\n",
      "2022-06-30 00:58:42,712 - NCNet pretrain, Epoch [24 / 300]: loss 0.4021, training auc: 0.6862, val_auc 0.7476, test auc 0.7143\n",
      "2022-06-30 00:58:42,846 - NCNet pretrain, Epoch [25 / 300]: loss 0.4019, training auc: 0.6598, val_auc 0.7553, test auc 0.7262\n",
      "2022-06-30 00:58:42,975 - NCNet pretrain, Epoch [26 / 300]: loss 0.3984, training auc: 0.7077, val_auc 0.7588, test auc 0.7280\n",
      "2022-06-30 00:58:43,104 - NCNet pretrain, Epoch [27 / 300]: loss 0.3960, training auc: 0.7042, val_auc 0.7673, test auc 0.7400\n",
      "2022-06-30 00:58:43,238 - NCNet pretrain, Epoch [28 / 300]: loss 0.3942, training auc: 0.7012, val_auc 0.7703, test auc 0.7429\n",
      "2022-06-30 00:58:43,367 - NCNet pretrain, Epoch [29 / 300]: loss 0.3888, training auc: 0.7261, val_auc 0.7779, test auc 0.7578\n",
      "2022-06-30 00:58:43,495 - NCNet pretrain, Epoch [30 / 300]: loss 0.3853, training auc: 0.7542, val_auc 0.7811, test auc 0.7644\n",
      "2022-06-30 00:58:43,613 - NCNet pretrain, Epoch [31 / 300]: loss 0.3867, training auc: 0.7447, val_auc 0.7745\n",
      "2022-06-30 00:58:43,744 - NCNet pretrain, Epoch [32 / 300]: loss 0.3797, training auc: 0.7392, val_auc 0.7860, test auc 0.7750\n",
      "2022-06-30 00:58:43,873 - NCNet pretrain, Epoch [33 / 300]: loss 0.3777, training auc: 0.7517, val_auc 0.7874, test auc 0.7795\n",
      "2022-06-30 00:58:43,990 - NCNet pretrain, Epoch [34 / 300]: loss 0.3722, training auc: 0.7727, val_auc 0.7650\n",
      "2022-06-30 00:58:44,108 - NCNet pretrain, Epoch [35 / 300]: loss 0.3714, training auc: 0.7453, val_auc 0.7818\n",
      "2022-06-30 00:58:44,226 - NCNet pretrain, Epoch [36 / 300]: loss 0.3704, training auc: 0.7802, val_auc 0.7682\n",
      "2022-06-30 00:58:44,344 - NCNet pretrain, Epoch [37 / 300]: loss 0.3609, training auc: 0.7755, val_auc 0.7709\n",
      "2022-06-30 00:58:44,463 - NCNet pretrain, Epoch [38 / 300]: loss 0.3657, training auc: 0.7646, val_auc 0.7674\n",
      "2022-06-30 00:58:44,581 - NCNet pretrain, Epoch [39 / 300]: loss 0.3574, training auc: 0.7768, val_auc 0.7713\n",
      "2022-06-30 00:58:44,700 - NCNet pretrain, Epoch [40 / 300]: loss 0.3592, training auc: 0.7807, val_auc 0.7675\n",
      "2022-06-30 00:58:44,841 - NCNet pretrain, Epoch [41 / 300]: loss 0.3582, training auc: 0.7651, val_auc 0.7735\n",
      "2022-06-30 00:58:44,967 - NCNet pretrain, Epoch [42 / 300]: loss 0.3447, training auc: 0.7943, val_auc 0.7701\n",
      "2022-06-30 00:58:45,104 - NCNet pretrain, Epoch [43 / 300]: loss 0.3517, training auc: 0.7781, val_auc 0.7780\n",
      "2022-06-30 00:58:45,223 - NCNet pretrain, Epoch [44 / 300]: loss 0.3441, training auc: 0.7880, val_auc 0.7756\n",
      "2022-06-30 00:58:45,349 - NCNet pretrain, Epoch [45 / 300]: loss 0.3517, training auc: 0.7756, val_auc 0.7865\n",
      "2022-06-30 00:58:45,467 - NCNet pretrain, Epoch [46 / 300]: loss 0.3454, training auc: 0.8064, val_auc 0.7681\n",
      "2022-06-30 00:58:45,585 - NCNet pretrain, Epoch [47 / 300]: loss 0.3500, training auc: 0.7780, val_auc 0.7854\n",
      "2022-06-30 00:58:45,715 - NCNet pretrain, Epoch [48 / 300]: loss 0.3444, training auc: 0.7999, val_auc 0.7906, test auc 0.7972\n",
      "2022-06-30 00:58:45,852 - NCNet pretrain, Epoch [49 / 300]: loss 0.3449, training auc: 0.8007, val_auc 0.7793\n",
      "2022-06-30 00:58:45,970 - NCNet pretrain, Epoch [50 / 300]: loss 0.3397, training auc: 0.7799, val_auc 0.7788\n",
      "2022-06-30 00:58:46,088 - NCNet pretrain, Epoch [51 / 300]: loss 0.3386, training auc: 0.7893, val_auc 0.7903\n",
      "2022-06-30 00:58:46,216 - NCNet pretrain, Epoch [52 / 300]: loss 0.3321, training auc: 0.8134, val_auc 0.7919, test auc 0.7980\n",
      "2022-06-30 00:58:46,334 - NCNet pretrain, Epoch [53 / 300]: loss 0.3384, training auc: 0.8090, val_auc 0.7785\n",
      "2022-06-30 00:58:46,452 - NCNet pretrain, Epoch [54 / 300]: loss 0.3370, training auc: 0.7941, val_auc 0.7837\n",
      "2022-06-30 00:58:46,581 - NCNet pretrain, Epoch [55 / 300]: loss 0.3272, training auc: 0.8101, val_auc 0.7946, test auc 0.8011\n",
      "2022-06-30 00:58:46,699 - NCNet pretrain, Epoch [56 / 300]: loss 0.3405, training auc: 0.8127, val_auc 0.7928\n",
      "2022-06-30 00:58:46,837 - NCNet pretrain, Epoch [57 / 300]: loss 0.3281, training auc: 0.8158, val_auc 0.7860\n",
      "2022-06-30 00:58:46,954 - NCNet pretrain, Epoch [58 / 300]: loss 0.3288, training auc: 0.8021, val_auc 0.7910\n",
      "2022-06-30 00:58:47,085 - NCNet pretrain, Epoch [59 / 300]: loss 0.3253, training auc: 0.8136, val_auc 0.7965, test auc 0.8021\n",
      "2022-06-30 00:58:47,203 - NCNet pretrain, Epoch [60 / 300]: loss 0.3299, training auc: 0.8162, val_auc 0.7917\n",
      "2022-06-30 00:58:47,322 - NCNet pretrain, Epoch [61 / 300]: loss 0.3267, training auc: 0.8040, val_auc 0.7902\n",
      "2022-06-30 00:58:47,456 - NCNet pretrain, Epoch [62 / 300]: loss 0.3281, training auc: 0.8048, val_auc 0.7973, test auc 0.8029\n",
      "2022-06-30 00:58:47,574 - NCNet pretrain, Epoch [63 / 300]: loss 0.3194, training auc: 0.8280, val_auc 0.7961\n",
      "2022-06-30 00:58:47,691 - NCNet pretrain, Epoch [64 / 300]: loss 0.3234, training auc: 0.8203, val_auc 0.7909\n",
      "2022-06-30 00:58:47,809 - NCNet pretrain, Epoch [65 / 300]: loss 0.3281, training auc: 0.8041, val_auc 0.7962\n",
      "2022-06-30 00:58:47,938 - NCNet pretrain, Epoch [66 / 300]: loss 0.3202, training auc: 0.8262, val_auc 0.8006, test auc 0.8069\n",
      "2022-06-30 00:58:48,055 - NCNet pretrain, Epoch [67 / 300]: loss 0.3188, training auc: 0.8328, val_auc 0.7956\n",
      "2022-06-30 00:58:48,173 - NCNet pretrain, Epoch [68 / 300]: loss 0.3251, training auc: 0.8170, val_auc 0.7924\n",
      "2022-06-30 00:58:48,291 - NCNet pretrain, Epoch [69 / 300]: loss 0.3125, training auc: 0.8323, val_auc 0.7982\n",
      "2022-06-30 00:58:48,439 - NCNet pretrain, Epoch [70 / 300]: loss 0.3143, training auc: 0.8295, val_auc 0.8020, test auc 0.8093\n",
      "2022-06-30 00:58:48,576 - NCNet pretrain, Epoch [71 / 300]: loss 0.3101, training auc: 0.8406, val_auc 0.7963\n",
      "2022-06-30 00:58:48,714 - NCNet pretrain, Epoch [72 / 300]: loss 0.3204, training auc: 0.8176, val_auc 0.7944\n",
      "2022-06-30 00:58:48,862 - NCNet pretrain, Epoch [73 / 300]: loss 0.3226, training auc: 0.8159, val_auc 0.8028, test auc 0.8127\n",
      "2022-06-30 00:58:48,990 - NCNet pretrain, Epoch [74 / 300]: loss 0.3147, training auc: 0.8365, val_auc 0.8003\n",
      "2022-06-30 00:58:49,129 - NCNet pretrain, Epoch [75 / 300]: loss 0.3188, training auc: 0.8279, val_auc 0.7953\n",
      "2022-06-30 00:58:49,267 - NCNet pretrain, Epoch [76 / 300]: loss 0.3134, training auc: 0.8273, val_auc 0.7969\n",
      "2022-06-30 00:58:49,405 - NCNet pretrain, Epoch [77 / 300]: loss 0.3131, training auc: 0.8356, val_auc 0.8013\n",
      "2022-06-30 00:58:49,547 - NCNet pretrain, Epoch [78 / 300]: loss 0.3137, training auc: 0.8451, val_auc 0.7929\n",
      "2022-06-30 00:58:49,686 - NCNet pretrain, Epoch [79 / 300]: loss 0.3055, training auc: 0.8372, val_auc 0.7938\n",
      "2022-06-30 00:58:49,824 - NCNet pretrain, Epoch [80 / 300]: loss 0.3139, training auc: 0.8261, val_auc 0.8002\n",
      "2022-06-30 00:58:49,941 - NCNet pretrain, Epoch [81 / 300]: loss 0.3136, training auc: 0.8453, val_auc 0.7922\n",
      "2022-06-30 00:58:50,059 - NCNet pretrain, Epoch [82 / 300]: loss 0.3114, training auc: 0.8238, val_auc 0.7899\n",
      "2022-06-30 00:58:50,177 - NCNet pretrain, Epoch [83 / 300]: loss 0.3082, training auc: 0.8309, val_auc 0.7995\n",
      "2022-06-30 00:58:50,295 - NCNet pretrain, Epoch [84 / 300]: loss 0.3119, training auc: 0.8452, val_auc 0.7982\n",
      "2022-06-30 00:58:50,432 - NCNet pretrain, Epoch [85 / 300]: loss 0.3154, training auc: 0.8318, val_auc 0.7916\n",
      "2022-06-30 00:58:50,550 - NCNet pretrain, Epoch [86 / 300]: loss 0.3111, training auc: 0.8296, val_auc 0.7983\n",
      "2022-06-30 00:58:50,668 - NCNet pretrain, Epoch [87 / 300]: loss 0.3025, training auc: 0.8497, val_auc 0.8007\n",
      "2022-06-30 00:58:50,787 - NCNet pretrain, Epoch [88 / 300]: loss 0.3114, training auc: 0.8527, val_auc 0.7923\n",
      "2022-06-30 00:58:50,906 - NCNet pretrain, Epoch [89 / 300]: loss 0.3064, training auc: 0.8395, val_auc 0.7976\n",
      "2022-06-30 00:58:51,024 - NCNet pretrain, Epoch [90 / 300]: loss 0.3055, training auc: 0.8483, val_auc 0.8008\n",
      "2022-06-30 00:58:51,142 - NCNet pretrain, Epoch [91 / 300]: loss 0.3021, training auc: 0.8513, val_auc 0.7873\n",
      "2022-06-30 00:58:51,261 - NCNet pretrain, Epoch [92 / 300]: loss 0.3090, training auc: 0.8339, val_auc 0.7948\n",
      "2022-06-30 00:58:51,380 - NCNet pretrain, Epoch [93 / 300]: loss 0.2994, training auc: 0.8430, val_auc 0.8011\n",
      "2022-06-30 00:58:51,499 - NCNet pretrain, Epoch [94 / 300]: loss 0.3066, training auc: 0.8554, val_auc 0.7882\n",
      "2022-06-30 00:58:51,621 - NCNet pretrain, Epoch [95 / 300]: loss 0.2992, training auc: 0.8487, val_auc 0.7917\n",
      "2022-06-30 00:58:51,747 - NCNet pretrain, Epoch [96 / 300]: loss 0.2993, training auc: 0.8472, val_auc 0.8011\n",
      "2022-06-30 00:58:51,868 - NCNet pretrain, Epoch [97 / 300]: loss 0.3023, training auc: 0.8633, val_auc 0.7933\n",
      "2022-06-30 00:58:51,985 - NCNet pretrain, Epoch [98 / 300]: loss 0.2951, training auc: 0.8485, val_auc 0.7914\n",
      "2022-06-30 00:58:52,115 - NCNet pretrain, Epoch [99 / 300]: loss 0.2973, training auc: 0.8497, val_auc 0.8002\n",
      "2022-06-30 00:58:52,236 - NCNet pretrain, Epoch [100 / 300]: loss 0.2968, training auc: 0.8540, val_auc 0.7947\n",
      "2022-06-30 00:58:52,354 - NCNet pretrain, Epoch [101 / 300]: loss 0.2925, training auc: 0.8566, val_auc 0.7883\n",
      "2022-06-30 00:58:52,471 - NCNet pretrain, Epoch [102 / 300]: loss 0.2941, training auc: 0.8494, val_auc 0.7982\n",
      "2022-06-30 00:58:52,589 - NCNet pretrain, Epoch [103 / 300]: loss 0.2948, training auc: 0.8618, val_auc 0.7969\n",
      "2022-06-30 00:58:52,706 - NCNet pretrain, Epoch [104 / 300]: loss 0.2876, training auc: 0.8626, val_auc 0.7862\n",
      "2022-06-30 00:58:52,827 - NCNet pretrain, Epoch [105 / 300]: loss 0.2974, training auc: 0.8481, val_auc 0.7944\n",
      "2022-06-30 00:58:52,945 - NCNet pretrain, Epoch [106 / 300]: loss 0.2912, training auc: 0.8535, val_auc 0.7960\n",
      "2022-06-30 00:58:53,063 - NCNet pretrain, Epoch [107 / 300]: loss 0.2956, training auc: 0.8539, val_auc 0.7879\n",
      "2022-06-30 00:58:53,180 - NCNet pretrain, Epoch [108 / 300]: loss 0.2949, training auc: 0.8544, val_auc 0.7894\n",
      "2022-06-30 00:58:53,298 - NCNet pretrain, Epoch [109 / 300]: loss 0.2959, training auc: 0.8509, val_auc 0.7976\n",
      "2022-06-30 00:58:53,418 - NCNet pretrain, Epoch [110 / 300]: loss 0.3028, training auc: 0.8617, val_auc 0.7820\n",
      "2022-06-30 00:58:53,555 - NCNet pretrain, Epoch [111 / 300]: loss 0.3011, training auc: 0.8435, val_auc 0.7874\n",
      "2022-06-30 00:58:53,692 - NCNet pretrain, Epoch [112 / 300]: loss 0.2951, training auc: 0.8525, val_auc 0.7985\n",
      "2022-06-30 00:58:53,819 - NCNet pretrain, Epoch [113 / 300]: loss 0.3164, training auc: 0.8670, val_auc 0.7818\n",
      "2022-06-30 00:58:53,937 - NCNet pretrain, Epoch [114 / 300]: loss 0.3021, training auc: 0.8486, val_auc 0.7838\n",
      "2022-06-30 00:58:54,056 - NCNet pretrain, Epoch [115 / 300]: loss 0.2996, training auc: 0.8499, val_auc 0.7983\n",
      "2022-06-30 00:58:54,174 - NCNet pretrain, Epoch [116 / 300]: loss 0.3037, training auc: 0.8683, val_auc 0.7933\n",
      "2022-06-30 00:58:54,291 - NCNet pretrain, Epoch [117 / 300]: loss 0.2911, training auc: 0.8640, val_auc 0.7835\n",
      "2022-06-30 00:58:54,408 - NCNet pretrain, Epoch [118 / 300]: loss 0.3095, training auc: 0.8469, val_auc 0.7929\n",
      "2022-06-30 00:58:54,525 - NCNet pretrain, Epoch [119 / 300]: loss 0.2858, training auc: 0.8657, val_auc 0.7989\n",
      "2022-06-30 00:58:54,643 - NCNet pretrain, Epoch [120 / 300]: loss 0.2991, training auc: 0.8697, val_auc 0.7901\n",
      "2022-06-30 00:58:54,781 - NCNet pretrain, Epoch [121 / 300]: loss 0.2861, training auc: 0.8658, val_auc 0.7862\n",
      "2022-06-30 00:58:54,901 - NCNet pretrain, Epoch [122 / 300]: loss 0.2943, training auc: 0.8504, val_auc 0.7938\n",
      "2022-06-30 00:58:55,019 - NCNet pretrain, Epoch [123 / 300]: loss 0.2851, training auc: 0.8625, val_auc 0.7976\n",
      "2022-06-30 00:58:55,020 - Early stop!\n",
      "2022-06-30 00:58:55,020 - Best Test Results: auc 0.8127, ap 0.4409, f1 0.2802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 248992.98it/s]\n",
      "2022-06-30 00:58:56,160 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:58:56,374 - NCNet pretrain, Epoch [1 / 300]: loss 0.8100, training auc: 0.6565, val_auc 0.4668, test auc 0.4380\n",
      "2022-06-30 00:58:56,493 - NCNet pretrain, Epoch [2 / 300]: loss 0.5047, training auc: 0.4697, val_auc 0.4611\n",
      "2022-06-30 00:58:56,610 - NCNet pretrain, Epoch [3 / 300]: loss 0.4846, training auc: 0.4402, val_auc 0.4600\n",
      "2022-06-30 00:58:56,727 - NCNet pretrain, Epoch [4 / 300]: loss 0.5483, training auc: 0.3837, val_auc 0.4610\n",
      "2022-06-30 00:58:56,846 - NCNet pretrain, Epoch [5 / 300]: loss 0.5241, training auc: 0.3990, val_auc 0.4633\n",
      "2022-06-30 00:58:56,964 - NCNet pretrain, Epoch [6 / 300]: loss 0.4847, training auc: 0.4514, val_auc 0.4668\n",
      "2022-06-30 00:58:57,093 - NCNet pretrain, Epoch [7 / 300]: loss 0.4799, training auc: 0.4244, val_auc 0.4705, test auc 0.4468\n",
      "2022-06-30 00:58:57,223 - NCNet pretrain, Epoch [8 / 300]: loss 0.4704, training auc: 0.4666, val_auc 0.4736, test auc 0.4517\n",
      "2022-06-30 00:58:57,353 - NCNet pretrain, Epoch [9 / 300]: loss 0.4860, training auc: 0.4211, val_auc 0.4757, test auc 0.4538\n",
      "2022-06-30 00:58:57,477 - NCNet pretrain, Epoch [10 / 300]: loss 0.4741, training auc: 0.5009, val_auc 0.4763, test auc 0.4538\n",
      "2022-06-30 00:58:57,628 - NCNet pretrain, Epoch [11 / 300]: loss 0.4753, training auc: 0.4393, val_auc 0.4773, test auc 0.4539\n",
      "2022-06-30 00:58:57,776 - NCNet pretrain, Epoch [12 / 300]: loss 0.4696, training auc: 0.4659, val_auc 0.4784, test auc 0.4542\n",
      "2022-06-30 00:58:57,909 - NCNet pretrain, Epoch [13 / 300]: loss 0.4758, training auc: 0.4310, val_auc 0.4788, test auc 0.4543\n",
      "2022-06-30 00:58:58,046 - NCNet pretrain, Epoch [14 / 300]: loss 0.4830, training auc: 0.4075, val_auc 0.4794, test auc 0.4551\n",
      "2022-06-30 00:58:58,191 - NCNet pretrain, Epoch [15 / 300]: loss 0.4786, training auc: 0.4182, val_auc 0.4799, test auc 0.4558\n",
      "2022-06-30 00:58:58,339 - NCNet pretrain, Epoch [16 / 300]: loss 0.4699, training auc: 0.4446, val_auc 0.4804, test auc 0.4568\n",
      "2022-06-30 00:58:58,468 - NCNet pretrain, Epoch [17 / 300]: loss 0.4616, training auc: 0.4728, val_auc 0.4812, test auc 0.4578\n",
      "2022-06-30 00:58:58,615 - NCNet pretrain, Epoch [18 / 300]: loss 0.4733, training auc: 0.4234, val_auc 0.4824, test auc 0.4594\n",
      "2022-06-30 00:58:58,762 - NCNet pretrain, Epoch [19 / 300]: loss 0.4656, training auc: 0.4556, val_auc 0.4835, test auc 0.4607\n",
      "2022-06-30 00:58:58,892 - NCNet pretrain, Epoch [20 / 300]: loss 0.4762, training auc: 0.4089, val_auc 0.4847, test auc 0.4620\n",
      "2022-06-30 00:58:59,020 - NCNet pretrain, Epoch [21 / 300]: loss 0.4693, training auc: 0.4248, val_auc 0.4859, test auc 0.4633\n",
      "2022-06-30 00:58:59,149 - NCNet pretrain, Epoch [22 / 300]: loss 0.4636, training auc: 0.4614, val_auc 0.4874, test auc 0.4644\n",
      "2022-06-30 00:58:59,279 - NCNet pretrain, Epoch [23 / 300]: loss 0.4655, training auc: 0.4379, val_auc 0.4890, test auc 0.4654\n",
      "2022-06-30 00:58:59,408 - NCNet pretrain, Epoch [24 / 300]: loss 0.4625, training auc: 0.4587, val_auc 0.4906, test auc 0.4664\n",
      "2022-06-30 00:58:59,536 - NCNet pretrain, Epoch [25 / 300]: loss 0.4540, training auc: 0.4859, val_auc 0.4921, test auc 0.4674\n",
      "2022-06-30 00:58:59,669 - NCNet pretrain, Epoch [26 / 300]: loss 0.4680, training auc: 0.4158, val_auc 0.4942, test auc 0.4693\n",
      "2022-06-30 00:58:59,803 - NCNet pretrain, Epoch [27 / 300]: loss 0.4534, training auc: 0.4748, val_auc 0.4967, test auc 0.4717\n",
      "2022-06-30 00:58:59,943 - NCNet pretrain, Epoch [28 / 300]: loss 0.4506, training auc: 0.4832, val_auc 0.4994, test auc 0.4746\n",
      "2022-06-30 00:59:00,073 - NCNet pretrain, Epoch [29 / 300]: loss 0.4446, training auc: 0.4976, val_auc 0.5023, test auc 0.4777\n",
      "2022-06-30 00:59:00,206 - NCNet pretrain, Epoch [30 / 300]: loss 0.4563, training auc: 0.4545, val_auc 0.5061, test auc 0.4820\n",
      "2022-06-30 00:59:00,335 - NCNet pretrain, Epoch [31 / 300]: loss 0.4567, training auc: 0.4576, val_auc 0.5117, test auc 0.4882\n",
      "2022-06-30 00:59:00,485 - NCNet pretrain, Epoch [32 / 300]: loss 0.4488, training auc: 0.4956, val_auc 0.5175, test auc 0.4941\n",
      "2022-06-30 00:59:00,614 - NCNet pretrain, Epoch [33 / 300]: loss 0.4552, training auc: 0.4638, val_auc 0.5235, test auc 0.5002\n",
      "2022-06-30 00:59:00,742 - NCNet pretrain, Epoch [34 / 300]: loss 0.4444, training auc: 0.5073, val_auc 0.5270, test auc 0.5037\n",
      "2022-06-30 00:59:00,872 - NCNet pretrain, Epoch [35 / 300]: loss 0.4450, training auc: 0.4934, val_auc 0.5296, test auc 0.5062\n",
      "2022-06-30 00:59:01,021 - NCNet pretrain, Epoch [36 / 300]: loss 0.4446, training auc: 0.4862, val_auc 0.5348, test auc 0.5111\n",
      "2022-06-30 00:59:01,150 - NCNet pretrain, Epoch [37 / 300]: loss 0.4322, training auc: 0.5486, val_auc 0.5413, test auc 0.5178\n",
      "2022-06-30 00:59:01,286 - NCNet pretrain, Epoch [38 / 300]: loss 0.4393, training auc: 0.5019, val_auc 0.5529, test auc 0.5303\n",
      "2022-06-30 00:59:01,417 - NCNet pretrain, Epoch [39 / 300]: loss 0.4383, training auc: 0.5198, val_auc 0.5691, test auc 0.5475\n",
      "2022-06-30 00:59:01,547 - NCNet pretrain, Epoch [40 / 300]: loss 0.4313, training auc: 0.5524, val_auc 0.5838, test auc 0.5629\n",
      "2022-06-30 00:59:01,679 - NCNet pretrain, Epoch [41 / 300]: loss 0.4281, training auc: 0.5884, val_auc 0.5920, test auc 0.5708\n",
      "2022-06-30 00:59:01,814 - NCNet pretrain, Epoch [42 / 300]: loss 0.4234, training auc: 0.5875, val_auc 0.5961, test auc 0.5742\n",
      "2022-06-30 00:59:01,959 - NCNet pretrain, Epoch [43 / 300]: loss 0.4253, training auc: 0.5737, val_auc 0.6034, test auc 0.5805\n",
      "2022-06-30 00:59:02,109 - NCNet pretrain, Epoch [44 / 300]: loss 0.4192, training auc: 0.6006, val_auc 0.6145, test auc 0.5907\n",
      "2022-06-30 00:59:02,242 - NCNet pretrain, Epoch [45 / 300]: loss 0.4228, training auc: 0.5642, val_auc 0.6391, test auc 0.6141\n",
      "2022-06-30 00:59:02,377 - NCNet pretrain, Epoch [46 / 300]: loss 0.4192, training auc: 0.5853, val_auc 0.6689, test auc 0.6428\n",
      "2022-06-30 00:59:02,508 - NCNet pretrain, Epoch [47 / 300]: loss 0.4188, training auc: 0.6147, val_auc 0.6807, test auc 0.6546\n",
      "2022-06-30 00:59:02,627 - NCNet pretrain, Epoch [48 / 300]: loss 0.4107, training auc: 0.6487, val_auc 0.6690\n",
      "2022-06-30 00:59:02,747 - NCNet pretrain, Epoch [49 / 300]: loss 0.4027, training auc: 0.6421, val_auc 0.6665\n",
      "2022-06-30 00:59:02,879 - NCNet pretrain, Epoch [50 / 300]: loss 0.4064, training auc: 0.6248, val_auc 0.6895, test auc 0.6632\n",
      "2022-06-30 00:59:03,012 - NCNet pretrain, Epoch [51 / 300]: loss 0.3978, training auc: 0.6603, val_auc 0.7205, test auc 0.7052\n",
      "2022-06-30 00:59:03,142 - NCNet pretrain, Epoch [52 / 300]: loss 0.3961, training auc: 0.6956, val_auc 0.7240, test auc 0.7105\n",
      "2022-06-30 00:59:03,280 - NCNet pretrain, Epoch [53 / 300]: loss 0.3875, training auc: 0.7232, val_auc 0.6985\n",
      "2022-06-30 00:59:03,399 - NCNet pretrain, Epoch [54 / 300]: loss 0.3882, training auc: 0.6854, val_auc 0.6915\n",
      "2022-06-30 00:59:03,535 - NCNet pretrain, Epoch [55 / 300]: loss 0.3995, training auc: 0.6736, val_auc 0.7162\n",
      "2022-06-30 00:59:03,684 - NCNet pretrain, Epoch [56 / 300]: loss 0.3822, training auc: 0.6947, val_auc 0.7387, test auc 0.7312\n",
      "2022-06-30 00:59:03,803 - NCNet pretrain, Epoch [57 / 300]: loss 0.3901, training auc: 0.7282, val_auc 0.7355\n",
      "2022-06-30 00:59:03,921 - NCNet pretrain, Epoch [58 / 300]: loss 0.3879, training auc: 0.7110, val_auc 0.7208\n",
      "2022-06-30 00:59:04,042 - NCNet pretrain, Epoch [59 / 300]: loss 0.3796, training auc: 0.7051, val_auc 0.7189\n",
      "2022-06-30 00:59:04,161 - NCNet pretrain, Epoch [60 / 300]: loss 0.3871, training auc: 0.6886, val_auc 0.7358\n",
      "2022-06-30 00:59:04,296 - NCNet pretrain, Epoch [61 / 300]: loss 0.3767, training auc: 0.7197, val_auc 0.7469, test auc 0.7418\n",
      "2022-06-30 00:59:04,419 - NCNet pretrain, Epoch [62 / 300]: loss 0.3808, training auc: 0.7409, val_auc 0.7446\n",
      "2022-06-30 00:59:04,540 - NCNet pretrain, Epoch [63 / 300]: loss 0.3707, training auc: 0.7487, val_auc 0.7379\n",
      "2022-06-30 00:59:04,657 - NCNet pretrain, Epoch [64 / 300]: loss 0.3677, training auc: 0.7334, val_auc 0.7426\n",
      "2022-06-30 00:59:04,792 - NCNet pretrain, Epoch [65 / 300]: loss 0.3718, training auc: 0.7349, val_auc 0.7510, test auc 0.7479\n",
      "2022-06-30 00:59:04,940 - NCNet pretrain, Epoch [66 / 300]: loss 0.3696, training auc: 0.7518, val_auc 0.7525, test auc 0.7502\n",
      "2022-06-30 00:59:05,058 - NCNet pretrain, Epoch [67 / 300]: loss 0.3719, training auc: 0.7481, val_auc 0.7459\n",
      "2022-06-30 00:59:05,175 - NCNet pretrain, Epoch [68 / 300]: loss 0.3623, training auc: 0.7456, val_auc 0.7420\n",
      "2022-06-30 00:59:05,293 - NCNet pretrain, Epoch [69 / 300]: loss 0.3598, training auc: 0.7490, val_auc 0.7486\n",
      "2022-06-30 00:59:05,422 - NCNet pretrain, Epoch [70 / 300]: loss 0.3591, training auc: 0.7553, val_auc 0.7585, test auc 0.7578\n",
      "2022-06-30 00:59:05,569 - NCNet pretrain, Epoch [71 / 300]: loss 0.3600, training auc: 0.7733, val_auc 0.7596, test auc 0.7590\n",
      "2022-06-30 00:59:05,687 - NCNet pretrain, Epoch [72 / 300]: loss 0.3527, training auc: 0.7805, val_auc 0.7508\n",
      "2022-06-30 00:59:05,810 - NCNet pretrain, Epoch [73 / 300]: loss 0.3571, training auc: 0.7617, val_auc 0.7501\n",
      "2022-06-30 00:59:05,940 - NCNet pretrain, Epoch [74 / 300]: loss 0.3483, training auc: 0.7716, val_auc 0.7612, test auc 0.7601\n",
      "2022-06-30 00:59:06,070 - NCNet pretrain, Epoch [75 / 300]: loss 0.3628, training auc: 0.7646, val_auc 0.7645, test auc 0.7656\n",
      "2022-06-30 00:59:06,212 - NCNet pretrain, Epoch [76 / 300]: loss 0.3544, training auc: 0.7767, val_auc 0.7571\n",
      "2022-06-30 00:59:06,350 - NCNet pretrain, Epoch [77 / 300]: loss 0.3511, training auc: 0.7646, val_auc 0.7538\n",
      "2022-06-30 00:59:06,468 - NCNet pretrain, Epoch [78 / 300]: loss 0.3525, training auc: 0.7651, val_auc 0.7636\n",
      "2022-06-30 00:59:06,601 - NCNet pretrain, Epoch [79 / 300]: loss 0.3533, training auc: 0.7736, val_auc 0.7702, test auc 0.7750\n",
      "2022-06-30 00:59:06,723 - NCNet pretrain, Epoch [80 / 300]: loss 0.3564, training auc: 0.7841, val_auc 0.7607\n",
      "2022-06-30 00:59:06,840 - NCNet pretrain, Epoch [81 / 300]: loss 0.3539, training auc: 0.7750, val_auc 0.7522\n",
      "2022-06-30 00:59:06,959 - NCNet pretrain, Epoch [82 / 300]: loss 0.3589, training auc: 0.7680, val_auc 0.7661\n",
      "2022-06-30 00:59:07,088 - NCNet pretrain, Epoch [83 / 300]: loss 0.3393, training auc: 0.7952, val_auc 0.7728, test auc 0.7783\n",
      "2022-06-30 00:59:07,210 - NCNet pretrain, Epoch [84 / 300]: loss 0.3449, training auc: 0.8018, val_auc 0.7661\n",
      "2022-06-30 00:59:07,332 - NCNet pretrain, Epoch [85 / 300]: loss 0.3476, training auc: 0.7841, val_auc 0.7601\n",
      "2022-06-30 00:59:07,450 - NCNet pretrain, Epoch [86 / 300]: loss 0.3435, training auc: 0.7779, val_auc 0.7672\n",
      "2022-06-30 00:59:07,579 - NCNet pretrain, Epoch [87 / 300]: loss 0.3440, training auc: 0.7853, val_auc 0.7742, test auc 0.7798\n",
      "2022-06-30 00:59:07,696 - NCNet pretrain, Epoch [88 / 300]: loss 0.3456, training auc: 0.7945, val_auc 0.7729\n",
      "2022-06-30 00:59:07,815 - NCNet pretrain, Epoch [89 / 300]: loss 0.3387, training auc: 0.7994, val_auc 0.7619\n",
      "2022-06-30 00:59:07,933 - NCNet pretrain, Epoch [90 / 300]: loss 0.3489, training auc: 0.7783, val_auc 0.7696\n",
      "2022-06-30 00:59:08,077 - NCNet pretrain, Epoch [91 / 300]: loss 0.3361, training auc: 0.7988, val_auc 0.7768, test auc 0.7834\n",
      "2022-06-30 00:59:08,195 - NCNet pretrain, Epoch [92 / 300]: loss 0.3363, training auc: 0.8084, val_auc 0.7742\n",
      "2022-06-30 00:59:08,314 - NCNet pretrain, Epoch [93 / 300]: loss 0.3322, training auc: 0.8119, val_auc 0.7678\n",
      "2022-06-30 00:59:08,435 - NCNet pretrain, Epoch [94 / 300]: loss 0.3398, training auc: 0.7932, val_auc 0.7729\n",
      "2022-06-30 00:59:08,566 - NCNet pretrain, Epoch [95 / 300]: loss 0.3313, training auc: 0.8063, val_auc 0.7776, test auc 0.7848\n",
      "2022-06-30 00:59:08,685 - NCNet pretrain, Epoch [96 / 300]: loss 0.3334, training auc: 0.8113, val_auc 0.7728\n",
      "2022-06-30 00:59:08,831 - NCNet pretrain, Epoch [97 / 300]: loss 0.3315, training auc: 0.7995, val_auc 0.7691\n",
      "2022-06-30 00:59:08,948 - NCNet pretrain, Epoch [98 / 300]: loss 0.3318, training auc: 0.8037, val_auc 0.7769\n",
      "2022-06-30 00:59:09,095 - NCNet pretrain, Epoch [99 / 300]: loss 0.3280, training auc: 0.8163, val_auc 0.7796, test auc 0.7906\n",
      "2022-06-30 00:59:09,213 - NCNet pretrain, Epoch [100 / 300]: loss 0.3225, training auc: 0.8289, val_auc 0.7718\n",
      "2022-06-30 00:59:09,332 - NCNet pretrain, Epoch [101 / 300]: loss 0.3358, training auc: 0.8091, val_auc 0.7736\n",
      "2022-06-30 00:59:09,461 - NCNet pretrain, Epoch [102 / 300]: loss 0.3261, training auc: 0.8204, val_auc 0.7809, test auc 0.7934\n",
      "2022-06-30 00:59:09,580 - NCNet pretrain, Epoch [103 / 300]: loss 0.3295, training auc: 0.8271, val_auc 0.7771\n",
      "2022-06-30 00:59:09,698 - NCNet pretrain, Epoch [104 / 300]: loss 0.3320, training auc: 0.8111, val_auc 0.7708\n",
      "2022-06-30 00:59:09,817 - NCNet pretrain, Epoch [105 / 300]: loss 0.3308, training auc: 0.8117, val_auc 0.7770\n",
      "2022-06-30 00:59:09,947 - NCNet pretrain, Epoch [106 / 300]: loss 0.3273, training auc: 0.8141, val_auc 0.7813, test auc 0.7947\n",
      "2022-06-30 00:59:10,069 - NCNet pretrain, Epoch [107 / 300]: loss 0.3227, training auc: 0.8349, val_auc 0.7772\n",
      "2022-06-30 00:59:10,188 - NCNet pretrain, Epoch [108 / 300]: loss 0.3156, training auc: 0.8313, val_auc 0.7744\n",
      "2022-06-30 00:59:10,307 - NCNet pretrain, Epoch [109 / 300]: loss 0.3282, training auc: 0.8112, val_auc 0.7800\n",
      "2022-06-30 00:59:10,458 - NCNet pretrain, Epoch [110 / 300]: loss 0.3242, training auc: 0.8232, val_auc 0.7814, test auc 0.7962\n",
      "2022-06-30 00:59:10,582 - NCNet pretrain, Epoch [111 / 300]: loss 0.3222, training auc: 0.8331, val_auc 0.7735\n",
      "2022-06-30 00:59:10,720 - NCNet pretrain, Epoch [112 / 300]: loss 0.3196, training auc: 0.8224, val_auc 0.7746\n",
      "2022-06-30 00:59:10,850 - NCNet pretrain, Epoch [113 / 300]: loss 0.3211, training auc: 0.8222, val_auc 0.7814, test auc 0.7969\n",
      "2022-06-30 00:59:10,976 - NCNet pretrain, Epoch [114 / 300]: loss 0.3215, training auc: 0.8319, val_auc 0.7810\n",
      "2022-06-30 00:59:11,096 - NCNet pretrain, Epoch [115 / 300]: loss 0.3261, training auc: 0.8209, val_auc 0.7699\n",
      "2022-06-30 00:59:11,215 - NCNet pretrain, Epoch [116 / 300]: loss 0.3321, training auc: 0.8099, val_auc 0.7786\n",
      "2022-06-30 00:59:11,344 - NCNet pretrain, Epoch [117 / 300]: loss 0.3245, training auc: 0.8265, val_auc 0.7815, test auc 0.8001\n",
      "2022-06-30 00:59:11,482 - NCNet pretrain, Epoch [118 / 300]: loss 0.3277, training auc: 0.8317, val_auc 0.7788\n",
      "2022-06-30 00:59:11,600 - NCNet pretrain, Epoch [119 / 300]: loss 0.3166, training auc: 0.8247, val_auc 0.7733\n",
      "2022-06-30 00:59:11,738 - NCNet pretrain, Epoch [120 / 300]: loss 0.3235, training auc: 0.8291, val_auc 0.7790\n",
      "2022-06-30 00:59:11,867 - NCNet pretrain, Epoch [121 / 300]: loss 0.3113, training auc: 0.8424, val_auc 0.7821, test auc 0.8025\n",
      "2022-06-30 00:59:11,985 - NCNet pretrain, Epoch [122 / 300]: loss 0.3222, training auc: 0.8393, val_auc 0.7783\n",
      "2022-06-30 00:59:12,123 - NCNet pretrain, Epoch [123 / 300]: loss 0.3065, training auc: 0.8461, val_auc 0.7702\n",
      "2022-06-30 00:59:12,261 - NCNet pretrain, Epoch [124 / 300]: loss 0.3155, training auc: 0.8297, val_auc 0.7809\n",
      "2022-06-30 00:59:12,391 - NCNet pretrain, Epoch [125 / 300]: loss 0.3102, training auc: 0.8425, val_auc 0.7822, test auc 0.8037\n",
      "2022-06-30 00:59:12,511 - NCNet pretrain, Epoch [126 / 300]: loss 0.3238, training auc: 0.8465, val_auc 0.7747\n",
      "2022-06-30 00:59:12,650 - NCNet pretrain, Epoch [127 / 300]: loss 0.3318, training auc: 0.8122, val_auc 0.7756\n",
      "2022-06-30 00:59:12,769 - NCNet pretrain, Epoch [128 / 300]: loss 0.3065, training auc: 0.8426, val_auc 0.7816\n",
      "2022-06-30 00:59:12,890 - NCNet pretrain, Epoch [129 / 300]: loss 0.3103, training auc: 0.8469, val_auc 0.7804\n",
      "2022-06-30 00:59:13,010 - NCNet pretrain, Epoch [130 / 300]: loss 0.3234, training auc: 0.8244, val_auc 0.7766\n",
      "2022-06-30 00:59:13,149 - NCNet pretrain, Epoch [131 / 300]: loss 0.3169, training auc: 0.8277, val_auc 0.7787\n",
      "2022-06-30 00:59:13,273 - NCNet pretrain, Epoch [132 / 300]: loss 0.3112, training auc: 0.8327, val_auc 0.7798\n",
      "2022-06-30 00:59:13,415 - NCNet pretrain, Epoch [133 / 300]: loss 0.3114, training auc: 0.8361, val_auc 0.7801\n",
      "2022-06-30 00:59:13,534 - NCNet pretrain, Epoch [134 / 300]: loss 0.3018, training auc: 0.8524, val_auc 0.7767\n",
      "2022-06-30 00:59:13,652 - NCNet pretrain, Epoch [135 / 300]: loss 0.3139, training auc: 0.8352, val_auc 0.7798\n",
      "2022-06-30 00:59:13,772 - NCNet pretrain, Epoch [136 / 300]: loss 0.3054, training auc: 0.8441, val_auc 0.7811\n",
      "2022-06-30 00:59:13,890 - NCNet pretrain, Epoch [137 / 300]: loss 0.3139, training auc: 0.8425, val_auc 0.7744\n",
      "2022-06-30 00:59:14,007 - NCNet pretrain, Epoch [138 / 300]: loss 0.3100, training auc: 0.8360, val_auc 0.7779\n",
      "2022-06-30 00:59:14,132 - NCNet pretrain, Epoch [139 / 300]: loss 0.3017, training auc: 0.8493, val_auc 0.7811\n",
      "2022-06-30 00:59:14,250 - NCNet pretrain, Epoch [140 / 300]: loss 0.2961, training auc: 0.8634, val_auc 0.7787\n",
      "2022-06-30 00:59:14,369 - NCNet pretrain, Epoch [141 / 300]: loss 0.3040, training auc: 0.8478, val_auc 0.7749\n",
      "2022-06-30 00:59:14,489 - NCNet pretrain, Epoch [142 / 300]: loss 0.3066, training auc: 0.8446, val_auc 0.7788\n",
      "2022-06-30 00:59:14,628 - NCNet pretrain, Epoch [143 / 300]: loss 0.3036, training auc: 0.8462, val_auc 0.7811\n",
      "2022-06-30 00:59:14,750 - NCNet pretrain, Epoch [144 / 300]: loss 0.3077, training auc: 0.8479, val_auc 0.7764\n",
      "2022-06-30 00:59:14,869 - NCNet pretrain, Epoch [145 / 300]: loss 0.3069, training auc: 0.8401, val_auc 0.7757\n",
      "2022-06-30 00:59:14,987 - NCNet pretrain, Epoch [146 / 300]: loss 0.3070, training auc: 0.8407, val_auc 0.7806\n",
      "2022-06-30 00:59:15,107 - NCNet pretrain, Epoch [147 / 300]: loss 0.3085, training auc: 0.8372, val_auc 0.7779\n",
      "2022-06-30 00:59:15,246 - NCNet pretrain, Epoch [148 / 300]: loss 0.3046, training auc: 0.8420, val_auc 0.7773\n",
      "2022-06-30 00:59:15,367 - NCNet pretrain, Epoch [149 / 300]: loss 0.3081, training auc: 0.8377, val_auc 0.7778\n",
      "2022-06-30 00:59:15,493 - NCNet pretrain, Epoch [150 / 300]: loss 0.3006, training auc: 0.8483, val_auc 0.7800\n",
      "2022-06-30 00:59:15,612 - NCNet pretrain, Epoch [151 / 300]: loss 0.3056, training auc: 0.8518, val_auc 0.7768\n",
      "2022-06-30 00:59:15,729 - NCNet pretrain, Epoch [152 / 300]: loss 0.3017, training auc: 0.8492, val_auc 0.7748\n",
      "2022-06-30 00:59:15,847 - NCNet pretrain, Epoch [153 / 300]: loss 0.3103, training auc: 0.8414, val_auc 0.7799\n",
      "2022-06-30 00:59:15,965 - NCNet pretrain, Epoch [154 / 300]: loss 0.3072, training auc: 0.8626, val_auc 0.7739\n",
      "2022-06-30 00:59:16,104 - NCNet pretrain, Epoch [155 / 300]: loss 0.3006, training auc: 0.8484, val_auc 0.7754\n",
      "2022-06-30 00:59:16,222 - NCNet pretrain, Epoch [156 / 300]: loss 0.3061, training auc: 0.8430, val_auc 0.7791\n",
      "2022-06-30 00:59:16,340 - NCNet pretrain, Epoch [157 / 300]: loss 0.3134, training auc: 0.8619, val_auc 0.7713\n",
      "2022-06-30 00:59:16,460 - NCNet pretrain, Epoch [158 / 300]: loss 0.3067, training auc: 0.8380, val_auc 0.7717\n",
      "2022-06-30 00:59:16,580 - NCNet pretrain, Epoch [159 / 300]: loss 0.2961, training auc: 0.8615, val_auc 0.7796\n",
      "2022-06-30 00:59:16,703 - NCNet pretrain, Epoch [160 / 300]: loss 0.3161, training auc: 0.8459, val_auc 0.7782\n",
      "2022-06-30 00:59:16,842 - NCNet pretrain, Epoch [161 / 300]: loss 0.2970, training auc: 0.8610, val_auc 0.7725\n",
      "2022-06-30 00:59:16,961 - NCNet pretrain, Epoch [162 / 300]: loss 0.2956, training auc: 0.8571, val_auc 0.7780\n",
      "2022-06-30 00:59:17,100 - NCNet pretrain, Epoch [163 / 300]: loss 0.2941, training auc: 0.8580, val_auc 0.7788\n",
      "2022-06-30 00:59:17,234 - NCNet pretrain, Epoch [164 / 300]: loss 0.2992, training auc: 0.8551, val_auc 0.7748\n",
      "2022-06-30 00:59:17,363 - NCNet pretrain, Epoch [165 / 300]: loss 0.3002, training auc: 0.8483, val_auc 0.7773\n",
      "2022-06-30 00:59:17,486 - NCNet pretrain, Epoch [166 / 300]: loss 0.2940, training auc: 0.8567, val_auc 0.7792\n",
      "2022-06-30 00:59:17,613 - NCNet pretrain, Epoch [167 / 300]: loss 0.2994, training auc: 0.8549, val_auc 0.7753\n",
      "2022-06-30 00:59:17,729 - NCNet pretrain, Epoch [168 / 300]: loss 0.2934, training auc: 0.8557, val_auc 0.7738\n",
      "2022-06-30 00:59:17,856 - NCNet pretrain, Epoch [169 / 300]: loss 0.2939, training auc: 0.8568, val_auc 0.7789\n",
      "2022-06-30 00:59:17,972 - NCNet pretrain, Epoch [170 / 300]: loss 0.2972, training auc: 0.8651, val_auc 0.7766\n",
      "2022-06-30 00:59:18,089 - NCNet pretrain, Epoch [171 / 300]: loss 0.2967, training auc: 0.8540, val_auc 0.7739\n",
      "2022-06-30 00:59:18,206 - NCNet pretrain, Epoch [172 / 300]: loss 0.2939, training auc: 0.8570, val_auc 0.7778\n",
      "2022-06-30 00:59:18,323 - NCNet pretrain, Epoch [173 / 300]: loss 0.2990, training auc: 0.8611, val_auc 0.7756\n",
      "2022-06-30 00:59:18,440 - NCNet pretrain, Epoch [174 / 300]: loss 0.2908, training auc: 0.8623, val_auc 0.7756\n",
      "2022-06-30 00:59:18,558 - NCNet pretrain, Epoch [175 / 300]: loss 0.2919, training auc: 0.8629, val_auc 0.7777\n",
      "2022-06-30 00:59:18,559 - Early stop!\n",
      "2022-06-30 00:59:18,560 - Best Test Results: auc 0.8037, ap 0.4166, f1 0.3429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 237730.53it/s]\n",
      "2022-06-30 00:59:19,736 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:59:19,914 - NCNet pretrain, Epoch [1 / 300]: loss 0.7422, training auc: 0.5827, val_auc 0.4585, test auc 0.4285\n",
      "2022-06-30 00:59:20,041 - NCNet pretrain, Epoch [2 / 300]: loss 0.5129, training auc: 0.4407, val_auc 0.4608, test auc 0.4316\n",
      "2022-06-30 00:59:20,177 - NCNet pretrain, Epoch [3 / 300]: loss 0.4704, training auc: 0.4135, val_auc 0.4601\n",
      "2022-06-30 00:59:20,294 - NCNet pretrain, Epoch [4 / 300]: loss 0.4844, training auc: 0.4720, val_auc 0.4604\n",
      "2022-06-30 00:59:20,411 - NCNet pretrain, Epoch [5 / 300]: loss 0.5019, training auc: 0.4625, val_auc 0.4606\n",
      "2022-06-30 00:59:20,556 - NCNet pretrain, Epoch [6 / 300]: loss 0.5218, training auc: 0.3725, val_auc 0.4621, test auc 0.4344\n",
      "2022-06-30 00:59:20,688 - NCNet pretrain, Epoch [7 / 300]: loss 0.4810, training auc: 0.4201, val_auc 0.4640, test auc 0.4372\n",
      "2022-06-30 00:59:20,816 - NCNet pretrain, Epoch [8 / 300]: loss 0.4660, training auc: 0.4381, val_auc 0.4672, test auc 0.4417\n",
      "2022-06-30 00:59:20,965 - NCNet pretrain, Epoch [9 / 300]: loss 0.4788, training auc: 0.3865, val_auc 0.4708, test auc 0.4468\n",
      "2022-06-30 00:59:21,115 - NCNet pretrain, Epoch [10 / 300]: loss 0.4671, training auc: 0.4733, val_auc 0.4729, test auc 0.4494\n",
      "2022-06-30 00:59:21,266 - NCNet pretrain, Epoch [11 / 300]: loss 0.4707, training auc: 0.4719, val_auc 0.4736, test auc 0.4500\n",
      "2022-06-30 00:59:21,393 - NCNet pretrain, Epoch [12 / 300]: loss 0.4690, training auc: 0.4748, val_auc 0.4737, test auc 0.4496\n",
      "2022-06-30 00:59:21,519 - NCNet pretrain, Epoch [13 / 300]: loss 0.4695, training auc: 0.4249, val_auc 0.4740, test auc 0.4490\n",
      "2022-06-30 00:59:21,644 - NCNet pretrain, Epoch [14 / 300]: loss 0.4660, training auc: 0.4166, val_auc 0.4744, test auc 0.4492\n",
      "2022-06-30 00:59:21,778 - NCNet pretrain, Epoch [15 / 300]: loss 0.4603, training auc: 0.4621, val_auc 0.4753, test auc 0.4497\n",
      "2022-06-30 00:59:21,907 - NCNet pretrain, Epoch [16 / 300]: loss 0.4586, training auc: 0.4674, val_auc 0.4760, test auc 0.4504\n",
      "2022-06-30 00:59:22,037 - NCNet pretrain, Epoch [17 / 300]: loss 0.4614, training auc: 0.4511, val_auc 0.4772, test auc 0.4518\n",
      "2022-06-30 00:59:22,168 - NCNet pretrain, Epoch [18 / 300]: loss 0.4611, training auc: 0.4545, val_auc 0.4787, test auc 0.4535\n",
      "2022-06-30 00:59:22,316 - NCNet pretrain, Epoch [19 / 300]: loss 0.4696, training auc: 0.4114, val_auc 0.4800, test auc 0.4555\n",
      "2022-06-30 00:59:22,446 - NCNet pretrain, Epoch [20 / 300]: loss 0.4526, training auc: 0.4820, val_auc 0.4816, test auc 0.4573\n",
      "2022-06-30 00:59:22,596 - NCNet pretrain, Epoch [21 / 300]: loss 0.4632, training auc: 0.4287, val_auc 0.4832, test auc 0.4595\n",
      "2022-06-30 00:59:22,745 - NCNet pretrain, Epoch [22 / 300]: loss 0.4600, training auc: 0.4433, val_auc 0.4845, test auc 0.4613\n",
      "2022-06-30 00:59:22,875 - NCNet pretrain, Epoch [23 / 300]: loss 0.4561, training auc: 0.4602, val_auc 0.4861, test auc 0.4629\n",
      "2022-06-30 00:59:23,025 - NCNet pretrain, Epoch [24 / 300]: loss 0.4609, training auc: 0.4457, val_auc 0.4879, test auc 0.4642\n",
      "2022-06-30 00:59:23,175 - NCNet pretrain, Epoch [25 / 300]: loss 0.4607, training auc: 0.4242, val_auc 0.4896, test auc 0.4656\n",
      "2022-06-30 00:59:23,311 - NCNet pretrain, Epoch [26 / 300]: loss 0.4538, training auc: 0.4699, val_auc 0.4913, test auc 0.4672\n",
      "2022-06-30 00:59:23,443 - NCNet pretrain, Epoch [27 / 300]: loss 0.4583, training auc: 0.4460, val_auc 0.4937, test auc 0.4694\n",
      "2022-06-30 00:59:23,575 - NCNet pretrain, Epoch [28 / 300]: loss 0.4488, training auc: 0.4876, val_auc 0.4954, test auc 0.4710\n",
      "2022-06-30 00:59:23,705 - NCNet pretrain, Epoch [29 / 300]: loss 0.4513, training auc: 0.4717, val_auc 0.4970, test auc 0.4723\n",
      "2022-06-30 00:59:23,859 - NCNet pretrain, Epoch [30 / 300]: loss 0.4556, training auc: 0.4455, val_auc 0.4984, test auc 0.4739\n",
      "2022-06-30 00:59:23,994 - NCNet pretrain, Epoch [31 / 300]: loss 0.4479, training auc: 0.4975, val_auc 0.4997, test auc 0.4753\n",
      "2022-06-30 00:59:24,125 - NCNet pretrain, Epoch [32 / 300]: loss 0.4482, training auc: 0.4767, val_auc 0.5014, test auc 0.4770\n",
      "2022-06-30 00:59:24,255 - NCNet pretrain, Epoch [33 / 300]: loss 0.4427, training auc: 0.4978, val_auc 0.5036, test auc 0.4793\n",
      "2022-06-30 00:59:24,386 - NCNet pretrain, Epoch [34 / 300]: loss 0.4445, training auc: 0.4754, val_auc 0.5063, test auc 0.4820\n",
      "2022-06-30 00:59:24,516 - NCNet pretrain, Epoch [35 / 300]: loss 0.4502, training auc: 0.4583, val_auc 0.5094, test auc 0.4852\n",
      "2022-06-30 00:59:24,647 - NCNet pretrain, Epoch [36 / 300]: loss 0.4483, training auc: 0.4633, val_auc 0.5129, test auc 0.4889\n",
      "2022-06-30 00:59:24,778 - NCNet pretrain, Epoch [37 / 300]: loss 0.4397, training auc: 0.5061, val_auc 0.5171, test auc 0.4928\n",
      "2022-06-30 00:59:24,926 - NCNet pretrain, Epoch [38 / 300]: loss 0.4451, training auc: 0.4847, val_auc 0.5214, test auc 0.4968\n",
      "2022-06-30 00:59:25,056 - NCNet pretrain, Epoch [39 / 300]: loss 0.4418, training auc: 0.4960, val_auc 0.5261, test auc 0.5012\n",
      "2022-06-30 00:59:25,190 - NCNet pretrain, Epoch [40 / 300]: loss 0.4394, training auc: 0.5129, val_auc 0.5302, test auc 0.5056\n",
      "2022-06-30 00:59:25,321 - NCNet pretrain, Epoch [41 / 300]: loss 0.4327, training auc: 0.5394, val_auc 0.5350, test auc 0.5105\n",
      "2022-06-30 00:59:25,452 - NCNet pretrain, Epoch [42 / 300]: loss 0.4401, training auc: 0.4914, val_auc 0.5423, test auc 0.5179\n",
      "2022-06-30 00:59:25,584 - NCNet pretrain, Epoch [43 / 300]: loss 0.4267, training auc: 0.5528, val_auc 0.5497, test auc 0.5259\n",
      "2022-06-30 00:59:25,716 - NCNet pretrain, Epoch [44 / 300]: loss 0.4321, training auc: 0.5404, val_auc 0.5585, test auc 0.5351\n",
      "2022-06-30 00:59:25,847 - NCNet pretrain, Epoch [45 / 300]: loss 0.4226, training auc: 0.5757, val_auc 0.5675, test auc 0.5444\n",
      "2022-06-30 00:59:25,979 - NCNet pretrain, Epoch [46 / 300]: loss 0.4302, training auc: 0.5430, val_auc 0.5754, test auc 0.5527\n",
      "2022-06-30 00:59:26,114 - NCNet pretrain, Epoch [47 / 300]: loss 0.4220, training auc: 0.5787, val_auc 0.5787, test auc 0.5551\n",
      "2022-06-30 00:59:26,244 - NCNet pretrain, Epoch [48 / 300]: loss 0.4277, training auc: 0.5583, val_auc 0.5879, test auc 0.5649\n",
      "2022-06-30 00:59:26,375 - NCNet pretrain, Epoch [49 / 300]: loss 0.4197, training auc: 0.5838, val_auc 0.6005, test auc 0.5788\n",
      "2022-06-30 00:59:26,505 - NCNet pretrain, Epoch [50 / 300]: loss 0.4231, training auc: 0.5764, val_auc 0.6130, test auc 0.5920\n",
      "2022-06-30 00:59:26,635 - NCNet pretrain, Epoch [51 / 300]: loss 0.4187, training auc: 0.6032, val_auc 0.6183, test auc 0.5962\n",
      "2022-06-30 00:59:26,766 - NCNet pretrain, Epoch [52 / 300]: loss 0.4163, training auc: 0.6012, val_auc 0.6205, test auc 0.5973\n",
      "2022-06-30 00:59:26,895 - NCNet pretrain, Epoch [53 / 300]: loss 0.4101, training auc: 0.6274, val_auc 0.6248, test auc 0.6002\n",
      "2022-06-30 00:59:27,026 - NCNet pretrain, Epoch [54 / 300]: loss 0.4106, training auc: 0.6153, val_auc 0.6382, test auc 0.6132\n",
      "2022-06-30 00:59:27,157 - NCNet pretrain, Epoch [55 / 300]: loss 0.4097, training auc: 0.6120, val_auc 0.6622, test auc 0.6385\n",
      "2022-06-30 00:59:27,307 - NCNet pretrain, Epoch [56 / 300]: loss 0.4089, training auc: 0.6444, val_auc 0.6743, test auc 0.6520\n",
      "2022-06-30 00:59:27,441 - NCNet pretrain, Epoch [57 / 300]: loss 0.4052, training auc: 0.6620, val_auc 0.6725\n",
      "2022-06-30 00:59:27,581 - NCNet pretrain, Epoch [58 / 300]: loss 0.4027, training auc: 0.6501, val_auc 0.6732\n",
      "2022-06-30 00:59:27,711 - NCNet pretrain, Epoch [59 / 300]: loss 0.3971, training auc: 0.6628, val_auc 0.6848, test auc 0.6603\n",
      "2022-06-30 00:59:27,845 - NCNet pretrain, Epoch [60 / 300]: loss 0.3978, training auc: 0.6693, val_auc 0.6990, test auc 0.6790\n",
      "2022-06-30 00:59:27,976 - NCNet pretrain, Epoch [61 / 300]: loss 0.3951, training auc: 0.6889, val_auc 0.7050, test auc 0.6865\n",
      "2022-06-30 00:59:28,100 - NCNet pretrain, Epoch [62 / 300]: loss 0.3962, training auc: 0.6855, val_auc 0.7019\n",
      "2022-06-30 00:59:28,229 - NCNet pretrain, Epoch [63 / 300]: loss 0.3807, training auc: 0.7043, val_auc 0.7016\n",
      "2022-06-30 00:59:28,358 - NCNet pretrain, Epoch [64 / 300]: loss 0.3923, training auc: 0.6759, val_auc 0.7145, test auc 0.6984\n",
      "2022-06-30 00:59:28,489 - NCNet pretrain, Epoch [65 / 300]: loss 0.3842, training auc: 0.7098, val_auc 0.7233, test auc 0.7107\n",
      "2022-06-30 00:59:28,608 - NCNet pretrain, Epoch [66 / 300]: loss 0.3840, training auc: 0.7171, val_auc 0.7163\n",
      "2022-06-30 00:59:28,727 - NCNet pretrain, Epoch [67 / 300]: loss 0.3820, training auc: 0.7119, val_auc 0.7152\n",
      "2022-06-30 00:59:28,858 - NCNet pretrain, Epoch [68 / 300]: loss 0.3767, training auc: 0.7182, val_auc 0.7265, test auc 0.7134\n",
      "2022-06-30 00:59:28,988 - NCNet pretrain, Epoch [69 / 300]: loss 0.3807, training auc: 0.7188, val_auc 0.7339, test auc 0.7241\n",
      "2022-06-30 00:59:29,108 - NCNet pretrain, Epoch [70 / 300]: loss 0.3764, training auc: 0.7337, val_auc 0.7296\n",
      "2022-06-30 00:59:29,230 - NCNet pretrain, Epoch [71 / 300]: loss 0.3738, training auc: 0.7299, val_auc 0.7286\n",
      "2022-06-30 00:59:29,361 - NCNet pretrain, Epoch [72 / 300]: loss 0.3788, training auc: 0.7160, val_auc 0.7422, test auc 0.7343\n",
      "2022-06-30 00:59:29,492 - NCNet pretrain, Epoch [73 / 300]: loss 0.3718, training auc: 0.7444, val_auc 0.7448, test auc 0.7375\n",
      "2022-06-30 00:59:29,612 - NCNet pretrain, Epoch [74 / 300]: loss 0.3755, training auc: 0.7321, val_auc 0.7351\n",
      "2022-06-30 00:59:29,732 - NCNet pretrain, Epoch [75 / 300]: loss 0.3701, training auc: 0.7340, val_auc 0.7364\n",
      "2022-06-30 00:59:29,865 - NCNet pretrain, Epoch [76 / 300]: loss 0.3715, training auc: 0.7289, val_auc 0.7531, test auc 0.7477\n",
      "2022-06-30 00:59:29,985 - NCNet pretrain, Epoch [77 / 300]: loss 0.3669, training auc: 0.7626, val_auc 0.7529\n",
      "2022-06-30 00:59:30,106 - NCNet pretrain, Epoch [78 / 300]: loss 0.3686, training auc: 0.7536, val_auc 0.7416\n",
      "2022-06-30 00:59:30,228 - NCNet pretrain, Epoch [79 / 300]: loss 0.3649, training auc: 0.7486, val_auc 0.7467\n",
      "2022-06-30 00:59:30,365 - NCNet pretrain, Epoch [80 / 300]: loss 0.3648, training auc: 0.7481, val_auc 0.7578, test auc 0.7529\n",
      "2022-06-30 00:59:30,494 - NCNet pretrain, Epoch [81 / 300]: loss 0.3664, training auc: 0.7584, val_auc 0.7578, test auc 0.7531\n",
      "2022-06-30 00:59:30,612 - NCNet pretrain, Epoch [82 / 300]: loss 0.3608, training auc: 0.7677, val_auc 0.7517\n",
      "2022-06-30 00:59:30,730 - NCNet pretrain, Epoch [83 / 300]: loss 0.3582, training auc: 0.7666, val_auc 0.7538\n",
      "2022-06-30 00:59:30,858 - NCNet pretrain, Epoch [84 / 300]: loss 0.3497, training auc: 0.7772, val_auc 0.7595, test auc 0.7554\n",
      "2022-06-30 00:59:30,987 - NCNet pretrain, Epoch [85 / 300]: loss 0.3557, training auc: 0.7710, val_auc 0.7638, test auc 0.7614\n",
      "2022-06-30 00:59:31,104 - NCNet pretrain, Epoch [86 / 300]: loss 0.3561, training auc: 0.7741, val_auc 0.7584\n",
      "2022-06-30 00:59:31,234 - NCNet pretrain, Epoch [87 / 300]: loss 0.3596, training auc: 0.7616, val_auc 0.7647, test auc 0.7630\n",
      "2022-06-30 00:59:31,362 - NCNet pretrain, Epoch [88 / 300]: loss 0.3470, training auc: 0.7855, val_auc 0.7684, test auc 0.7682\n",
      "2022-06-30 00:59:31,481 - NCNet pretrain, Epoch [89 / 300]: loss 0.3544, training auc: 0.7788, val_auc 0.7638\n",
      "2022-06-30 00:59:31,600 - NCNet pretrain, Epoch [90 / 300]: loss 0.3516, training auc: 0.7695, val_auc 0.7618\n",
      "2022-06-30 00:59:31,729 - NCNet pretrain, Epoch [91 / 300]: loss 0.3452, training auc: 0.7879, val_auc 0.7715, test auc 0.7739\n",
      "2022-06-30 00:59:31,847 - NCNet pretrain, Epoch [92 / 300]: loss 0.3444, training auc: 0.8059, val_auc 0.7640\n",
      "2022-06-30 00:59:31,965 - NCNet pretrain, Epoch [93 / 300]: loss 0.3430, training auc: 0.7903, val_auc 0.7609\n",
      "2022-06-30 00:59:32,086 - NCNet pretrain, Epoch [94 / 300]: loss 0.3489, training auc: 0.7819, val_auc 0.7713\n",
      "2022-06-30 00:59:32,204 - NCNet pretrain, Epoch [95 / 300]: loss 0.3434, training auc: 0.7986, val_auc 0.7663\n",
      "2022-06-30 00:59:32,325 - NCNet pretrain, Epoch [96 / 300]: loss 0.3395, training auc: 0.7999, val_auc 0.7613\n",
      "2022-06-30 00:59:32,451 - NCNet pretrain, Epoch [97 / 300]: loss 0.3394, training auc: 0.7982, val_auc 0.7654\n",
      "2022-06-30 00:59:32,568 - NCNet pretrain, Epoch [98 / 300]: loss 0.3404, training auc: 0.7921, val_auc 0.7689\n",
      "2022-06-30 00:59:32,705 - NCNet pretrain, Epoch [99 / 300]: loss 0.3359, training auc: 0.8084, val_auc 0.7654\n",
      "2022-06-30 00:59:32,823 - NCNet pretrain, Epoch [100 / 300]: loss 0.3321, training auc: 0.8077, val_auc 0.7657\n",
      "2022-06-30 00:59:32,940 - NCNet pretrain, Epoch [101 / 300]: loss 0.3480, training auc: 0.7893, val_auc 0.7701\n",
      "2022-06-30 00:59:33,056 - NCNet pretrain, Epoch [102 / 300]: loss 0.3403, training auc: 0.7991, val_auc 0.7667\n",
      "2022-06-30 00:59:33,173 - NCNet pretrain, Epoch [103 / 300]: loss 0.3320, training auc: 0.8056, val_auc 0.7675\n",
      "2022-06-30 00:59:33,301 - NCNet pretrain, Epoch [104 / 300]: loss 0.3380, training auc: 0.8007, val_auc 0.7738, test auc 0.7825\n",
      "2022-06-30 00:59:33,438 - NCNet pretrain, Epoch [105 / 300]: loss 0.3313, training auc: 0.8148, val_auc 0.7692\n",
      "2022-06-30 00:59:33,565 - NCNet pretrain, Epoch [106 / 300]: loss 0.3376, training auc: 0.8033, val_auc 0.7717\n",
      "2022-06-30 00:59:33,683 - NCNet pretrain, Epoch [107 / 300]: loss 0.3308, training auc: 0.8143, val_auc 0.7717\n",
      "2022-06-30 00:59:33,801 - NCNet pretrain, Epoch [108 / 300]: loss 0.3335, training auc: 0.8094, val_auc 0.7735\n",
      "2022-06-30 00:59:33,918 - NCNet pretrain, Epoch [109 / 300]: loss 0.3267, training auc: 0.8172, val_auc 0.7679\n",
      "2022-06-30 00:59:34,065 - NCNet pretrain, Epoch [110 / 300]: loss 0.3251, training auc: 0.8148, val_auc 0.7782, test auc 0.7892\n",
      "2022-06-30 00:59:34,204 - NCNet pretrain, Epoch [111 / 300]: loss 0.3433, training auc: 0.8124, val_auc 0.7687\n",
      "2022-06-30 00:59:34,340 - NCNet pretrain, Epoch [112 / 300]: loss 0.3333, training auc: 0.8038, val_auc 0.7721\n",
      "2022-06-30 00:59:34,467 - NCNet pretrain, Epoch [113 / 300]: loss 0.3300, training auc: 0.8098, val_auc 0.7764\n",
      "2022-06-30 00:59:34,588 - NCNet pretrain, Epoch [114 / 300]: loss 0.3324, training auc: 0.8150, val_auc 0.7722\n",
      "2022-06-30 00:59:34,707 - NCNet pretrain, Epoch [115 / 300]: loss 0.3287, training auc: 0.8186, val_auc 0.7751\n",
      "2022-06-30 00:59:34,822 - NCNet pretrain, Epoch [116 / 300]: loss 0.3352, training auc: 0.8123, val_auc 0.7684\n",
      "2022-06-30 00:59:34,937 - NCNet pretrain, Epoch [117 / 300]: loss 0.3240, training auc: 0.8212, val_auc 0.7774\n",
      "2022-06-30 00:59:35,053 - NCNet pretrain, Epoch [118 / 300]: loss 0.3247, training auc: 0.8247, val_auc 0.7709\n",
      "2022-06-30 00:59:35,168 - NCNet pretrain, Epoch [119 / 300]: loss 0.3131, training auc: 0.8356, val_auc 0.7686\n",
      "2022-06-30 00:59:35,314 - NCNet pretrain, Epoch [120 / 300]: loss 0.3219, training auc: 0.8270, val_auc 0.7790, test auc 0.7922\n",
      "2022-06-30 00:59:35,437 - NCNet pretrain, Epoch [121 / 300]: loss 0.3272, training auc: 0.8326, val_auc 0.7672\n",
      "2022-06-30 00:59:35,552 - NCNet pretrain, Epoch [122 / 300]: loss 0.3195, training auc: 0.8210, val_auc 0.7713\n",
      "2022-06-30 00:59:35,669 - NCNet pretrain, Epoch [123 / 300]: loss 0.3193, training auc: 0.8262, val_auc 0.7774\n",
      "2022-06-30 00:59:35,805 - NCNet pretrain, Epoch [124 / 300]: loss 0.3172, training auc: 0.8371, val_auc 0.7624\n",
      "2022-06-30 00:59:35,941 - NCNet pretrain, Epoch [125 / 300]: loss 0.3296, training auc: 0.8105, val_auc 0.7741\n",
      "2022-06-30 00:59:36,057 - NCNet pretrain, Epoch [126 / 300]: loss 0.3087, training auc: 0.8416, val_auc 0.7750\n",
      "2022-06-30 00:59:36,173 - NCNet pretrain, Epoch [127 / 300]: loss 0.3209, training auc: 0.8307, val_auc 0.7633\n",
      "2022-06-30 00:59:36,291 - NCNet pretrain, Epoch [128 / 300]: loss 0.3301, training auc: 0.8181, val_auc 0.7780\n",
      "2022-06-30 00:59:36,408 - NCNet pretrain, Epoch [129 / 300]: loss 0.3130, training auc: 0.8452, val_auc 0.7715\n",
      "2022-06-30 00:59:36,524 - NCNet pretrain, Epoch [130 / 300]: loss 0.3077, training auc: 0.8374, val_auc 0.7704\n",
      "2022-06-30 00:59:36,641 - NCNet pretrain, Epoch [131 / 300]: loss 0.3224, training auc: 0.8278, val_auc 0.7747\n",
      "2022-06-30 00:59:36,765 - NCNet pretrain, Epoch [132 / 300]: loss 0.3132, training auc: 0.8409, val_auc 0.7732\n",
      "2022-06-30 00:59:36,879 - NCNet pretrain, Epoch [133 / 300]: loss 0.3072, training auc: 0.8358, val_auc 0.7686\n",
      "2022-06-30 00:59:36,995 - NCNet pretrain, Epoch [134 / 300]: loss 0.3197, training auc: 0.8295, val_auc 0.7783\n",
      "2022-06-30 00:59:37,110 - NCNet pretrain, Epoch [135 / 300]: loss 0.3216, training auc: 0.8311, val_auc 0.7690\n",
      "2022-06-30 00:59:37,245 - NCNet pretrain, Epoch [136 / 300]: loss 0.3164, training auc: 0.8280, val_auc 0.7712\n",
      "2022-06-30 00:59:37,380 - NCNet pretrain, Epoch [137 / 300]: loss 0.3116, training auc: 0.8342, val_auc 0.7770\n",
      "2022-06-30 00:59:37,515 - NCNet pretrain, Epoch [138 / 300]: loss 0.3134, training auc: 0.8399, val_auc 0.7627\n",
      "2022-06-30 00:59:37,630 - NCNet pretrain, Epoch [139 / 300]: loss 0.3205, training auc: 0.8265, val_auc 0.7759\n",
      "2022-06-30 00:59:37,747 - NCNet pretrain, Epoch [140 / 300]: loss 0.3165, training auc: 0.8447, val_auc 0.7725\n",
      "2022-06-30 00:59:37,864 - NCNet pretrain, Epoch [141 / 300]: loss 0.3080, training auc: 0.8418, val_auc 0.7666\n",
      "2022-06-30 00:59:37,981 - NCNet pretrain, Epoch [142 / 300]: loss 0.3206, training auc: 0.8290, val_auc 0.7742\n",
      "2022-06-30 00:59:38,097 - NCNet pretrain, Epoch [143 / 300]: loss 0.3127, training auc: 0.8428, val_auc 0.7743\n",
      "2022-06-30 00:59:38,213 - NCNet pretrain, Epoch [144 / 300]: loss 0.3058, training auc: 0.8434, val_auc 0.7669\n",
      "2022-06-30 00:59:38,350 - NCNet pretrain, Epoch [145 / 300]: loss 0.3087, training auc: 0.8378, val_auc 0.7742\n",
      "2022-06-30 00:59:38,487 - NCNet pretrain, Epoch [146 / 300]: loss 0.3125, training auc: 0.8386, val_auc 0.7744\n",
      "2022-06-30 00:59:38,624 - NCNet pretrain, Epoch [147 / 300]: loss 0.3143, training auc: 0.8399, val_auc 0.7552\n",
      "2022-06-30 00:59:38,740 - NCNet pretrain, Epoch [148 / 300]: loss 0.3259, training auc: 0.8278, val_auc 0.7754\n",
      "2022-06-30 00:59:38,860 - NCNet pretrain, Epoch [149 / 300]: loss 0.3104, training auc: 0.8408, val_auc 0.7782\n",
      "2022-06-30 00:59:38,998 - NCNet pretrain, Epoch [150 / 300]: loss 0.3214, training auc: 0.8384, val_auc 0.7600\n",
      "2022-06-30 00:59:39,133 - NCNet pretrain, Epoch [151 / 300]: loss 0.3315, training auc: 0.8198, val_auc 0.7747\n",
      "2022-06-30 00:59:39,252 - NCNet pretrain, Epoch [152 / 300]: loss 0.3046, training auc: 0.8505, val_auc 0.7767\n",
      "2022-06-30 00:59:39,387 - NCNet pretrain, Epoch [153 / 300]: loss 0.3165, training auc: 0.8460, val_auc 0.7655\n",
      "2022-06-30 00:59:39,522 - NCNet pretrain, Epoch [154 / 300]: loss 0.3116, training auc: 0.8309, val_auc 0.7641\n",
      "2022-06-30 00:59:39,638 - NCNet pretrain, Epoch [155 / 300]: loss 0.3091, training auc: 0.8366, val_auc 0.7752\n",
      "2022-06-30 00:59:39,753 - NCNet pretrain, Epoch [156 / 300]: loss 0.3150, training auc: 0.8487, val_auc 0.7712\n",
      "2022-06-30 00:59:39,868 - NCNet pretrain, Epoch [157 / 300]: loss 0.3039, training auc: 0.8461, val_auc 0.7580\n",
      "2022-06-30 00:59:39,983 - NCNet pretrain, Epoch [158 / 300]: loss 0.3177, training auc: 0.8377, val_auc 0.7741\n",
      "2022-06-30 00:59:40,100 - NCNet pretrain, Epoch [159 / 300]: loss 0.3085, training auc: 0.8534, val_auc 0.7736\n",
      "2022-06-30 00:59:40,236 - NCNet pretrain, Epoch [160 / 300]: loss 0.3071, training auc: 0.8556, val_auc 0.7620\n",
      "2022-06-30 00:59:40,358 - NCNet pretrain, Epoch [161 / 300]: loss 0.2960, training auc: 0.8616, val_auc 0.7643\n",
      "2022-06-30 00:59:40,480 - NCNet pretrain, Epoch [162 / 300]: loss 0.3114, training auc: 0.8393, val_auc 0.7751\n",
      "2022-06-30 00:59:40,597 - NCNet pretrain, Epoch [163 / 300]: loss 0.3207, training auc: 0.8528, val_auc 0.7675\n",
      "2022-06-30 00:59:40,715 - NCNet pretrain, Epoch [164 / 300]: loss 0.3058, training auc: 0.8452, val_auc 0.7619\n",
      "2022-06-30 00:59:40,832 - NCNet pretrain, Epoch [165 / 300]: loss 0.3141, training auc: 0.8409, val_auc 0.7713\n",
      "2022-06-30 00:59:40,968 - NCNet pretrain, Epoch [166 / 300]: loss 0.3007, training auc: 0.8533, val_auc 0.7747\n",
      "2022-06-30 00:59:41,110 - NCNet pretrain, Epoch [167 / 300]: loss 0.3139, training auc: 0.8520, val_auc 0.7650\n",
      "2022-06-30 00:59:41,226 - NCNet pretrain, Epoch [168 / 300]: loss 0.2972, training auc: 0.8563, val_auc 0.7625\n",
      "2022-06-30 00:59:41,344 - NCNet pretrain, Epoch [169 / 300]: loss 0.3163, training auc: 0.8362, val_auc 0.7747\n",
      "2022-06-30 00:59:41,460 - NCNet pretrain, Epoch [170 / 300]: loss 0.3225, training auc: 0.8528, val_auc 0.7679\n",
      "2022-06-30 00:59:41,461 - Early stop!\n",
      "2022-06-30 00:59:41,462 - Best Test Results: auc 0.7922, ap 0.4088, f1 0.2590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 333916.42it/s]\n",
      "2022-06-30 00:59:42,414 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 00:59:42,592 - NCNet pretrain, Epoch [1 / 300]: loss 1.0669, training auc: 0.4832, val_auc 0.4591, test auc 0.4306\n",
      "2022-06-30 00:59:42,719 - NCNet pretrain, Epoch [2 / 300]: loss 0.7096, training auc: 0.4274, val_auc 0.4606, test auc 0.4315\n",
      "2022-06-30 00:59:42,841 - NCNet pretrain, Epoch [3 / 300]: loss 0.5268, training auc: 0.4285, val_auc 0.4599\n",
      "2022-06-30 00:59:42,959 - NCNet pretrain, Epoch [4 / 300]: loss 0.4927, training auc: 0.4583, val_auc 0.4602\n",
      "2022-06-30 00:59:43,082 - NCNet pretrain, Epoch [5 / 300]: loss 0.5578, training auc: 0.4245, val_auc 0.4609, test auc 0.4312\n",
      "2022-06-30 00:59:43,228 - NCNet pretrain, Epoch [6 / 300]: loss 0.5626, training auc: 0.4506, val_auc 0.4613, test auc 0.4321\n",
      "2022-06-30 00:59:43,379 - NCNet pretrain, Epoch [7 / 300]: loss 0.5585, training auc: 0.4112, val_auc 0.4623, test auc 0.4337\n",
      "2022-06-30 00:59:43,523 - NCNet pretrain, Epoch [8 / 300]: loss 0.5201, training auc: 0.4358, val_auc 0.4640, test auc 0.4361\n",
      "2022-06-30 00:59:43,650 - NCNet pretrain, Epoch [9 / 300]: loss 0.4895, training auc: 0.4434, val_auc 0.4659, test auc 0.4390\n",
      "2022-06-30 00:59:43,776 - NCNet pretrain, Epoch [10 / 300]: loss 0.4799, training auc: 0.4841, val_auc 0.4678, test auc 0.4418\n",
      "2022-06-30 00:59:43,902 - NCNet pretrain, Epoch [11 / 300]: loss 0.5033, training auc: 0.4321, val_auc 0.4696, test auc 0.4443\n",
      "2022-06-30 00:59:44,028 - NCNet pretrain, Epoch [12 / 300]: loss 0.4974, training auc: 0.4932, val_auc 0.4705, test auc 0.4455\n",
      "2022-06-30 00:59:44,154 - NCNet pretrain, Epoch [13 / 300]: loss 0.5079, training auc: 0.4060, val_auc 0.4713, test auc 0.4460\n",
      "2022-06-30 00:59:44,275 - NCNet pretrain, Epoch [14 / 300]: loss 0.4976, training auc: 0.4231, val_auc 0.4718, test auc 0.4459\n",
      "2022-06-30 00:59:44,399 - NCNet pretrain, Epoch [15 / 300]: loss 0.4888, training auc: 0.4490, val_auc 0.4724, test auc 0.4458\n",
      "2022-06-30 00:59:44,526 - NCNet pretrain, Epoch [16 / 300]: loss 0.5022, training auc: 0.4035, val_auc 0.4731, test auc 0.4463\n",
      "2022-06-30 00:59:44,652 - NCNet pretrain, Epoch [17 / 300]: loss 0.4958, training auc: 0.4318, val_auc 0.4741, test auc 0.4473\n",
      "2022-06-30 00:59:44,778 - NCNet pretrain, Epoch [18 / 300]: loss 0.4819, training auc: 0.4652, val_auc 0.4751, test auc 0.4488\n",
      "2022-06-30 00:59:44,905 - NCNet pretrain, Epoch [19 / 300]: loss 0.4805, training auc: 0.4760, val_auc 0.4764, test auc 0.4506\n",
      "2022-06-30 00:59:45,033 - NCNet pretrain, Epoch [20 / 300]: loss 0.4904, training auc: 0.4358, val_auc 0.4779, test auc 0.4527\n",
      "2022-06-30 00:59:45,163 - NCNet pretrain, Epoch [21 / 300]: loss 0.4778, training auc: 0.4756, val_auc 0.4795, test auc 0.4548\n",
      "2022-06-30 00:59:45,290 - NCNet pretrain, Epoch [22 / 300]: loss 0.4777, training auc: 0.4764, val_auc 0.4809, test auc 0.4563\n",
      "2022-06-30 00:59:45,438 - NCNet pretrain, Epoch [23 / 300]: loss 0.4896, training auc: 0.4239, val_auc 0.4821, test auc 0.4576\n",
      "2022-06-30 00:59:45,587 - NCNet pretrain, Epoch [24 / 300]: loss 0.4846, training auc: 0.4460, val_auc 0.4835, test auc 0.4588\n",
      "2022-06-30 00:59:45,713 - NCNet pretrain, Epoch [25 / 300]: loss 0.4865, training auc: 0.4239, val_auc 0.4847, test auc 0.4598\n",
      "2022-06-30 00:59:45,839 - NCNet pretrain, Epoch [26 / 300]: loss 0.4690, training auc: 0.4852, val_auc 0.4854, test auc 0.4605\n",
      "2022-06-30 00:59:45,966 - NCNet pretrain, Epoch [27 / 300]: loss 0.4727, training auc: 0.4766, val_auc 0.4864, test auc 0.4611\n",
      "2022-06-30 00:59:46,113 - NCNet pretrain, Epoch [28 / 300]: loss 0.4778, training auc: 0.4636, val_auc 0.4875, test auc 0.4619\n",
      "2022-06-30 00:59:46,258 - NCNet pretrain, Epoch [29 / 300]: loss 0.4732, training auc: 0.4634, val_auc 0.4884, test auc 0.4630\n",
      "2022-06-30 00:59:46,384 - NCNet pretrain, Epoch [30 / 300]: loss 0.4746, training auc: 0.4621, val_auc 0.4902, test auc 0.4646\n",
      "2022-06-30 00:59:46,511 - NCNet pretrain, Epoch [31 / 300]: loss 0.4743, training auc: 0.4664, val_auc 0.4920, test auc 0.4664\n",
      "2022-06-30 00:59:46,638 - NCNet pretrain, Epoch [32 / 300]: loss 0.4800, training auc: 0.4504, val_auc 0.4944, test auc 0.4689\n",
      "2022-06-30 00:59:46,765 - NCNet pretrain, Epoch [33 / 300]: loss 0.4824, training auc: 0.4298, val_auc 0.4971, test auc 0.4720\n",
      "2022-06-30 00:59:46,892 - NCNet pretrain, Epoch [34 / 300]: loss 0.4660, training auc: 0.5028, val_auc 0.4996, test auc 0.4748\n",
      "2022-06-30 00:59:47,019 - NCNet pretrain, Epoch [35 / 300]: loss 0.4621, training auc: 0.4947, val_auc 0.5020, test auc 0.4776\n",
      "2022-06-30 00:59:47,147 - NCNet pretrain, Epoch [36 / 300]: loss 0.4683, training auc: 0.4668, val_auc 0.5048, test auc 0.4802\n",
      "2022-06-30 00:59:47,275 - NCNet pretrain, Epoch [37 / 300]: loss 0.4653, training auc: 0.4859, val_auc 0.5071, test auc 0.4822\n",
      "2022-06-30 00:59:47,402 - NCNet pretrain, Epoch [38 / 300]: loss 0.4726, training auc: 0.4424, val_auc 0.5099, test auc 0.4846\n",
      "2022-06-30 00:59:47,533 - NCNet pretrain, Epoch [39 / 300]: loss 0.4544, training auc: 0.5150, val_auc 0.5124, test auc 0.4868\n",
      "2022-06-30 00:59:47,664 - NCNet pretrain, Epoch [40 / 300]: loss 0.4583, training auc: 0.4912, val_auc 0.5152, test auc 0.4898\n",
      "2022-06-30 00:59:47,790 - NCNet pretrain, Epoch [41 / 300]: loss 0.4569, training auc: 0.4955, val_auc 0.5187, test auc 0.4936\n",
      "2022-06-30 00:59:47,917 - NCNet pretrain, Epoch [42 / 300]: loss 0.4598, training auc: 0.5002, val_auc 0.5233, test auc 0.4984\n",
      "2022-06-30 00:59:48,043 - NCNet pretrain, Epoch [43 / 300]: loss 0.4543, training auc: 0.5078, val_auc 0.5282, test auc 0.5037\n",
      "2022-06-30 00:59:48,169 - NCNet pretrain, Epoch [44 / 300]: loss 0.4531, training auc: 0.5102, val_auc 0.5345, test auc 0.5102\n",
      "2022-06-30 00:59:48,314 - NCNet pretrain, Epoch [45 / 300]: loss 0.4439, training auc: 0.5384, val_auc 0.5399, test auc 0.5161\n",
      "2022-06-30 00:59:48,443 - NCNet pretrain, Epoch [46 / 300]: loss 0.4608, training auc: 0.4780, val_auc 0.5470, test auc 0.5238\n",
      "2022-06-30 00:59:48,570 - NCNet pretrain, Epoch [47 / 300]: loss 0.4441, training auc: 0.5576, val_auc 0.5532, test auc 0.5300\n",
      "2022-06-30 00:59:48,697 - NCNet pretrain, Epoch [48 / 300]: loss 0.4426, training auc: 0.5458, val_auc 0.5578, test auc 0.5345\n",
      "2022-06-30 00:59:48,842 - NCNet pretrain, Epoch [49 / 300]: loss 0.4339, training auc: 0.5827, val_auc 0.5615, test auc 0.5375\n",
      "2022-06-30 00:59:48,972 - NCNet pretrain, Epoch [50 / 300]: loss 0.4404, training auc: 0.5491, val_auc 0.5693, test auc 0.5456\n",
      "2022-06-30 00:59:49,118 - NCNet pretrain, Epoch [51 / 300]: loss 0.4320, training auc: 0.5766, val_auc 0.5804, test auc 0.5578\n",
      "2022-06-30 00:59:49,246 - NCNet pretrain, Epoch [52 / 300]: loss 0.4316, training auc: 0.5766, val_auc 0.5916, test auc 0.5705\n",
      "2022-06-30 00:59:49,373 - NCNet pretrain, Epoch [53 / 300]: loss 0.4293, training auc: 0.5757, val_auc 0.6029, test auc 0.5822\n",
      "2022-06-30 00:59:49,502 - NCNet pretrain, Epoch [54 / 300]: loss 0.4335, training auc: 0.5704, val_auc 0.6101, test auc 0.5889\n",
      "2022-06-30 00:59:49,628 - NCNet pretrain, Epoch [55 / 300]: loss 0.4260, training auc: 0.6148, val_auc 0.6106, test auc 0.5884\n",
      "2022-06-30 00:59:49,758 - NCNet pretrain, Epoch [56 / 300]: loss 0.4279, training auc: 0.5811, val_auc 0.6206, test auc 0.5983\n",
      "2022-06-30 00:59:49,884 - NCNet pretrain, Epoch [57 / 300]: loss 0.4266, training auc: 0.5969, val_auc 0.6347, test auc 0.6132\n",
      "2022-06-30 00:59:50,010 - NCNet pretrain, Epoch [58 / 300]: loss 0.4186, training auc: 0.6132, val_auc 0.6490, test auc 0.6291\n",
      "2022-06-30 00:59:50,137 - NCNet pretrain, Epoch [59 / 300]: loss 0.4145, training auc: 0.6474, val_auc 0.6532, test auc 0.6322\n",
      "2022-06-30 00:59:50,252 - NCNet pretrain, Epoch [60 / 300]: loss 0.4111, training auc: 0.6446, val_auc 0.6522\n",
      "2022-06-30 00:59:50,378 - NCNet pretrain, Epoch [61 / 300]: loss 0.4150, training auc: 0.6230, val_auc 0.6621, test auc 0.6399\n",
      "2022-06-30 00:59:50,506 - NCNet pretrain, Epoch [62 / 300]: loss 0.4048, training auc: 0.6634, val_auc 0.6739, test auc 0.6528\n",
      "2022-06-30 00:59:50,632 - NCNet pretrain, Epoch [63 / 300]: loss 0.4056, training auc: 0.6559, val_auc 0.6829, test auc 0.6633\n",
      "2022-06-30 00:59:50,748 - NCNet pretrain, Epoch [64 / 300]: loss 0.4069, training auc: 0.6693, val_auc 0.6826\n",
      "2022-06-30 00:59:50,875 - NCNet pretrain, Epoch [65 / 300]: loss 0.4081, training auc: 0.6657, val_auc 0.6881, test auc 0.6675\n",
      "2022-06-30 00:59:51,002 - NCNet pretrain, Epoch [66 / 300]: loss 0.4032, training auc: 0.6619, val_auc 0.6989, test auc 0.6817\n",
      "2022-06-30 00:59:51,129 - NCNet pretrain, Epoch [67 / 300]: loss 0.4007, training auc: 0.6845, val_auc 0.7067, test auc 0.6926\n",
      "2022-06-30 00:59:51,251 - NCNet pretrain, Epoch [68 / 300]: loss 0.3907, training auc: 0.7111, val_auc 0.7022\n",
      "2022-06-30 00:59:51,367 - NCNet pretrain, Epoch [69 / 300]: loss 0.3975, training auc: 0.6762, val_auc 0.7058\n",
      "2022-06-30 00:59:51,496 - NCNet pretrain, Epoch [70 / 300]: loss 0.3898, training auc: 0.6927, val_auc 0.7123, test auc 0.6994\n",
      "2022-06-30 00:59:51,623 - NCNet pretrain, Epoch [71 / 300]: loss 0.3803, training auc: 0.7156, val_auc 0.7188, test auc 0.7087\n",
      "2022-06-30 00:59:51,740 - NCNet pretrain, Epoch [72 / 300]: loss 0.3795, training auc: 0.7237, val_auc 0.7167\n",
      "2022-06-30 00:59:51,872 - NCNet pretrain, Epoch [73 / 300]: loss 0.3895, training auc: 0.7022, val_auc 0.7270, test auc 0.7192\n",
      "2022-06-30 00:59:51,993 - NCNet pretrain, Epoch [74 / 300]: loss 0.3704, training auc: 0.7463, val_auc 0.7272, test auc 0.7189\n",
      "2022-06-30 00:59:52,127 - NCNet pretrain, Epoch [75 / 300]: loss 0.3769, training auc: 0.7341, val_auc 0.7226\n",
      "2022-06-30 00:59:52,269 - NCNet pretrain, Epoch [76 / 300]: loss 0.3763, training auc: 0.7256, val_auc 0.7343, test auc 0.7275\n",
      "2022-06-30 00:59:52,395 - NCNet pretrain, Epoch [77 / 300]: loss 0.3787, training auc: 0.7282, val_auc 0.7419, test auc 0.7374\n",
      "2022-06-30 00:59:52,512 - NCNet pretrain, Epoch [78 / 300]: loss 0.3780, training auc: 0.7446, val_auc 0.7335\n",
      "2022-06-30 00:59:52,627 - NCNet pretrain, Epoch [79 / 300]: loss 0.3732, training auc: 0.7382, val_auc 0.7349\n",
      "2022-06-30 00:59:52,752 - NCNet pretrain, Epoch [80 / 300]: loss 0.3764, training auc: 0.7269, val_auc 0.7466, test auc 0.7430\n",
      "2022-06-30 00:59:52,888 - NCNet pretrain, Epoch [81 / 300]: loss 0.3668, training auc: 0.7637, val_auc 0.7446\n",
      "2022-06-30 00:59:53,008 - NCNet pretrain, Epoch [82 / 300]: loss 0.3648, training auc: 0.7597, val_auc 0.7401\n",
      "2022-06-30 00:59:53,124 - NCNet pretrain, Epoch [83 / 300]: loss 0.3725, training auc: 0.7427, val_auc 0.7464\n",
      "2022-06-30 00:59:53,250 - NCNet pretrain, Epoch [84 / 300]: loss 0.3549, training auc: 0.7718, val_auc 0.7493, test auc 0.7487\n",
      "2022-06-30 00:59:53,366 - NCNet pretrain, Epoch [85 / 300]: loss 0.3626, training auc: 0.7600, val_auc 0.7421\n",
      "2022-06-30 00:59:53,502 - NCNet pretrain, Epoch [86 / 300]: loss 0.3651, training auc: 0.7536, val_auc 0.7484\n",
      "2022-06-30 00:59:53,629 - NCNet pretrain, Epoch [87 / 300]: loss 0.3549, training auc: 0.7745, val_auc 0.7516, test auc 0.7542\n",
      "2022-06-30 00:59:53,745 - NCNet pretrain, Epoch [88 / 300]: loss 0.3545, training auc: 0.7793, val_auc 0.7481\n",
      "2022-06-30 00:59:53,865 - NCNet pretrain, Epoch [89 / 300]: loss 0.3528, training auc: 0.7691, val_auc 0.7439\n",
      "2022-06-30 00:59:53,997 - NCNet pretrain, Epoch [90 / 300]: loss 0.3431, training auc: 0.7832, val_auc 0.7517, test auc 0.7556\n",
      "2022-06-30 00:59:54,122 - NCNet pretrain, Epoch [91 / 300]: loss 0.3503, training auc: 0.7800, val_auc 0.7565, test auc 0.7628\n",
      "2022-06-30 00:59:54,256 - NCNet pretrain, Epoch [92 / 300]: loss 0.3419, training auc: 0.7966, val_auc 0.7516\n",
      "2022-06-30 00:59:54,386 - NCNet pretrain, Epoch [93 / 300]: loss 0.3555, training auc: 0.7771, val_auc 0.7538\n",
      "2022-06-30 00:59:54,515 - NCNet pretrain, Epoch [94 / 300]: loss 0.3407, training auc: 0.7888, val_auc 0.7627, test auc 0.7736\n",
      "2022-06-30 00:59:54,650 - NCNet pretrain, Epoch [95 / 300]: loss 0.3514, training auc: 0.7985, val_auc 0.7526\n",
      "2022-06-30 00:59:54,784 - NCNet pretrain, Epoch [96 / 300]: loss 0.3422, training auc: 0.7902, val_auc 0.7548\n",
      "2022-06-30 00:59:54,918 - NCNet pretrain, Epoch [97 / 300]: loss 0.3489, training auc: 0.7830, val_auc 0.7623\n",
      "2022-06-30 00:59:55,045 - NCNet pretrain, Epoch [98 / 300]: loss 0.3484, training auc: 0.7790, val_auc 0.7659, test auc 0.7790\n",
      "2022-06-30 00:59:55,160 - NCNet pretrain, Epoch [99 / 300]: loss 0.3479, training auc: 0.8086, val_auc 0.7539\n",
      "2022-06-30 00:59:55,276 - NCNet pretrain, Epoch [100 / 300]: loss 0.3528, training auc: 0.7831, val_auc 0.7592\n",
      "2022-06-30 00:59:55,422 - NCNet pretrain, Epoch [101 / 300]: loss 0.3581, training auc: 0.7644, val_auc 0.7664, test auc 0.7794\n",
      "2022-06-30 00:59:55,558 - NCNet pretrain, Epoch [102 / 300]: loss 0.3724, training auc: 0.7532, val_auc 0.7614\n",
      "2022-06-30 00:59:55,676 - NCNet pretrain, Epoch [103 / 300]: loss 0.3410, training auc: 0.8013, val_auc 0.7498\n",
      "2022-06-30 00:59:55,792 - NCNet pretrain, Epoch [104 / 300]: loss 0.3587, training auc: 0.7801, val_auc 0.7616\n",
      "2022-06-30 00:59:55,919 - NCNet pretrain, Epoch [105 / 300]: loss 0.3398, training auc: 0.8026, val_auc 0.7666, test auc 0.7814\n",
      "2022-06-30 00:59:56,040 - NCNet pretrain, Epoch [106 / 300]: loss 0.3491, training auc: 0.8186, val_auc 0.7593\n",
      "2022-06-30 00:59:56,155 - NCNet pretrain, Epoch [107 / 300]: loss 0.3466, training auc: 0.7866, val_auc 0.7541\n",
      "2022-06-30 00:59:56,271 - NCNet pretrain, Epoch [108 / 300]: loss 0.3389, training auc: 0.8026, val_auc 0.7604\n",
      "2022-06-30 00:59:56,386 - NCNet pretrain, Epoch [109 / 300]: loss 0.3321, training auc: 0.8086, val_auc 0.7648\n",
      "2022-06-30 00:59:56,502 - NCNet pretrain, Epoch [110 / 300]: loss 0.3352, training auc: 0.8141, val_auc 0.7630\n",
      "2022-06-30 00:59:56,617 - NCNet pretrain, Epoch [111 / 300]: loss 0.3352, training auc: 0.8075, val_auc 0.7585\n",
      "2022-06-30 00:59:56,732 - NCNet pretrain, Epoch [112 / 300]: loss 0.3383, training auc: 0.8009, val_auc 0.7589\n",
      "2022-06-30 00:59:56,847 - NCNet pretrain, Epoch [113 / 300]: loss 0.3355, training auc: 0.7902, val_auc 0.7649\n",
      "2022-06-30 00:59:56,962 - NCNet pretrain, Epoch [114 / 300]: loss 0.3434, training auc: 0.7926, val_auc 0.7661\n",
      "2022-06-30 00:59:57,077 - NCNet pretrain, Epoch [115 / 300]: loss 0.3408, training auc: 0.8101, val_auc 0.7614\n",
      "2022-06-30 00:59:57,193 - NCNet pretrain, Epoch [116 / 300]: loss 0.3314, training auc: 0.8081, val_auc 0.7614\n",
      "2022-06-30 00:59:57,320 - NCNet pretrain, Epoch [117 / 300]: loss 0.3354, training auc: 0.8063, val_auc 0.7671, test auc 0.7833\n",
      "2022-06-30 00:59:57,436 - NCNet pretrain, Epoch [118 / 300]: loss 0.3295, training auc: 0.8265, val_auc 0.7653\n",
      "2022-06-30 00:59:57,552 - NCNet pretrain, Epoch [119 / 300]: loss 0.3226, training auc: 0.8182, val_auc 0.7610\n",
      "2022-06-30 00:59:57,667 - NCNet pretrain, Epoch [120 / 300]: loss 0.3358, training auc: 0.8006, val_auc 0.7661\n",
      "2022-06-30 00:59:57,784 - NCNet pretrain, Epoch [121 / 300]: loss 0.3264, training auc: 0.8181, val_auc 0.7667\n",
      "2022-06-30 00:59:57,900 - NCNet pretrain, Epoch [122 / 300]: loss 0.3344, training auc: 0.8124, val_auc 0.7658\n",
      "2022-06-30 00:59:58,015 - NCNet pretrain, Epoch [123 / 300]: loss 0.3348, training auc: 0.8129, val_auc 0.7609\n",
      "2022-06-30 00:59:58,135 - NCNet pretrain, Epoch [124 / 300]: loss 0.3313, training auc: 0.8082, val_auc 0.7654\n",
      "2022-06-30 00:59:58,273 - NCNet pretrain, Epoch [125 / 300]: loss 0.3261, training auc: 0.8170, val_auc 0.7668\n",
      "2022-06-30 00:59:58,398 - NCNet pretrain, Epoch [126 / 300]: loss 0.3339, training auc: 0.8146, val_auc 0.7648\n",
      "2022-06-30 00:59:58,516 - NCNet pretrain, Epoch [127 / 300]: loss 0.3341, training auc: 0.8104, val_auc 0.7659\n",
      "2022-06-30 00:59:58,642 - NCNet pretrain, Epoch [128 / 300]: loss 0.3259, training auc: 0.8197, val_auc 0.7676, test auc 0.7849\n",
      "2022-06-30 00:59:58,776 - NCNet pretrain, Epoch [129 / 300]: loss 0.3300, training auc: 0.8180, val_auc 0.7630\n",
      "2022-06-30 00:59:58,890 - NCNet pretrain, Epoch [130 / 300]: loss 0.3298, training auc: 0.8144, val_auc 0.7637\n",
      "2022-06-30 00:59:59,016 - NCNet pretrain, Epoch [131 / 300]: loss 0.3303, training auc: 0.8137, val_auc 0.7678, test auc 0.7854\n",
      "2022-06-30 00:59:59,131 - NCNet pretrain, Epoch [132 / 300]: loss 0.3208, training auc: 0.8313, val_auc 0.7651\n",
      "2022-06-30 00:59:59,246 - NCNet pretrain, Epoch [133 / 300]: loss 0.3301, training auc: 0.8143, val_auc 0.7617\n",
      "2022-06-30 00:59:59,362 - NCNet pretrain, Epoch [134 / 300]: loss 0.3253, training auc: 0.8148, val_auc 0.7659\n",
      "2022-06-30 00:59:59,477 - NCNet pretrain, Epoch [135 / 300]: loss 0.3200, training auc: 0.8303, val_auc 0.7665\n",
      "2022-06-30 00:59:59,595 - NCNet pretrain, Epoch [136 / 300]: loss 0.3222, training auc: 0.8291, val_auc 0.7605\n",
      "2022-06-30 00:59:59,710 - NCNet pretrain, Epoch [137 / 300]: loss 0.3255, training auc: 0.8173, val_auc 0.7631\n",
      "2022-06-30 00:59:59,846 - NCNet pretrain, Epoch [138 / 300]: loss 0.3168, training auc: 0.8269, val_auc 0.7665\n",
      "2022-06-30 00:59:59,983 - NCNet pretrain, Epoch [139 / 300]: loss 0.3297, training auc: 0.8184, val_auc 0.7629\n",
      "2022-06-30 01:00:00,099 - NCNet pretrain, Epoch [140 / 300]: loss 0.3324, training auc: 0.8130, val_auc 0.7614\n",
      "2022-06-30 01:00:00,216 - NCNet pretrain, Epoch [141 / 300]: loss 0.3191, training auc: 0.8241, val_auc 0.7659\n",
      "2022-06-30 01:00:00,336 - NCNet pretrain, Epoch [142 / 300]: loss 0.3141, training auc: 0.8379, val_auc 0.7653\n",
      "2022-06-30 01:00:00,454 - NCNet pretrain, Epoch [143 / 300]: loss 0.3102, training auc: 0.8401, val_auc 0.7612\n",
      "2022-06-30 01:00:00,582 - NCNet pretrain, Epoch [144 / 300]: loss 0.3107, training auc: 0.8386, val_auc 0.7649\n",
      "2022-06-30 01:00:00,712 - NCNet pretrain, Epoch [145 / 300]: loss 0.3213, training auc: 0.8259, val_auc 0.7669\n",
      "2022-06-30 01:00:00,828 - NCNet pretrain, Epoch [146 / 300]: loss 0.3240, training auc: 0.8237, val_auc 0.7617\n",
      "2022-06-30 01:00:00,945 - NCNet pretrain, Epoch [147 / 300]: loss 0.3270, training auc: 0.8174, val_auc 0.7621\n",
      "2022-06-30 01:00:01,061 - NCNet pretrain, Epoch [148 / 300]: loss 0.3150, training auc: 0.8317, val_auc 0.7665\n",
      "2022-06-30 01:00:01,177 - NCNet pretrain, Epoch [149 / 300]: loss 0.3192, training auc: 0.8299, val_auc 0.7673\n",
      "2022-06-30 01:00:01,294 - NCNet pretrain, Epoch [150 / 300]: loss 0.3077, training auc: 0.8510, val_auc 0.7584\n",
      "2022-06-30 01:00:01,426 - NCNet pretrain, Epoch [151 / 300]: loss 0.3166, training auc: 0.8290, val_auc 0.7634\n",
      "2022-06-30 01:00:01,546 - NCNet pretrain, Epoch [152 / 300]: loss 0.3178, training auc: 0.8264, val_auc 0.7668\n",
      "2022-06-30 01:00:01,664 - NCNet pretrain, Epoch [153 / 300]: loss 0.3158, training auc: 0.8502, val_auc 0.7613\n",
      "2022-06-30 01:00:01,791 - NCNet pretrain, Epoch [154 / 300]: loss 0.3252, training auc: 0.8168, val_auc 0.7606\n",
      "2022-06-30 01:00:01,910 - NCNet pretrain, Epoch [155 / 300]: loss 0.3202, training auc: 0.8278, val_auc 0.7661\n",
      "2022-06-30 01:00:02,029 - NCNet pretrain, Epoch [156 / 300]: loss 0.3284, training auc: 0.8370, val_auc 0.7641\n",
      "2022-06-30 01:00:02,172 - NCNet pretrain, Epoch [157 / 300]: loss 0.3102, training auc: 0.8377, val_auc 0.7603\n",
      "2022-06-30 01:00:02,290 - NCNet pretrain, Epoch [158 / 300]: loss 0.3198, training auc: 0.8324, val_auc 0.7657\n",
      "2022-06-30 01:00:02,427 - NCNet pretrain, Epoch [159 / 300]: loss 0.3125, training auc: 0.8419, val_auc 0.7646\n",
      "2022-06-30 01:00:02,549 - NCNet pretrain, Epoch [160 / 300]: loss 0.3090, training auc: 0.8443, val_auc 0.7550\n",
      "2022-06-30 01:00:02,670 - NCNet pretrain, Epoch [161 / 300]: loss 0.3261, training auc: 0.8276, val_auc 0.7647\n",
      "2022-06-30 01:00:02,788 - NCNet pretrain, Epoch [162 / 300]: loss 0.3112, training auc: 0.8454, val_auc 0.7647\n",
      "2022-06-30 01:00:02,906 - NCNet pretrain, Epoch [163 / 300]: loss 0.3145, training auc: 0.8449, val_auc 0.7556\n",
      "2022-06-30 01:00:03,023 - NCNet pretrain, Epoch [164 / 300]: loss 0.3399, training auc: 0.8117, val_auc 0.7640\n",
      "2022-06-30 01:00:03,141 - NCNet pretrain, Epoch [165 / 300]: loss 0.3041, training auc: 0.8445, val_auc 0.7641\n",
      "2022-06-30 01:00:03,258 - NCNet pretrain, Epoch [166 / 300]: loss 0.3317, training auc: 0.8358, val_auc 0.7529\n",
      "2022-06-30 01:00:03,377 - NCNet pretrain, Epoch [167 / 300]: loss 0.3243, training auc: 0.8365, val_auc 0.7562\n",
      "2022-06-30 01:00:03,494 - NCNet pretrain, Epoch [168 / 300]: loss 0.3113, training auc: 0.8368, val_auc 0.7623\n",
      "2022-06-30 01:00:03,611 - NCNet pretrain, Epoch [169 / 300]: loss 0.3221, training auc: 0.8481, val_auc 0.7622\n",
      "2022-06-30 01:00:03,729 - NCNet pretrain, Epoch [170 / 300]: loss 0.3140, training auc: 0.8472, val_auc 0.7481\n",
      "2022-06-30 01:00:03,847 - NCNet pretrain, Epoch [171 / 300]: loss 0.3270, training auc: 0.8348, val_auc 0.7566\n",
      "2022-06-30 01:00:03,963 - NCNet pretrain, Epoch [172 / 300]: loss 0.3144, training auc: 0.8405, val_auc 0.7581\n",
      "2022-06-30 01:00:04,100 - NCNet pretrain, Epoch [173 / 300]: loss 0.3347, training auc: 0.8479, val_auc 0.7613\n",
      "2022-06-30 01:00:04,224 - NCNet pretrain, Epoch [174 / 300]: loss 0.3102, training auc: 0.8507, val_auc 0.7528\n",
      "2022-06-30 01:00:04,362 - NCNet pretrain, Epoch [175 / 300]: loss 0.3244, training auc: 0.8316, val_auc 0.7564\n",
      "2022-06-30 01:00:04,480 - NCNet pretrain, Epoch [176 / 300]: loss 0.3217, training auc: 0.8271, val_auc 0.7609\n",
      "2022-06-30 01:00:04,604 - NCNet pretrain, Epoch [177 / 300]: loss 0.3167, training auc: 0.8516, val_auc 0.7617\n",
      "2022-06-30 01:00:04,721 - NCNet pretrain, Epoch [178 / 300]: loss 0.3106, training auc: 0.8571, val_auc 0.7574\n",
      "2022-06-30 01:00:04,843 - NCNet pretrain, Epoch [179 / 300]: loss 0.3150, training auc: 0.8412, val_auc 0.7583\n",
      "2022-06-30 01:00:04,963 - NCNet pretrain, Epoch [180 / 300]: loss 0.3054, training auc: 0.8426, val_auc 0.7628\n",
      "2022-06-30 01:00:05,080 - NCNet pretrain, Epoch [181 / 300]: loss 0.3102, training auc: 0.8509, val_auc 0.7632\n",
      "2022-06-30 01:00:05,081 - Early stop!\n",
      "2022-06-30 01:00:05,082 - Best Test Results: auc 0.7854, ap 0.3898, f1 0.2412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 243787.61it/s]\n",
      "2022-06-30 01:00:06,231 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 01:00:06,411 - NCNet pretrain, Epoch [1 / 300]: loss 0.5515, training auc: 0.6080, val_auc 0.4643, test auc 0.4347\n",
      "2022-06-30 01:00:06,529 - NCNet pretrain, Epoch [2 / 300]: loss 0.4459, training auc: 0.4208, val_auc 0.4640\n",
      "2022-06-30 01:00:06,661 - NCNet pretrain, Epoch [3 / 300]: loss 0.4713, training auc: 0.4204, val_auc 0.4661, test auc 0.4389\n",
      "2022-06-30 01:00:06,791 - NCNet pretrain, Epoch [4 / 300]: loss 0.4615, training auc: 0.4288, val_auc 0.4691, test auc 0.4445\n",
      "2022-06-30 01:00:06,920 - NCNet pretrain, Epoch [5 / 300]: loss 0.4460, training auc: 0.4505, val_auc 0.4737, test auc 0.4531\n",
      "2022-06-30 01:00:07,054 - NCNet pretrain, Epoch [6 / 300]: loss 0.4428, training auc: 0.4526, val_auc 0.4806, test auc 0.4626\n",
      "2022-06-30 01:00:07,182 - NCNet pretrain, Epoch [7 / 300]: loss 0.4515, training auc: 0.4481, val_auc 0.4811, test auc 0.4630\n",
      "2022-06-30 01:00:07,298 - NCNet pretrain, Epoch [8 / 300]: loss 0.4466, training auc: 0.4565, val_auc 0.4792\n",
      "2022-06-30 01:00:07,417 - NCNet pretrain, Epoch [9 / 300]: loss 0.4438, training auc: 0.4516, val_auc 0.4778\n",
      "2022-06-30 01:00:07,536 - NCNet pretrain, Epoch [10 / 300]: loss 0.4385, training auc: 0.4785, val_auc 0.4774\n",
      "2022-06-30 01:00:07,666 - NCNet pretrain, Epoch [11 / 300]: loss 0.4334, training auc: 0.5154, val_auc 0.4769\n",
      "2022-06-30 01:00:07,789 - NCNet pretrain, Epoch [12 / 300]: loss 0.4368, training auc: 0.4979, val_auc 0.4769\n",
      "2022-06-30 01:00:07,908 - NCNet pretrain, Epoch [13 / 300]: loss 0.4351, training auc: 0.5026, val_auc 0.4775\n",
      "2022-06-30 01:00:08,044 - NCNet pretrain, Epoch [14 / 300]: loss 0.4471, training auc: 0.4182, val_auc 0.4794\n",
      "2022-06-30 01:00:08,176 - NCNet pretrain, Epoch [15 / 300]: loss 0.4359, training auc: 0.4841, val_auc 0.4813, test auc 0.4582\n",
      "2022-06-30 01:00:08,301 - NCNet pretrain, Epoch [16 / 300]: loss 0.4372, training auc: 0.4851, val_auc 0.4838, test auc 0.4607\n",
      "2022-06-30 01:00:08,426 - NCNet pretrain, Epoch [17 / 300]: loss 0.4370, training auc: 0.4942, val_auc 0.4864, test auc 0.4626\n",
      "2022-06-30 01:00:08,557 - NCNet pretrain, Epoch [18 / 300]: loss 0.4376, training auc: 0.4810, val_auc 0.4883, test auc 0.4637\n",
      "2022-06-30 01:00:08,692 - NCNet pretrain, Epoch [19 / 300]: loss 0.4463, training auc: 0.3785, val_auc 0.4903, test auc 0.4651\n",
      "2022-06-30 01:00:08,823 - NCNet pretrain, Epoch [20 / 300]: loss 0.4352, training auc: 0.4558, val_auc 0.4924, test auc 0.4668\n",
      "2022-06-30 01:00:08,952 - NCNet pretrain, Epoch [21 / 300]: loss 0.4431, training auc: 0.4069, val_auc 0.4958, test auc 0.4697\n",
      "2022-06-30 01:00:09,086 - NCNet pretrain, Epoch [22 / 300]: loss 0.4333, training auc: 0.4595, val_auc 0.5008, test auc 0.4743\n",
      "2022-06-30 01:00:09,235 - NCNet pretrain, Epoch [23 / 300]: loss 0.4389, training auc: 0.4292, val_auc 0.5075, test auc 0.4809\n",
      "2022-06-30 01:00:09,369 - NCNet pretrain, Epoch [24 / 300]: loss 0.4246, training auc: 0.5207, val_auc 0.5158, test auc 0.4892\n",
      "2022-06-30 01:00:09,499 - NCNet pretrain, Epoch [25 / 300]: loss 0.4381, training auc: 0.4283, val_auc 0.5279, test auc 0.5016\n",
      "2022-06-30 01:00:09,649 - NCNet pretrain, Epoch [26 / 300]: loss 0.4298, training auc: 0.4789, val_auc 0.5415, test auc 0.5152\n",
      "2022-06-30 01:00:09,799 - NCNet pretrain, Epoch [27 / 300]: loss 0.4271, training auc: 0.4912, val_auc 0.5537, test auc 0.5270\n",
      "2022-06-30 01:00:09,928 - NCNet pretrain, Epoch [28 / 300]: loss 0.4195, training auc: 0.5532, val_auc 0.5644, test auc 0.5371\n",
      "2022-06-30 01:00:10,057 - NCNet pretrain, Epoch [29 / 300]: loss 0.4186, training auc: 0.5487, val_auc 0.5852, test auc 0.5573\n",
      "2022-06-30 01:00:10,187 - NCNet pretrain, Epoch [30 / 300]: loss 0.4168, training auc: 0.5729, val_auc 0.6118, test auc 0.5831\n",
      "2022-06-30 01:00:10,317 - NCNet pretrain, Epoch [31 / 300]: loss 0.4104, training auc: 0.6020, val_auc 0.6236, test auc 0.5938\n",
      "2022-06-30 01:00:10,447 - NCNet pretrain, Epoch [32 / 300]: loss 0.4078, training auc: 0.6108, val_auc 0.6299, test auc 0.5994\n",
      "2022-06-30 01:00:10,578 - NCNet pretrain, Epoch [33 / 300]: loss 0.4114, training auc: 0.5903, val_auc 0.6489, test auc 0.6177\n",
      "2022-06-30 01:00:10,708 - NCNet pretrain, Epoch [34 / 300]: loss 0.4060, training auc: 0.6296, val_auc 0.6712, test auc 0.6406\n",
      "2022-06-30 01:00:10,840 - NCNet pretrain, Epoch [35 / 300]: loss 0.4039, training auc: 0.6418, val_auc 0.6806, test auc 0.6506\n",
      "2022-06-30 01:00:10,972 - NCNet pretrain, Epoch [36 / 300]: loss 0.3986, training auc: 0.6532, val_auc 0.6911, test auc 0.6622\n",
      "2022-06-30 01:00:11,103 - NCNet pretrain, Epoch [37 / 300]: loss 0.3956, training auc: 0.6624, val_auc 0.7197, test auc 0.6992\n",
      "2022-06-30 01:00:11,223 - NCNet pretrain, Epoch [38 / 300]: loss 0.3909, training auc: 0.7214, val_auc 0.7086\n",
      "2022-06-30 01:00:11,367 - NCNet pretrain, Epoch [39 / 300]: loss 0.3917, training auc: 0.6765, val_auc 0.7239, test auc 0.7060\n",
      "2022-06-30 01:00:11,497 - NCNet pretrain, Epoch [40 / 300]: loss 0.3774, training auc: 0.7275, val_auc 0.7288, test auc 0.7137\n",
      "2022-06-30 01:00:11,616 - NCNet pretrain, Epoch [41 / 300]: loss 0.3905, training auc: 0.6942, val_auc 0.7224\n",
      "2022-06-30 01:00:11,760 - NCNet pretrain, Epoch [42 / 300]: loss 0.3766, training auc: 0.7227, val_auc 0.7289, test auc 0.7137\n",
      "2022-06-30 01:00:11,910 - NCNet pretrain, Epoch [43 / 300]: loss 0.3962, training auc: 0.6624, val_auc 0.7448, test auc 0.7398\n",
      "2022-06-30 01:00:12,030 - NCNet pretrain, Epoch [44 / 300]: loss 0.3853, training auc: 0.7356, val_auc 0.7247\n",
      "2022-06-30 01:00:12,148 - NCNet pretrain, Epoch [45 / 300]: loss 0.3768, training auc: 0.7014, val_auc 0.7364\n",
      "2022-06-30 01:00:12,277 - NCNet pretrain, Epoch [46 / 300]: loss 0.3705, training auc: 0.7304, val_auc 0.7530, test auc 0.7512\n",
      "2022-06-30 01:00:12,410 - NCNet pretrain, Epoch [47 / 300]: loss 0.3706, training auc: 0.7536, val_auc 0.7419\n",
      "2022-06-30 01:00:12,528 - NCNet pretrain, Epoch [48 / 300]: loss 0.3650, training auc: 0.7412, val_auc 0.7380\n",
      "2022-06-30 01:00:12,648 - NCNet pretrain, Epoch [49 / 300]: loss 0.3690, training auc: 0.7357, val_auc 0.7520\n",
      "2022-06-30 01:00:12,792 - NCNet pretrain, Epoch [50 / 300]: loss 0.3584, training auc: 0.7628, val_auc 0.7581, test auc 0.7552\n",
      "2022-06-30 01:00:12,911 - NCNet pretrain, Epoch [51 / 300]: loss 0.3600, training auc: 0.7632, val_auc 0.7518\n",
      "2022-06-30 01:00:13,030 - NCNet pretrain, Epoch [52 / 300]: loss 0.3573, training auc: 0.7582, val_auc 0.7473\n",
      "2022-06-30 01:00:13,165 - NCNet pretrain, Epoch [53 / 300]: loss 0.3649, training auc: 0.7413, val_auc 0.7654, test auc 0.7638\n",
      "2022-06-30 01:00:13,284 - NCNet pretrain, Epoch [54 / 300]: loss 0.3605, training auc: 0.7691, val_auc 0.7591\n",
      "2022-06-30 01:00:13,403 - NCNet pretrain, Epoch [55 / 300]: loss 0.3519, training auc: 0.7750, val_auc 0.7550\n",
      "2022-06-30 01:00:13,539 - NCNet pretrain, Epoch [56 / 300]: loss 0.3458, training auc: 0.7682, val_auc 0.7669, test auc 0.7647\n",
      "2022-06-30 01:00:13,670 - NCNet pretrain, Epoch [57 / 300]: loss 0.3579, training auc: 0.7704, val_auc 0.7672, test auc 0.7650\n",
      "2022-06-30 01:00:13,789 - NCNet pretrain, Epoch [58 / 300]: loss 0.3455, training auc: 0.7804, val_auc 0.7535\n",
      "2022-06-30 01:00:13,919 - NCNet pretrain, Epoch [59 / 300]: loss 0.3541, training auc: 0.7586, val_auc 0.7675, test auc 0.7654\n",
      "2022-06-30 01:00:14,048 - NCNet pretrain, Epoch [60 / 300]: loss 0.3466, training auc: 0.7804, val_auc 0.7735, test auc 0.7743\n",
      "2022-06-30 01:00:14,168 - NCNet pretrain, Epoch [61 / 300]: loss 0.3499, training auc: 0.7904, val_auc 0.7573\n",
      "2022-06-30 01:00:14,287 - NCNet pretrain, Epoch [62 / 300]: loss 0.3497, training auc: 0.7692, val_auc 0.7615\n",
      "2022-06-30 01:00:14,417 - NCNet pretrain, Epoch [63 / 300]: loss 0.3404, training auc: 0.7936, val_auc 0.7792, test auc 0.7836\n",
      "2022-06-30 01:00:14,535 - NCNet pretrain, Epoch [64 / 300]: loss 0.3519, training auc: 0.8079, val_auc 0.7667\n",
      "2022-06-30 01:00:14,653 - NCNet pretrain, Epoch [65 / 300]: loss 0.3357, training auc: 0.7943, val_auc 0.7542\n",
      "2022-06-30 01:00:14,775 - NCNet pretrain, Epoch [66 / 300]: loss 0.3470, training auc: 0.7752, val_auc 0.7685\n",
      "2022-06-30 01:00:14,894 - NCNet pretrain, Epoch [67 / 300]: loss 0.3404, training auc: 0.7850, val_auc 0.7774\n",
      "2022-06-30 01:00:15,012 - NCNet pretrain, Epoch [68 / 300]: loss 0.3445, training auc: 0.7919, val_auc 0.7698\n",
      "2022-06-30 01:00:15,131 - NCNet pretrain, Epoch [69 / 300]: loss 0.3345, training auc: 0.7975, val_auc 0.7626\n",
      "2022-06-30 01:00:15,272 - NCNet pretrain, Epoch [70 / 300]: loss 0.3418, training auc: 0.7945, val_auc 0.7737\n",
      "2022-06-30 01:00:15,409 - NCNet pretrain, Epoch [71 / 300]: loss 0.3306, training auc: 0.8023, val_auc 0.7718\n",
      "2022-06-30 01:00:15,532 - NCNet pretrain, Epoch [72 / 300]: loss 0.3345, training auc: 0.8003, val_auc 0.7748\n",
      "2022-06-30 01:00:15,655 - NCNet pretrain, Epoch [73 / 300]: loss 0.3344, training auc: 0.8056, val_auc 0.7732\n",
      "2022-06-30 01:00:15,778 - NCNet pretrain, Epoch [74 / 300]: loss 0.3362, training auc: 0.7987, val_auc 0.7724\n",
      "2022-06-30 01:00:15,896 - NCNet pretrain, Epoch [75 / 300]: loss 0.3301, training auc: 0.8072, val_auc 0.7759\n",
      "2022-06-30 01:00:16,018 - NCNet pretrain, Epoch [76 / 300]: loss 0.3260, training auc: 0.8109, val_auc 0.7735\n",
      "2022-06-30 01:00:16,136 - NCNet pretrain, Epoch [77 / 300]: loss 0.3242, training auc: 0.8068, val_auc 0.7716\n",
      "2022-06-30 01:00:16,265 - NCNet pretrain, Epoch [78 / 300]: loss 0.3287, training auc: 0.8055, val_auc 0.7798, test auc 0.7863\n",
      "2022-06-30 01:00:16,382 - NCNet pretrain, Epoch [79 / 300]: loss 0.3259, training auc: 0.8226, val_auc 0.7774\n",
      "2022-06-30 01:00:16,499 - NCNet pretrain, Epoch [80 / 300]: loss 0.3274, training auc: 0.8082, val_auc 0.7711\n",
      "2022-06-30 01:00:16,628 - NCNet pretrain, Epoch [81 / 300]: loss 0.3227, training auc: 0.8159, val_auc 0.7824, test auc 0.7913\n",
      "2022-06-30 01:00:16,767 - NCNet pretrain, Epoch [82 / 300]: loss 0.3344, training auc: 0.8188, val_auc 0.7789\n",
      "2022-06-30 01:00:16,890 - NCNet pretrain, Epoch [83 / 300]: loss 0.3203, training auc: 0.8227, val_auc 0.7758\n",
      "2022-06-30 01:00:17,008 - NCNet pretrain, Epoch [84 / 300]: loss 0.3174, training auc: 0.8279, val_auc 0.7796\n",
      "2022-06-30 01:00:17,127 - NCNet pretrain, Epoch [85 / 300]: loss 0.3244, training auc: 0.8179, val_auc 0.7790\n",
      "2022-06-30 01:00:17,245 - NCNet pretrain, Epoch [86 / 300]: loss 0.3238, training auc: 0.8261, val_auc 0.7805\n",
      "2022-06-30 01:00:17,363 - NCNet pretrain, Epoch [87 / 300]: loss 0.3260, training auc: 0.8211, val_auc 0.7738\n",
      "2022-06-30 01:00:17,482 - NCNet pretrain, Epoch [88 / 300]: loss 0.3237, training auc: 0.8175, val_auc 0.7800\n",
      "2022-06-30 01:00:17,601 - NCNet pretrain, Epoch [89 / 300]: loss 0.3234, training auc: 0.8224, val_auc 0.7821\n",
      "2022-06-30 01:00:17,719 - NCNet pretrain, Epoch [90 / 300]: loss 0.3196, training auc: 0.8325, val_auc 0.7748\n",
      "2022-06-30 01:00:17,842 - NCNet pretrain, Epoch [91 / 300]: loss 0.3219, training auc: 0.8171, val_auc 0.7807\n",
      "2022-06-30 01:00:17,961 - NCNet pretrain, Epoch [92 / 300]: loss 0.3181, training auc: 0.8255, val_auc 0.7809\n",
      "2022-06-30 01:00:18,079 - NCNet pretrain, Epoch [93 / 300]: loss 0.3190, training auc: 0.8291, val_auc 0.7778\n",
      "2022-06-30 01:00:18,207 - NCNet pretrain, Epoch [94 / 300]: loss 0.3212, training auc: 0.8154, val_auc 0.7825, test auc 0.7949\n",
      "2022-06-30 01:00:18,324 - NCNet pretrain, Epoch [95 / 300]: loss 0.3142, training auc: 0.8324, val_auc 0.7771\n",
      "2022-06-30 01:00:18,441 - NCNet pretrain, Epoch [96 / 300]: loss 0.3110, training auc: 0.8340, val_auc 0.7814\n",
      "2022-06-30 01:00:18,559 - NCNet pretrain, Epoch [97 / 300]: loss 0.3176, training auc: 0.8270, val_auc 0.7760\n",
      "2022-06-30 01:00:18,678 - NCNet pretrain, Epoch [98 / 300]: loss 0.3228, training auc: 0.8214, val_auc 0.7814\n",
      "2022-06-30 01:00:18,827 - NCNet pretrain, Epoch [99 / 300]: loss 0.3113, training auc: 0.8350, val_auc 0.7842, test auc 0.7970\n",
      "2022-06-30 01:00:18,945 - NCNet pretrain, Epoch [100 / 300]: loss 0.3121, training auc: 0.8356, val_auc 0.7771\n",
      "2022-06-30 01:00:19,063 - NCNet pretrain, Epoch [101 / 300]: loss 0.3108, training auc: 0.8357, val_auc 0.7829\n",
      "2022-06-30 01:00:19,180 - NCNet pretrain, Epoch [102 / 300]: loss 0.3120, training auc: 0.8323, val_auc 0.7828\n",
      "2022-06-30 01:00:19,318 - NCNet pretrain, Epoch [103 / 300]: loss 0.3116, training auc: 0.8352, val_auc 0.7763\n",
      "2022-06-30 01:00:19,465 - NCNet pretrain, Epoch [104 / 300]: loss 0.3186, training auc: 0.8287, val_auc 0.7871, test auc 0.8041\n",
      "2022-06-30 01:00:19,588 - NCNet pretrain, Epoch [105 / 300]: loss 0.3148, training auc: 0.8422, val_auc 0.7668\n",
      "2022-06-30 01:00:19,706 - NCNet pretrain, Epoch [106 / 300]: loss 0.3270, training auc: 0.8252, val_auc 0.7849\n",
      "2022-06-30 01:00:19,827 - NCNet pretrain, Epoch [107 / 300]: loss 0.3066, training auc: 0.8514, val_auc 0.7819\n",
      "2022-06-30 01:00:19,950 - NCNet pretrain, Epoch [108 / 300]: loss 0.3024, training auc: 0.8394, val_auc 0.7744\n",
      "2022-06-30 01:00:20,073 - NCNet pretrain, Epoch [109 / 300]: loss 0.3056, training auc: 0.8378, val_auc 0.7857\n",
      "2022-06-30 01:00:20,191 - NCNet pretrain, Epoch [110 / 300]: loss 0.3173, training auc: 0.8442, val_auc 0.7804\n",
      "2022-06-30 01:00:20,329 - NCNet pretrain, Epoch [111 / 300]: loss 0.3020, training auc: 0.8444, val_auc 0.7791\n",
      "2022-06-30 01:00:20,446 - NCNet pretrain, Epoch [112 / 300]: loss 0.3019, training auc: 0.8446, val_auc 0.7836\n",
      "2022-06-30 01:00:20,584 - NCNet pretrain, Epoch [113 / 300]: loss 0.3043, training auc: 0.8425, val_auc 0.7790\n",
      "2022-06-30 01:00:20,719 - NCNet pretrain, Epoch [114 / 300]: loss 0.2991, training auc: 0.8435, val_auc 0.7771\n",
      "2022-06-30 01:00:20,839 - NCNet pretrain, Epoch [115 / 300]: loss 0.3057, training auc: 0.8362, val_auc 0.7855\n",
      "2022-06-30 01:00:20,977 - NCNet pretrain, Epoch [116 / 300]: loss 0.3047, training auc: 0.8606, val_auc 0.7693\n",
      "2022-06-30 01:00:21,096 - NCNet pretrain, Epoch [117 / 300]: loss 0.3007, training auc: 0.8499, val_auc 0.7804\n",
      "2022-06-30 01:00:21,214 - NCNet pretrain, Epoch [118 / 300]: loss 0.3026, training auc: 0.8499, val_auc 0.7854\n",
      "2022-06-30 01:00:21,335 - NCNet pretrain, Epoch [119 / 300]: loss 0.3124, training auc: 0.8528, val_auc 0.7606\n",
      "2022-06-30 01:00:21,454 - NCNet pretrain, Epoch [120 / 300]: loss 0.3212, training auc: 0.8301, val_auc 0.7811\n",
      "2022-06-30 01:00:21,573 - NCNet pretrain, Epoch [121 / 300]: loss 0.3000, training auc: 0.8541, val_auc 0.7850\n",
      "2022-06-30 01:00:21,693 - NCNet pretrain, Epoch [122 / 300]: loss 0.3069, training auc: 0.8619, val_auc 0.7675\n",
      "2022-06-30 01:00:21,814 - NCNet pretrain, Epoch [123 / 300]: loss 0.3092, training auc: 0.8378, val_auc 0.7762\n",
      "2022-06-30 01:00:21,932 - NCNet pretrain, Epoch [124 / 300]: loss 0.2942, training auc: 0.8578, val_auc 0.7862\n",
      "2022-06-30 01:00:22,055 - NCNet pretrain, Epoch [125 / 300]: loss 0.3161, training auc: 0.8766, val_auc 0.7635\n",
      "2022-06-30 01:00:22,177 - NCNet pretrain, Epoch [126 / 300]: loss 0.3227, training auc: 0.8364, val_auc 0.7681\n",
      "2022-06-30 01:00:22,296 - NCNet pretrain, Epoch [127 / 300]: loss 0.3044, training auc: 0.8384, val_auc 0.7857\n",
      "2022-06-30 01:00:22,414 - NCNet pretrain, Epoch [128 / 300]: loss 0.3115, training auc: 0.8642, val_auc 0.7816\n",
      "2022-06-30 01:00:22,550 - NCNet pretrain, Epoch [129 / 300]: loss 0.2968, training auc: 0.8583, val_auc 0.7710\n",
      "2022-06-30 01:00:22,668 - NCNet pretrain, Epoch [130 / 300]: loss 0.3089, training auc: 0.8332, val_auc 0.7779\n",
      "2022-06-30 01:00:22,785 - NCNet pretrain, Epoch [131 / 300]: loss 0.3009, training auc: 0.8462, val_auc 0.7850\n",
      "2022-06-30 01:00:22,922 - NCNet pretrain, Epoch [132 / 300]: loss 0.3003, training auc: 0.8663, val_auc 0.7816\n",
      "2022-06-30 01:00:23,039 - NCNet pretrain, Epoch [133 / 300]: loss 0.2998, training auc: 0.8595, val_auc 0.7738\n",
      "2022-06-30 01:00:23,157 - NCNet pretrain, Epoch [134 / 300]: loss 0.2989, training auc: 0.8432, val_auc 0.7790\n",
      "2022-06-30 01:00:23,275 - NCNet pretrain, Epoch [135 / 300]: loss 0.2950, training auc: 0.8554, val_auc 0.7847\n",
      "2022-06-30 01:00:23,394 - NCNet pretrain, Epoch [136 / 300]: loss 0.2999, training auc: 0.8599, val_auc 0.7808\n",
      "2022-06-30 01:00:23,512 - NCNet pretrain, Epoch [137 / 300]: loss 0.2939, training auc: 0.8554, val_auc 0.7760\n",
      "2022-06-30 01:00:23,631 - NCNet pretrain, Epoch [138 / 300]: loss 0.2931, training auc: 0.8549, val_auc 0.7806\n",
      "2022-06-30 01:00:23,751 - NCNet pretrain, Epoch [139 / 300]: loss 0.2873, training auc: 0.8625, val_auc 0.7831\n",
      "2022-06-30 01:00:23,871 - NCNet pretrain, Epoch [140 / 300]: loss 0.2950, training auc: 0.8579, val_auc 0.7787\n",
      "2022-06-30 01:00:23,989 - NCNet pretrain, Epoch [141 / 300]: loss 0.2910, training auc: 0.8560, val_auc 0.7768\n",
      "2022-06-30 01:00:24,109 - NCNet pretrain, Epoch [142 / 300]: loss 0.2894, training auc: 0.8569, val_auc 0.7829\n",
      "2022-06-30 01:00:24,235 - NCNet pretrain, Epoch [143 / 300]: loss 0.2992, training auc: 0.8631, val_auc 0.7818\n",
      "2022-06-30 01:00:24,358 - NCNet pretrain, Epoch [144 / 300]: loss 0.2866, training auc: 0.8672, val_auc 0.7790\n",
      "2022-06-30 01:00:24,481 - NCNet pretrain, Epoch [145 / 300]: loss 0.2903, training auc: 0.8566, val_auc 0.7800\n",
      "2022-06-30 01:00:24,600 - NCNet pretrain, Epoch [146 / 300]: loss 0.2939, training auc: 0.8538, val_auc 0.7806\n",
      "2022-06-30 01:00:24,717 - NCNet pretrain, Epoch [147 / 300]: loss 0.2857, training auc: 0.8683, val_auc 0.7782\n",
      "2022-06-30 01:00:24,838 - NCNet pretrain, Epoch [148 / 300]: loss 0.2875, training auc: 0.8624, val_auc 0.7770\n",
      "2022-06-30 01:00:24,955 - NCNet pretrain, Epoch [149 / 300]: loss 0.2865, training auc: 0.8626, val_auc 0.7792\n",
      "2022-06-30 01:00:25,074 - NCNet pretrain, Epoch [150 / 300]: loss 0.2918, training auc: 0.8611, val_auc 0.7802\n",
      "2022-06-30 01:00:25,191 - NCNet pretrain, Epoch [151 / 300]: loss 0.2838, training auc: 0.8636, val_auc 0.7763\n",
      "2022-06-30 01:00:25,310 - NCNet pretrain, Epoch [152 / 300]: loss 0.2859, training auc: 0.8589, val_auc 0.7809\n",
      "2022-06-30 01:00:25,429 - NCNet pretrain, Epoch [153 / 300]: loss 0.2855, training auc: 0.8684, val_auc 0.7815\n",
      "2022-06-30 01:00:25,547 - NCNet pretrain, Epoch [154 / 300]: loss 0.2906, training auc: 0.8635, val_auc 0.7762\n",
      "2022-06-30 01:00:25,548 - Early stop!\n",
      "2022-06-30 01:00:25,549 - Best Test Results: auc 0.8041, ap 0.4335, f1 0.3199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 246930.14it/s]\n",
      "2022-06-30 01:00:26,701 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 01:00:26,902 - NCNet pretrain, Epoch [1 / 300]: loss 0.5280, training auc: 0.5321, val_auc 0.4727, test auc 0.4479\n",
      "2022-06-30 01:00:27,038 - NCNet pretrain, Epoch [2 / 300]: loss 0.4405, training auc: 0.4445, val_auc 0.4699\n",
      "2022-06-30 01:00:27,174 - NCNet pretrain, Epoch [3 / 300]: loss 0.4598, training auc: 0.4399, val_auc 0.4718\n",
      "2022-06-30 01:00:27,303 - NCNet pretrain, Epoch [4 / 300]: loss 0.4596, training auc: 0.3961, val_auc 0.4755, test auc 0.4556\n",
      "2022-06-30 01:00:27,432 - NCNet pretrain, Epoch [5 / 300]: loss 0.4323, training auc: 0.4989, val_auc 0.4830, test auc 0.4647\n",
      "2022-06-30 01:00:27,562 - NCNet pretrain, Epoch [6 / 300]: loss 0.4320, training auc: 0.5248, val_auc 0.4867, test auc 0.4688\n",
      "2022-06-30 01:00:27,693 - NCNet pretrain, Epoch [7 / 300]: loss 0.4448, training auc: 0.4287, val_auc 0.4875, test auc 0.4694\n",
      "2022-06-30 01:00:27,829 - NCNet pretrain, Epoch [8 / 300]: loss 0.4374, training auc: 0.4970, val_auc 0.4876, test auc 0.4689\n",
      "2022-06-30 01:00:27,949 - NCNet pretrain, Epoch [9 / 300]: loss 0.4390, training auc: 0.4552, val_auc 0.4863\n",
      "2022-06-30 01:00:28,090 - NCNet pretrain, Epoch [10 / 300]: loss 0.4342, training auc: 0.4817, val_auc 0.4866\n",
      "2022-06-30 01:00:28,211 - NCNet pretrain, Epoch [11 / 300]: loss 0.4331, training auc: 0.4980, val_auc 0.4864\n",
      "2022-06-30 01:00:28,335 - NCNet pretrain, Epoch [12 / 300]: loss 0.4286, training auc: 0.5184, val_auc 0.4862\n",
      "2022-06-30 01:00:28,459 - NCNet pretrain, Epoch [13 / 300]: loss 0.4322, training auc: 0.4792, val_auc 0.4867\n",
      "2022-06-30 01:00:28,586 - NCNet pretrain, Epoch [14 / 300]: loss 0.4301, training auc: 0.4899, val_auc 0.4878, test auc 0.4629\n",
      "2022-06-30 01:00:28,712 - NCNet pretrain, Epoch [15 / 300]: loss 0.4414, training auc: 0.3924, val_auc 0.4895, test auc 0.4644\n",
      "2022-06-30 01:00:28,857 - NCNet pretrain, Epoch [16 / 300]: loss 0.4274, training auc: 0.5051, val_auc 0.4915, test auc 0.4662\n",
      "2022-06-30 01:00:28,987 - NCNet pretrain, Epoch [17 / 300]: loss 0.4330, training auc: 0.4578, val_auc 0.4964, test auc 0.4699\n",
      "2022-06-30 01:00:29,117 - NCNet pretrain, Epoch [18 / 300]: loss 0.4326, training auc: 0.4298, val_auc 0.5020, test auc 0.4757\n",
      "2022-06-30 01:00:29,251 - NCNet pretrain, Epoch [19 / 300]: loss 0.4258, training auc: 0.4950, val_auc 0.5083, test auc 0.4815\n",
      "2022-06-30 01:00:29,382 - NCNet pretrain, Epoch [20 / 300]: loss 0.4266, training auc: 0.4866, val_auc 0.5153, test auc 0.4883\n",
      "2022-06-30 01:00:29,512 - NCNet pretrain, Epoch [21 / 300]: loss 0.4239, training auc: 0.5084, val_auc 0.5249, test auc 0.4971\n",
      "2022-06-30 01:00:29,644 - NCNet pretrain, Epoch [22 / 300]: loss 0.4189, training auc: 0.5414, val_auc 0.5366, test auc 0.5082\n",
      "2022-06-30 01:00:29,775 - NCNet pretrain, Epoch [23 / 300]: loss 0.4241, training auc: 0.4944, val_auc 0.5554, test auc 0.5264\n",
      "2022-06-30 01:00:29,909 - NCNet pretrain, Epoch [24 / 300]: loss 0.4199, training auc: 0.5414, val_auc 0.5768, test auc 0.5468\n",
      "2022-06-30 01:00:30,040 - NCNet pretrain, Epoch [25 / 300]: loss 0.4169, training auc: 0.5752, val_auc 0.5911, test auc 0.5596\n",
      "2022-06-30 01:00:30,172 - NCNet pretrain, Epoch [26 / 300]: loss 0.4173, training auc: 0.5477, val_auc 0.5976, test auc 0.5653\n",
      "2022-06-30 01:00:30,303 - NCNet pretrain, Epoch [27 / 300]: loss 0.4122, training auc: 0.5900, val_auc 0.6105, test auc 0.5770\n",
      "2022-06-30 01:00:30,436 - NCNet pretrain, Epoch [28 / 300]: loss 0.4167, training auc: 0.5423, val_auc 0.6377, test auc 0.6018\n",
      "2022-06-30 01:00:30,572 - NCNet pretrain, Epoch [29 / 300]: loss 0.4047, training auc: 0.6224, val_auc 0.6635, test auc 0.6261\n",
      "2022-06-30 01:00:30,703 - NCNet pretrain, Epoch [30 / 300]: loss 0.4031, training auc: 0.6534, val_auc 0.6716, test auc 0.6343\n",
      "2022-06-30 01:00:30,823 - NCNet pretrain, Epoch [31 / 300]: loss 0.4029, training auc: 0.6465, val_auc 0.6699\n",
      "2022-06-30 01:00:30,954 - NCNet pretrain, Epoch [32 / 300]: loss 0.4001, training auc: 0.6326, val_auc 0.6955, test auc 0.6597\n",
      "2022-06-30 01:00:31,085 - NCNet pretrain, Epoch [33 / 300]: loss 0.3996, training auc: 0.6542, val_auc 0.7209, test auc 0.6897\n",
      "2022-06-30 01:00:31,215 - NCNet pretrain, Epoch [34 / 300]: loss 0.3980, training auc: 0.6458, val_auc 0.7364, test auc 0.7106\n",
      "2022-06-30 01:00:31,333 - NCNet pretrain, Epoch [35 / 300]: loss 0.3907, training auc: 0.7003, val_auc 0.7231\n",
      "2022-06-30 01:00:31,472 - NCNet pretrain, Epoch [36 / 300]: loss 0.3818, training auc: 0.7080, val_auc 0.7310\n",
      "2022-06-30 01:00:31,621 - NCNet pretrain, Epoch [37 / 300]: loss 0.3861, training auc: 0.6963, val_auc 0.7498, test auc 0.7318\n",
      "2022-06-30 01:00:31,742 - NCNet pretrain, Epoch [38 / 300]: loss 0.3807, training auc: 0.7289, val_auc 0.7389\n",
      "2022-06-30 01:00:31,875 - NCNet pretrain, Epoch [39 / 300]: loss 0.3760, training auc: 0.7186, val_auc 0.7528, test auc 0.7372\n",
      "2022-06-30 01:00:31,995 - NCNet pretrain, Epoch [40 / 300]: loss 0.3734, training auc: 0.7432, val_auc 0.7488\n",
      "2022-06-30 01:00:32,126 - NCNet pretrain, Epoch [41 / 300]: loss 0.3715, training auc: 0.7288, val_auc 0.7562, test auc 0.7419\n",
      "2022-06-30 01:00:32,247 - NCNet pretrain, Epoch [42 / 300]: loss 0.3652, training auc: 0.7688, val_auc 0.7560\n",
      "2022-06-30 01:00:32,378 - NCNet pretrain, Epoch [43 / 300]: loss 0.3777, training auc: 0.7227, val_auc 0.7592, test auc 0.7472\n",
      "2022-06-30 01:00:32,498 - NCNet pretrain, Epoch [44 / 300]: loss 0.3612, training auc: 0.7549, val_auc 0.7558\n",
      "2022-06-30 01:00:32,633 - NCNet pretrain, Epoch [45 / 300]: loss 0.3683, training auc: 0.7481, val_auc 0.7639, test auc 0.7549\n",
      "2022-06-30 01:00:32,763 - NCNet pretrain, Epoch [46 / 300]: loss 0.3583, training auc: 0.7531, val_auc 0.7639, test auc 0.7547\n",
      "2022-06-30 01:00:32,883 - NCNet pretrain, Epoch [47 / 300]: loss 0.3569, training auc: 0.7659, val_auc 0.7576\n",
      "2022-06-30 01:00:33,014 - NCNet pretrain, Epoch [48 / 300]: loss 0.3565, training auc: 0.7621, val_auc 0.7734, test auc 0.7770\n",
      "2022-06-30 01:00:33,144 - NCNet pretrain, Epoch [49 / 300]: loss 0.3649, training auc: 0.7734, val_auc 0.7618\n",
      "2022-06-30 01:00:33,263 - NCNet pretrain, Epoch [50 / 300]: loss 0.3499, training auc: 0.7604, val_auc 0.7617\n",
      "2022-06-30 01:00:33,393 - NCNet pretrain, Epoch [51 / 300]: loss 0.3619, training auc: 0.7527, val_auc 0.7752, test auc 0.7774\n",
      "2022-06-30 01:00:33,517 - NCNet pretrain, Epoch [52 / 300]: loss 0.3464, training auc: 0.7905, val_auc 0.7759, test auc 0.7774\n",
      "2022-06-30 01:00:33,636 - NCNet pretrain, Epoch [53 / 300]: loss 0.3500, training auc: 0.7783, val_auc 0.7611\n",
      "2022-06-30 01:00:33,756 - NCNet pretrain, Epoch [54 / 300]: loss 0.3492, training auc: 0.7703, val_auc 0.7696\n",
      "2022-06-30 01:00:33,888 - NCNet pretrain, Epoch [55 / 300]: loss 0.3495, training auc: 0.7818, val_auc 0.7839, test auc 0.7893\n",
      "2022-06-30 01:00:34,007 - NCNet pretrain, Epoch [56 / 300]: loss 0.3500, training auc: 0.8043, val_auc 0.7795\n",
      "2022-06-30 01:00:34,126 - NCNet pretrain, Epoch [57 / 300]: loss 0.3475, training auc: 0.7798, val_auc 0.7635\n",
      "2022-06-30 01:00:34,246 - NCNet pretrain, Epoch [58 / 300]: loss 0.3554, training auc: 0.7707, val_auc 0.7729\n",
      "2022-06-30 01:00:34,377 - NCNet pretrain, Epoch [59 / 300]: loss 0.3468, training auc: 0.7826, val_auc 0.7865, test auc 0.7931\n",
      "2022-06-30 01:00:34,497 - NCNet pretrain, Epoch [60 / 300]: loss 0.3499, training auc: 0.8142, val_auc 0.7817\n",
      "2022-06-30 01:00:34,616 - NCNet pretrain, Epoch [61 / 300]: loss 0.3374, training auc: 0.8049, val_auc 0.7709\n",
      "2022-06-30 01:00:34,736 - NCNet pretrain, Epoch [62 / 300]: loss 0.3462, training auc: 0.7744, val_auc 0.7744\n",
      "2022-06-30 01:00:34,861 - NCNet pretrain, Epoch [63 / 300]: loss 0.3318, training auc: 0.8001, val_auc 0.7822\n",
      "2022-06-30 01:00:34,979 - NCNet pretrain, Epoch [64 / 300]: loss 0.3370, training auc: 0.8027, val_auc 0.7836\n",
      "2022-06-30 01:00:35,097 - NCNet pretrain, Epoch [65 / 300]: loss 0.3390, training auc: 0.8027, val_auc 0.7793\n",
      "2022-06-30 01:00:35,223 - NCNet pretrain, Epoch [66 / 300]: loss 0.3372, training auc: 0.7927, val_auc 0.7777\n",
      "2022-06-30 01:00:35,340 - NCNet pretrain, Epoch [67 / 300]: loss 0.3336, training auc: 0.8069, val_auc 0.7825\n",
      "2022-06-30 01:00:35,458 - NCNet pretrain, Epoch [68 / 300]: loss 0.3325, training auc: 0.8148, val_auc 0.7850\n",
      "2022-06-30 01:00:35,579 - NCNet pretrain, Epoch [69 / 300]: loss 0.3392, training auc: 0.7928, val_auc 0.7823\n",
      "2022-06-30 01:00:35,716 - NCNet pretrain, Epoch [70 / 300]: loss 0.3344, training auc: 0.8067, val_auc 0.7786\n",
      "2022-06-30 01:00:35,834 - NCNet pretrain, Epoch [71 / 300]: loss 0.3375, training auc: 0.7971, val_auc 0.7840\n",
      "2022-06-30 01:00:35,966 - NCNet pretrain, Epoch [72 / 300]: loss 0.3277, training auc: 0.8065, val_auc 0.7874, test auc 0.7942\n",
      "2022-06-30 01:00:36,084 - NCNet pretrain, Epoch [73 / 300]: loss 0.3306, training auc: 0.8180, val_auc 0.7859\n",
      "2022-06-30 01:00:36,203 - NCNet pretrain, Epoch [74 / 300]: loss 0.3178, training auc: 0.8308, val_auc 0.7829\n",
      "2022-06-30 01:00:36,322 - NCNet pretrain, Epoch [75 / 300]: loss 0.3267, training auc: 0.8158, val_auc 0.7843\n",
      "2022-06-30 01:00:36,451 - NCNet pretrain, Epoch [76 / 300]: loss 0.3347, training auc: 0.8023, val_auc 0.7877, test auc 0.7953\n",
      "2022-06-30 01:00:36,583 - NCNet pretrain, Epoch [77 / 300]: loss 0.3270, training auc: 0.8120, val_auc 0.7881, test auc 0.7972\n",
      "2022-06-30 01:00:36,703 - NCNet pretrain, Epoch [78 / 300]: loss 0.3219, training auc: 0.8289, val_auc 0.7850\n",
      "2022-06-30 01:00:36,822 - NCNet pretrain, Epoch [79 / 300]: loss 0.3242, training auc: 0.8205, val_auc 0.7857\n",
      "2022-06-30 01:00:36,957 - NCNet pretrain, Epoch [80 / 300]: loss 0.3279, training auc: 0.8163, val_auc 0.7893, test auc 0.7996\n",
      "2022-06-30 01:00:37,088 - NCNet pretrain, Epoch [81 / 300]: loss 0.3218, training auc: 0.8202, val_auc 0.7895, test auc 0.7997\n",
      "2022-06-30 01:00:37,206 - NCNet pretrain, Epoch [82 / 300]: loss 0.3278, training auc: 0.8220, val_auc 0.7864\n",
      "2022-06-30 01:00:37,324 - NCNet pretrain, Epoch [83 / 300]: loss 0.3179, training auc: 0.8254, val_auc 0.7876\n",
      "2022-06-30 01:00:37,453 - NCNet pretrain, Epoch [84 / 300]: loss 0.3209, training auc: 0.8209, val_auc 0.7895, test auc 0.8006\n",
      "2022-06-30 01:00:37,602 - NCNet pretrain, Epoch [85 / 300]: loss 0.3230, training auc: 0.8277, val_auc 0.7901, test auc 0.8017\n",
      "2022-06-30 01:00:37,721 - NCNet pretrain, Epoch [86 / 300]: loss 0.3197, training auc: 0.8245, val_auc 0.7867\n",
      "2022-06-30 01:00:37,839 - NCNet pretrain, Epoch [87 / 300]: loss 0.3176, training auc: 0.8269, val_auc 0.7888\n",
      "2022-06-30 01:00:37,968 - NCNet pretrain, Epoch [88 / 300]: loss 0.3133, training auc: 0.8415, val_auc 0.7906, test auc 0.8027\n",
      "2022-06-30 01:00:38,086 - NCNet pretrain, Epoch [89 / 300]: loss 0.3201, training auc: 0.8276, val_auc 0.7863\n",
      "2022-06-30 01:00:38,216 - NCNet pretrain, Epoch [90 / 300]: loss 0.3185, training auc: 0.8260, val_auc 0.7932, test auc 0.8062\n",
      "2022-06-30 01:00:38,335 - NCNet pretrain, Epoch [91 / 300]: loss 0.3190, training auc: 0.8357, val_auc 0.7889\n",
      "2022-06-30 01:00:38,453 - NCNet pretrain, Epoch [92 / 300]: loss 0.3178, training auc: 0.8262, val_auc 0.7871\n",
      "2022-06-30 01:00:38,572 - NCNet pretrain, Epoch [93 / 300]: loss 0.3223, training auc: 0.8209, val_auc 0.7931\n",
      "2022-06-30 01:00:38,699 - NCNet pretrain, Epoch [94 / 300]: loss 0.3170, training auc: 0.8369, val_auc 0.7888\n",
      "2022-06-30 01:00:38,819 - NCNet pretrain, Epoch [95 / 300]: loss 0.3104, training auc: 0.8341, val_auc 0.7872\n",
      "2022-06-30 01:00:38,940 - NCNet pretrain, Epoch [96 / 300]: loss 0.3134, training auc: 0.8287, val_auc 0.7913\n",
      "2022-06-30 01:00:39,063 - NCNet pretrain, Epoch [97 / 300]: loss 0.3130, training auc: 0.8360, val_auc 0.7903\n",
      "2022-06-30 01:00:39,203 - NCNet pretrain, Epoch [98 / 300]: loss 0.3129, training auc: 0.8322, val_auc 0.7860\n",
      "2022-06-30 01:00:39,322 - NCNet pretrain, Epoch [99 / 300]: loss 0.3114, training auc: 0.8332, val_auc 0.7916\n",
      "2022-06-30 01:00:39,439 - NCNet pretrain, Epoch [100 / 300]: loss 0.3121, training auc: 0.8415, val_auc 0.7900\n",
      "2022-06-30 01:00:39,557 - NCNet pretrain, Epoch [101 / 300]: loss 0.3052, training auc: 0.8462, val_auc 0.7882\n",
      "2022-06-30 01:00:39,676 - NCNet pretrain, Epoch [102 / 300]: loss 0.3082, training auc: 0.8356, val_auc 0.7908\n",
      "2022-06-30 01:00:39,793 - NCNet pretrain, Epoch [103 / 300]: loss 0.3015, training auc: 0.8517, val_auc 0.7905\n",
      "2022-06-30 01:00:39,912 - NCNet pretrain, Epoch [104 / 300]: loss 0.3021, training auc: 0.8470, val_auc 0.7891\n",
      "2022-06-30 01:00:40,030 - NCNet pretrain, Epoch [105 / 300]: loss 0.3112, training auc: 0.8368, val_auc 0.7892\n",
      "2022-06-30 01:00:40,170 - NCNet pretrain, Epoch [106 / 300]: loss 0.3040, training auc: 0.8388, val_auc 0.7897\n",
      "2022-06-30 01:00:40,289 - NCNet pretrain, Epoch [107 / 300]: loss 0.3073, training auc: 0.8390, val_auc 0.7899\n",
      "2022-06-30 01:00:40,408 - NCNet pretrain, Epoch [108 / 300]: loss 0.2996, training auc: 0.8484, val_auc 0.7889\n",
      "2022-06-30 01:00:40,547 - NCNet pretrain, Epoch [109 / 300]: loss 0.3060, training auc: 0.8453, val_auc 0.7892\n",
      "2022-06-30 01:00:40,684 - NCNet pretrain, Epoch [110 / 300]: loss 0.3002, training auc: 0.8471, val_auc 0.7906\n",
      "2022-06-30 01:00:40,805 - NCNet pretrain, Epoch [111 / 300]: loss 0.2985, training auc: 0.8557, val_auc 0.7882\n",
      "2022-06-30 01:00:40,924 - NCNet pretrain, Epoch [112 / 300]: loss 0.3039, training auc: 0.8480, val_auc 0.7897\n",
      "2022-06-30 01:00:41,044 - NCNet pretrain, Epoch [113 / 300]: loss 0.2989, training auc: 0.8522, val_auc 0.7887\n",
      "2022-06-30 01:00:41,169 - NCNet pretrain, Epoch [114 / 300]: loss 0.3013, training auc: 0.8445, val_auc 0.7883\n",
      "2022-06-30 01:00:41,288 - NCNet pretrain, Epoch [115 / 300]: loss 0.3014, training auc: 0.8467, val_auc 0.7892\n",
      "2022-06-30 01:00:41,407 - NCNet pretrain, Epoch [116 / 300]: loss 0.2933, training auc: 0.8572, val_auc 0.7883\n",
      "2022-06-30 01:00:41,525 - NCNet pretrain, Epoch [117 / 300]: loss 0.3005, training auc: 0.8469, val_auc 0.7896\n",
      "2022-06-30 01:00:41,642 - NCNet pretrain, Epoch [118 / 300]: loss 0.3022, training auc: 0.8535, val_auc 0.7878\n",
      "2022-06-30 01:00:41,761 - NCNet pretrain, Epoch [119 / 300]: loss 0.2934, training auc: 0.8586, val_auc 0.7913\n",
      "2022-06-30 01:00:41,879 - NCNet pretrain, Epoch [120 / 300]: loss 0.3104, training auc: 0.8577, val_auc 0.7752\n",
      "2022-06-30 01:00:41,998 - NCNet pretrain, Epoch [121 / 300]: loss 0.3330, training auc: 0.8300, val_auc 0.7913\n",
      "2022-06-30 01:00:42,116 - NCNet pretrain, Epoch [122 / 300]: loss 0.2973, training auc: 0.8666, val_auc 0.7904\n",
      "2022-06-30 01:00:42,234 - NCNet pretrain, Epoch [123 / 300]: loss 0.2936, training auc: 0.8558, val_auc 0.7847\n",
      "2022-06-30 01:00:42,353 - NCNet pretrain, Epoch [124 / 300]: loss 0.2993, training auc: 0.8537, val_auc 0.7903\n",
      "2022-06-30 01:00:42,471 - NCNet pretrain, Epoch [125 / 300]: loss 0.2974, training auc: 0.8578, val_auc 0.7893\n",
      "2022-06-30 01:00:42,590 - NCNet pretrain, Epoch [126 / 300]: loss 0.2883, training auc: 0.8638, val_auc 0.7886\n",
      "2022-06-30 01:00:42,709 - NCNet pretrain, Epoch [127 / 300]: loss 0.2893, training auc: 0.8631, val_auc 0.7888\n",
      "2022-06-30 01:00:42,829 - NCNet pretrain, Epoch [128 / 300]: loss 0.2946, training auc: 0.8557, val_auc 0.7896\n",
      "2022-06-30 01:00:42,948 - NCNet pretrain, Epoch [129 / 300]: loss 0.3025, training auc: 0.8551, val_auc 0.7866\n",
      "2022-06-30 01:00:43,068 - NCNet pretrain, Epoch [130 / 300]: loss 0.2947, training auc: 0.8569, val_auc 0.7883\n",
      "2022-06-30 01:00:43,206 - NCNet pretrain, Epoch [131 / 300]: loss 0.2967, training auc: 0.8599, val_auc 0.7885\n",
      "2022-06-30 01:00:43,332 - NCNet pretrain, Epoch [132 / 300]: loss 0.2977, training auc: 0.8616, val_auc 0.7748\n",
      "2022-06-30 01:00:43,450 - NCNet pretrain, Epoch [133 / 300]: loss 0.3230, training auc: 0.8400, val_auc 0.7876\n",
      "2022-06-30 01:00:43,569 - NCNet pretrain, Epoch [134 / 300]: loss 0.2934, training auc: 0.8712, val_auc 0.7870\n",
      "2022-06-30 01:00:43,688 - NCNet pretrain, Epoch [135 / 300]: loss 0.2938, training auc: 0.8776, val_auc 0.7707\n",
      "2022-06-30 01:00:43,806 - NCNet pretrain, Epoch [136 / 300]: loss 0.3210, training auc: 0.8447, val_auc 0.7837\n",
      "2022-06-30 01:00:43,924 - NCNet pretrain, Epoch [137 / 300]: loss 0.2895, training auc: 0.8667, val_auc 0.7812\n",
      "2022-06-30 01:00:44,062 - NCNet pretrain, Epoch [138 / 300]: loss 0.3597, training auc: 0.8590, val_auc 0.7640\n",
      "2022-06-30 01:00:44,179 - NCNet pretrain, Epoch [139 / 300]: loss 0.3437, training auc: 0.8378, val_auc 0.7624\n",
      "2022-06-30 01:00:44,299 - NCNet pretrain, Epoch [140 / 300]: loss 0.3412, training auc: 0.8463, val_auc 0.7878\n",
      "2022-06-30 01:00:44,300 - Early stop!\n",
      "2022-06-30 01:00:44,301 - Best Test Results: auc 0.8062, ap 0.4277, f1 0.2951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 314733.65it/s]\n",
      "2022-06-30 01:00:45,293 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 01:00:45,482 - NCNet pretrain, Epoch [1 / 300]: loss 1.1334, training auc: 0.6124, val_auc 0.4683, test auc 0.4415\n",
      "2022-06-30 01:00:45,600 - NCNet pretrain, Epoch [2 / 300]: loss 0.6912, training auc: 0.4127, val_auc 0.4639\n",
      "2022-06-30 01:00:45,718 - NCNet pretrain, Epoch [3 / 300]: loss 0.5248, training auc: 0.4291, val_auc 0.4617\n",
      "2022-06-30 01:00:45,836 - NCNet pretrain, Epoch [4 / 300]: loss 0.5263, training auc: 0.4105, val_auc 0.4617\n",
      "2022-06-30 01:00:45,953 - NCNet pretrain, Epoch [5 / 300]: loss 0.5730, training auc: 0.4201, val_auc 0.4616\n",
      "2022-06-30 01:00:46,074 - NCNet pretrain, Epoch [6 / 300]: loss 0.5665, training auc: 0.4543, val_auc 0.4625\n",
      "2022-06-30 01:00:46,192 - NCNet pretrain, Epoch [7 / 300]: loss 0.5344, training auc: 0.4638, val_auc 0.4641\n",
      "2022-06-30 01:00:46,310 - NCNet pretrain, Epoch [8 / 300]: loss 0.5471, training auc: 0.3881, val_auc 0.4666\n",
      "2022-06-30 01:00:46,439 - NCNet pretrain, Epoch [9 / 300]: loss 0.5019, training auc: 0.4249, val_auc 0.4697, test auc 0.4446\n",
      "2022-06-30 01:00:46,586 - NCNet pretrain, Epoch [10 / 300]: loss 0.5139, training auc: 0.3936, val_auc 0.4733, test auc 0.4492\n",
      "2022-06-30 01:00:46,716 - NCNet pretrain, Epoch [11 / 300]: loss 0.5252, training auc: 0.4137, val_auc 0.4754, test auc 0.4520\n",
      "2022-06-30 01:00:46,866 - NCNet pretrain, Epoch [12 / 300]: loss 0.5302, training auc: 0.4150, val_auc 0.4763, test auc 0.4528\n",
      "2022-06-30 01:00:46,985 - NCNet pretrain, Epoch [13 / 300]: loss 0.5247, training auc: 0.4263, val_auc 0.4762\n",
      "2022-06-30 01:00:47,107 - NCNet pretrain, Epoch [14 / 300]: loss 0.5053, training auc: 0.4501, val_auc 0.4759\n",
      "2022-06-30 01:00:47,227 - NCNet pretrain, Epoch [15 / 300]: loss 0.4907, training auc: 0.4660, val_auc 0.4754\n",
      "2022-06-30 01:00:47,346 - NCNet pretrain, Epoch [16 / 300]: loss 0.5041, training auc: 0.4298, val_auc 0.4758\n",
      "2022-06-30 01:00:47,484 - NCNet pretrain, Epoch [17 / 300]: loss 0.5045, training auc: 0.4249, val_auc 0.4763\n",
      "2022-06-30 01:00:47,619 - NCNet pretrain, Epoch [18 / 300]: loss 0.5041, training auc: 0.4261, val_auc 0.4771, test auc 0.4515\n",
      "2022-06-30 01:00:47,767 - NCNet pretrain, Epoch [19 / 300]: loss 0.5036, training auc: 0.4333, val_auc 0.4783, test auc 0.4531\n",
      "2022-06-30 01:00:47,915 - NCNet pretrain, Epoch [20 / 300]: loss 0.4877, training auc: 0.4771, val_auc 0.4796, test auc 0.4548\n",
      "2022-06-30 01:00:48,064 - NCNet pretrain, Epoch [21 / 300]: loss 0.5019, training auc: 0.4301, val_auc 0.4811, test auc 0.4566\n",
      "2022-06-30 01:00:48,208 - NCNet pretrain, Epoch [22 / 300]: loss 0.4762, training auc: 0.4981, val_auc 0.4823, test auc 0.4580\n",
      "2022-06-30 01:00:48,342 - NCNet pretrain, Epoch [23 / 300]: loss 0.4960, training auc: 0.4380, val_auc 0.4833, test auc 0.4593\n",
      "2022-06-30 01:00:48,471 - NCNet pretrain, Epoch [24 / 300]: loss 0.4888, training auc: 0.4741, val_auc 0.4841, test auc 0.4602\n",
      "2022-06-30 01:00:48,600 - NCNet pretrain, Epoch [25 / 300]: loss 0.4867, training auc: 0.4646, val_auc 0.4849, test auc 0.4610\n",
      "2022-06-30 01:00:48,733 - NCNet pretrain, Epoch [26 / 300]: loss 0.4860, training auc: 0.4774, val_auc 0.4854, test auc 0.4613\n",
      "2022-06-30 01:00:48,877 - NCNet pretrain, Epoch [27 / 300]: loss 0.4770, training auc: 0.4789, val_auc 0.4860, test auc 0.4615\n",
      "2022-06-30 01:00:49,006 - NCNet pretrain, Epoch [28 / 300]: loss 0.4753, training auc: 0.4917, val_auc 0.4866, test auc 0.4616\n",
      "2022-06-30 01:00:49,156 - NCNet pretrain, Epoch [29 / 300]: loss 0.4727, training auc: 0.4904, val_auc 0.4872, test auc 0.4619\n",
      "2022-06-30 01:00:49,305 - NCNet pretrain, Epoch [30 / 300]: loss 0.5005, training auc: 0.4043, val_auc 0.4880, test auc 0.4627\n",
      "2022-06-30 01:00:49,436 - NCNet pretrain, Epoch [31 / 300]: loss 0.4809, training auc: 0.4661, val_auc 0.4893, test auc 0.4637\n",
      "2022-06-30 01:00:49,565 - NCNet pretrain, Epoch [32 / 300]: loss 0.4795, training auc: 0.4658, val_auc 0.4906, test auc 0.4650\n",
      "2022-06-30 01:00:49,696 - NCNet pretrain, Epoch [33 / 300]: loss 0.4836, training auc: 0.4519, val_auc 0.4921, test auc 0.4667\n",
      "2022-06-30 01:00:49,831 - NCNet pretrain, Epoch [34 / 300]: loss 0.4745, training auc: 0.4675, val_auc 0.4937, test auc 0.4687\n",
      "2022-06-30 01:00:49,960 - NCNet pretrain, Epoch [35 / 300]: loss 0.4812, training auc: 0.4541, val_auc 0.4958, test auc 0.4708\n",
      "2022-06-30 01:00:50,090 - NCNet pretrain, Epoch [36 / 300]: loss 0.4703, training auc: 0.4784, val_auc 0.4977, test auc 0.4729\n",
      "2022-06-30 01:00:50,219 - NCNet pretrain, Epoch [37 / 300]: loss 0.4651, training auc: 0.5062, val_auc 0.4996, test auc 0.4749\n",
      "2022-06-30 01:00:50,347 - NCNet pretrain, Epoch [38 / 300]: loss 0.4811, training auc: 0.4427, val_auc 0.5017, test auc 0.4769\n",
      "2022-06-30 01:00:50,476 - NCNet pretrain, Epoch [39 / 300]: loss 0.4719, training auc: 0.4734, val_auc 0.5036, test auc 0.4786\n",
      "2022-06-30 01:00:50,605 - NCNet pretrain, Epoch [40 / 300]: loss 0.4768, training auc: 0.4494, val_auc 0.5056, test auc 0.4804\n",
      "2022-06-30 01:00:50,734 - NCNet pretrain, Epoch [41 / 300]: loss 0.4750, training auc: 0.4491, val_auc 0.5078, test auc 0.4822\n",
      "2022-06-30 01:00:50,862 - NCNet pretrain, Epoch [42 / 300]: loss 0.4731, training auc: 0.4578, val_auc 0.5100, test auc 0.4844\n",
      "2022-06-30 01:00:51,010 - NCNet pretrain, Epoch [43 / 300]: loss 0.4800, training auc: 0.4341, val_auc 0.5127, test auc 0.4873\n",
      "2022-06-30 01:00:51,158 - NCNet pretrain, Epoch [44 / 300]: loss 0.4734, training auc: 0.4454, val_auc 0.5165, test auc 0.4912\n",
      "2022-06-30 01:00:51,307 - NCNet pretrain, Epoch [45 / 300]: loss 0.4482, training auc: 0.5404, val_auc 0.5195, test auc 0.4946\n",
      "2022-06-30 01:00:51,436 - NCNet pretrain, Epoch [46 / 300]: loss 0.4499, training auc: 0.5335, val_auc 0.5231, test auc 0.4980\n",
      "2022-06-30 01:00:51,566 - NCNet pretrain, Epoch [47 / 300]: loss 0.4508, training auc: 0.5316, val_auc 0.5264, test auc 0.5012\n",
      "2022-06-30 01:00:51,696 - NCNet pretrain, Epoch [48 / 300]: loss 0.4592, training auc: 0.4876, val_auc 0.5299, test auc 0.5045\n",
      "2022-06-30 01:00:51,827 - NCNet pretrain, Epoch [49 / 300]: loss 0.4625, training auc: 0.4737, val_auc 0.5350, test auc 0.5098\n",
      "2022-06-30 01:00:51,964 - NCNet pretrain, Epoch [50 / 300]: loss 0.4480, training auc: 0.5273, val_auc 0.5411, test auc 0.5160\n",
      "2022-06-30 01:00:52,093 - NCNet pretrain, Epoch [51 / 300]: loss 0.4565, training auc: 0.5000, val_auc 0.5475, test auc 0.5228\n",
      "2022-06-30 01:00:52,222 - NCNet pretrain, Epoch [52 / 300]: loss 0.4476, training auc: 0.5250, val_auc 0.5545, test auc 0.5299\n",
      "2022-06-30 01:00:52,351 - NCNet pretrain, Epoch [53 / 300]: loss 0.4494, training auc: 0.5108, val_auc 0.5630, test auc 0.5382\n",
      "2022-06-30 01:00:52,481 - NCNet pretrain, Epoch [54 / 300]: loss 0.4404, training auc: 0.5582, val_auc 0.5696, test auc 0.5451\n",
      "2022-06-30 01:00:52,610 - NCNet pretrain, Epoch [55 / 300]: loss 0.4433, training auc: 0.5401, val_auc 0.5768, test auc 0.5522\n",
      "2022-06-30 01:00:52,739 - NCNet pretrain, Epoch [56 / 300]: loss 0.4442, training auc: 0.5266, val_auc 0.5852, test auc 0.5612\n",
      "2022-06-30 01:00:52,869 - NCNet pretrain, Epoch [57 / 300]: loss 0.4290, training auc: 0.5999, val_auc 0.5920, test auc 0.5685\n",
      "2022-06-30 01:00:52,998 - NCNet pretrain, Epoch [58 / 300]: loss 0.4310, training auc: 0.5890, val_auc 0.5980, test auc 0.5742\n",
      "2022-06-30 01:00:53,128 - NCNet pretrain, Epoch [59 / 300]: loss 0.4306, training auc: 0.5911, val_auc 0.6046, test auc 0.5810\n",
      "2022-06-30 01:00:53,257 - NCNet pretrain, Epoch [60 / 300]: loss 0.4318, training auc: 0.5704, val_auc 0.6158, test auc 0.5924\n",
      "2022-06-30 01:00:53,387 - NCNet pretrain, Epoch [61 / 300]: loss 0.4243, training auc: 0.5874, val_auc 0.6300, test auc 0.6070\n",
      "2022-06-30 01:00:53,516 - NCNet pretrain, Epoch [62 / 300]: loss 0.4252, training auc: 0.5957, val_auc 0.6419, test auc 0.6192\n",
      "2022-06-30 01:00:53,649 - NCNet pretrain, Epoch [63 / 300]: loss 0.4138, training auc: 0.6473, val_auc 0.6430, test auc 0.6189\n",
      "2022-06-30 01:00:53,770 - NCNet pretrain, Epoch [64 / 300]: loss 0.4136, training auc: 0.6373, val_auc 0.6423\n",
      "2022-06-30 01:00:53,899 - NCNet pretrain, Epoch [65 / 300]: loss 0.4157, training auc: 0.6154, val_auc 0.6557, test auc 0.6299\n",
      "2022-06-30 01:00:54,033 - NCNet pretrain, Epoch [66 / 300]: loss 0.4117, training auc: 0.6414, val_auc 0.6738, test auc 0.6483\n",
      "2022-06-30 01:00:54,172 - NCNet pretrain, Epoch [67 / 300]: loss 0.3933, training auc: 0.6892, val_auc 0.6859, test auc 0.6608\n",
      "2022-06-30 01:00:54,301 - NCNet pretrain, Epoch [68 / 300]: loss 0.4065, training auc: 0.6588, val_auc 0.6901, test auc 0.6646\n",
      "2022-06-30 01:00:54,418 - NCNet pretrain, Epoch [69 / 300]: loss 0.3930, training auc: 0.6869, val_auc 0.6883\n",
      "2022-06-30 01:00:54,547 - NCNet pretrain, Epoch [70 / 300]: loss 0.3977, training auc: 0.6694, val_auc 0.7002, test auc 0.6751\n",
      "2022-06-30 01:00:54,677 - NCNet pretrain, Epoch [71 / 300]: loss 0.3964, training auc: 0.6840, val_auc 0.7133, test auc 0.6922\n",
      "2022-06-30 01:00:54,806 - NCNet pretrain, Epoch [72 / 300]: loss 0.3943, training auc: 0.6828, val_auc 0.7175, test auc 0.6976\n",
      "2022-06-30 01:00:54,924 - NCNet pretrain, Epoch [73 / 300]: loss 0.3952, training auc: 0.6920, val_auc 0.7158\n",
      "2022-06-30 01:00:55,051 - NCNet pretrain, Epoch [74 / 300]: loss 0.3875, training auc: 0.6936, val_auc 0.7179, test auc 0.6974\n",
      "2022-06-30 01:00:55,181 - NCNet pretrain, Epoch [75 / 300]: loss 0.3887, training auc: 0.7020, val_auc 0.7273, test auc 0.7098\n",
      "2022-06-30 01:00:55,309 - NCNet pretrain, Epoch [76 / 300]: loss 0.3825, training auc: 0.7152, val_auc 0.7346, test auc 0.7196\n",
      "2022-06-30 01:00:55,427 - NCNet pretrain, Epoch [77 / 300]: loss 0.3806, training auc: 0.7298, val_auc 0.7243\n",
      "2022-06-30 01:00:55,545 - NCNet pretrain, Epoch [78 / 300]: loss 0.3830, training auc: 0.7089, val_auc 0.7281\n",
      "2022-06-30 01:00:55,675 - NCNet pretrain, Epoch [79 / 300]: loss 0.3698, training auc: 0.7400, val_auc 0.7408, test auc 0.7278\n",
      "2022-06-30 01:00:55,806 - NCNet pretrain, Epoch [80 / 300]: loss 0.3724, training auc: 0.7444, val_auc 0.7429, test auc 0.7303\n",
      "2022-06-30 01:00:55,924 - NCNet pretrain, Epoch [81 / 300]: loss 0.3751, training auc: 0.7372, val_auc 0.7401\n",
      "2022-06-30 01:00:56,044 - NCNet pretrain, Epoch [82 / 300]: loss 0.3732, training auc: 0.7348, val_auc 0.7378\n",
      "2022-06-30 01:00:56,183 - NCNet pretrain, Epoch [83 / 300]: loss 0.3861, training auc: 0.7125, val_auc 0.7517, test auc 0.7447\n",
      "2022-06-30 01:00:56,321 - NCNet pretrain, Epoch [84 / 300]: loss 0.3769, training auc: 0.7522, val_auc 0.7491\n",
      "2022-06-30 01:00:56,438 - NCNet pretrain, Epoch [85 / 300]: loss 0.3642, training auc: 0.7514, val_auc 0.7428\n",
      "2022-06-30 01:00:56,555 - NCNet pretrain, Epoch [86 / 300]: loss 0.3595, training auc: 0.7541, val_auc 0.7480\n",
      "2022-06-30 01:00:56,685 - NCNet pretrain, Epoch [87 / 300]: loss 0.3589, training auc: 0.7632, val_auc 0.7554, test auc 0.7512\n",
      "2022-06-30 01:00:56,802 - NCNet pretrain, Epoch [88 / 300]: loss 0.3633, training auc: 0.7698, val_auc 0.7547\n",
      "2022-06-30 01:00:56,940 - NCNet pretrain, Epoch [89 / 300]: loss 0.3718, training auc: 0.7517, val_auc 0.7548\n",
      "2022-06-30 01:00:57,058 - NCNet pretrain, Epoch [90 / 300]: loss 0.3633, training auc: 0.7577, val_auc 0.7518\n",
      "2022-06-30 01:00:57,191 - NCNet pretrain, Epoch [91 / 300]: loss 0.3604, training auc: 0.7548, val_auc 0.7557, test auc 0.7521\n",
      "2022-06-30 01:00:57,320 - NCNet pretrain, Epoch [92 / 300]: loss 0.3535, training auc: 0.7725, val_auc 0.7568, test auc 0.7547\n",
      "2022-06-30 01:00:57,438 - NCNet pretrain, Epoch [93 / 300]: loss 0.3558, training auc: 0.7814, val_auc 0.7517\n",
      "2022-06-30 01:00:57,577 - NCNet pretrain, Epoch [94 / 300]: loss 0.3512, training auc: 0.7689, val_auc 0.7562\n",
      "2022-06-30 01:00:57,695 - NCNet pretrain, Epoch [95 / 300]: loss 0.3594, training auc: 0.7639, val_auc 0.7562\n",
      "2022-06-30 01:00:57,814 - NCNet pretrain, Epoch [96 / 300]: loss 0.3465, training auc: 0.7762, val_auc 0.7558\n",
      "2022-06-30 01:00:57,945 - NCNet pretrain, Epoch [97 / 300]: loss 0.3572, training auc: 0.7635, val_auc 0.7582, test auc 0.7594\n",
      "2022-06-30 01:00:58,094 - NCNet pretrain, Epoch [98 / 300]: loss 0.3447, training auc: 0.7789, val_auc 0.7584, test auc 0.7600\n",
      "2022-06-30 01:00:58,246 - NCNet pretrain, Epoch [99 / 300]: loss 0.3497, training auc: 0.7788, val_auc 0.7611, test auc 0.7643\n",
      "2022-06-30 01:00:58,383 - NCNet pretrain, Epoch [100 / 300]: loss 0.3518, training auc: 0.7790, val_auc 0.7561\n",
      "2022-06-30 01:00:58,512 - NCNet pretrain, Epoch [101 / 300]: loss 0.3455, training auc: 0.7843, val_auc 0.7622, test auc 0.7662\n",
      "2022-06-30 01:00:58,641 - NCNet pretrain, Epoch [102 / 300]: loss 0.3516, training auc: 0.7773, val_auc 0.7654, test auc 0.7719\n",
      "2022-06-30 01:00:58,760 - NCNet pretrain, Epoch [103 / 300]: loss 0.3606, training auc: 0.7680, val_auc 0.7589\n",
      "2022-06-30 01:00:58,885 - NCNet pretrain, Epoch [104 / 300]: loss 0.3468, training auc: 0.7831, val_auc 0.7624\n",
      "2022-06-30 01:00:59,014 - NCNet pretrain, Epoch [105 / 300]: loss 0.3466, training auc: 0.7951, val_auc 0.7666, test auc 0.7734\n",
      "2022-06-30 01:00:59,132 - NCNet pretrain, Epoch [106 / 300]: loss 0.3505, training auc: 0.7947, val_auc 0.7634\n",
      "2022-06-30 01:00:59,253 - NCNet pretrain, Epoch [107 / 300]: loss 0.3461, training auc: 0.7893, val_auc 0.7603\n",
      "2022-06-30 01:00:59,382 - NCNet pretrain, Epoch [108 / 300]: loss 0.3538, training auc: 0.7738, val_auc 0.7670, test auc 0.7741\n",
      "2022-06-30 01:00:59,500 - NCNet pretrain, Epoch [109 / 300]: loss 0.3461, training auc: 0.7936, val_auc 0.7659\n",
      "2022-06-30 01:00:59,618 - NCNet pretrain, Epoch [110 / 300]: loss 0.3360, training auc: 0.8048, val_auc 0.7571\n",
      "2022-06-30 01:00:59,737 - NCNet pretrain, Epoch [111 / 300]: loss 0.3584, training auc: 0.7728, val_auc 0.7669\n",
      "2022-06-30 01:00:59,886 - NCNet pretrain, Epoch [112 / 300]: loss 0.3383, training auc: 0.8037, val_auc 0.7671, test auc 0.7754\n",
      "2022-06-30 01:01:00,005 - NCNet pretrain, Epoch [113 / 300]: loss 0.3391, training auc: 0.8145, val_auc 0.7571\n",
      "2022-06-30 01:01:00,125 - NCNet pretrain, Epoch [114 / 300]: loss 0.3386, training auc: 0.7982, val_auc 0.7640\n",
      "2022-06-30 01:01:00,256 - NCNet pretrain, Epoch [115 / 300]: loss 0.3331, training auc: 0.8041, val_auc 0.7682, test auc 0.7795\n",
      "2022-06-30 01:01:00,379 - NCNet pretrain, Epoch [116 / 300]: loss 0.3504, training auc: 0.7867, val_auc 0.7659\n",
      "2022-06-30 01:01:00,504 - NCNet pretrain, Epoch [117 / 300]: loss 0.3314, training auc: 0.8088, val_auc 0.7608\n",
      "2022-06-30 01:01:00,635 - NCNet pretrain, Epoch [118 / 300]: loss 0.3403, training auc: 0.8026, val_auc 0.7687, test auc 0.7795\n",
      "2022-06-30 01:01:00,754 - NCNet pretrain, Epoch [119 / 300]: loss 0.3372, training auc: 0.8151, val_auc 0.7685\n",
      "2022-06-30 01:01:00,873 - NCNet pretrain, Epoch [120 / 300]: loss 0.3382, training auc: 0.8052, val_auc 0.7615\n",
      "2022-06-30 01:01:00,996 - NCNet pretrain, Epoch [121 / 300]: loss 0.3379, training auc: 0.7944, val_auc 0.7672\n",
      "2022-06-30 01:01:01,126 - NCNet pretrain, Epoch [122 / 300]: loss 0.3355, training auc: 0.8024, val_auc 0.7694, test auc 0.7814\n",
      "2022-06-30 01:01:01,245 - NCNet pretrain, Epoch [123 / 300]: loss 0.3393, training auc: 0.8097, val_auc 0.7641\n",
      "2022-06-30 01:01:01,385 - NCNet pretrain, Epoch [124 / 300]: loss 0.3341, training auc: 0.8070, val_auc 0.7654\n",
      "2022-06-30 01:01:01,535 - NCNet pretrain, Epoch [125 / 300]: loss 0.3365, training auc: 0.8043, val_auc 0.7702, test auc 0.7851\n",
      "2022-06-30 01:01:01,655 - NCNet pretrain, Epoch [126 / 300]: loss 0.3452, training auc: 0.8087, val_auc 0.7641\n",
      "2022-06-30 01:01:01,797 - NCNet pretrain, Epoch [127 / 300]: loss 0.3234, training auc: 0.8199, val_auc 0.7602\n",
      "2022-06-30 01:01:01,918 - NCNet pretrain, Epoch [128 / 300]: loss 0.3245, training auc: 0.8177, val_auc 0.7676\n",
      "2022-06-30 01:01:02,050 - NCNet pretrain, Epoch [129 / 300]: loss 0.3308, training auc: 0.8107, val_auc 0.7705, test auc 0.7861\n",
      "2022-06-30 01:01:02,172 - NCNet pretrain, Epoch [130 / 300]: loss 0.3342, training auc: 0.8157, val_auc 0.7656\n",
      "2022-06-30 01:01:02,296 - NCNet pretrain, Epoch [131 / 300]: loss 0.3245, training auc: 0.8218, val_auc 0.7628\n",
      "2022-06-30 01:01:02,416 - NCNet pretrain, Epoch [132 / 300]: loss 0.3157, training auc: 0.8226, val_auc 0.7684\n",
      "2022-06-30 01:01:02,545 - NCNet pretrain, Epoch [133 / 300]: loss 0.3321, training auc: 0.8026, val_auc 0.7710, test auc 0.7856\n",
      "2022-06-30 01:01:02,667 - NCNet pretrain, Epoch [134 / 300]: loss 0.3283, training auc: 0.8259, val_auc 0.7694\n",
      "2022-06-30 01:01:02,787 - NCNet pretrain, Epoch [135 / 300]: loss 0.3292, training auc: 0.8171, val_auc 0.7656\n",
      "2022-06-30 01:01:02,927 - NCNet pretrain, Epoch [136 / 300]: loss 0.3206, training auc: 0.8205, val_auc 0.7664\n",
      "2022-06-30 01:01:03,078 - NCNet pretrain, Epoch [137 / 300]: loss 0.3154, training auc: 0.8251, val_auc 0.7716, test auc 0.7867\n",
      "2022-06-30 01:01:03,216 - NCNet pretrain, Epoch [138 / 300]: loss 0.3256, training auc: 0.8203, val_auc 0.7700\n",
      "2022-06-30 01:01:03,355 - NCNet pretrain, Epoch [139 / 300]: loss 0.3235, training auc: 0.8230, val_auc 0.7631\n",
      "2022-06-30 01:01:03,493 - NCNet pretrain, Epoch [140 / 300]: loss 0.3316, training auc: 0.8154, val_auc 0.7706\n",
      "2022-06-30 01:01:03,642 - NCNet pretrain, Epoch [141 / 300]: loss 0.3249, training auc: 0.8275, val_auc 0.7719, test auc 0.7900\n",
      "2022-06-30 01:01:03,760 - NCNet pretrain, Epoch [142 / 300]: loss 0.3226, training auc: 0.8365, val_auc 0.7640\n",
      "2022-06-30 01:01:03,883 - NCNet pretrain, Epoch [143 / 300]: loss 0.3270, training auc: 0.8148, val_auc 0.7696\n",
      "2022-06-30 01:01:04,013 - NCNet pretrain, Epoch [144 / 300]: loss 0.3222, training auc: 0.8225, val_auc 0.7727, test auc 0.7911\n",
      "2022-06-30 01:01:04,131 - NCNet pretrain, Epoch [145 / 300]: loss 0.3293, training auc: 0.8270, val_auc 0.7659\n",
      "2022-06-30 01:01:04,252 - NCNet pretrain, Epoch [146 / 300]: loss 0.3201, training auc: 0.8302, val_auc 0.7602\n",
      "2022-06-30 01:01:04,372 - NCNet pretrain, Epoch [147 / 300]: loss 0.3337, training auc: 0.8091, val_auc 0.7726\n",
      "2022-06-30 01:01:04,491 - NCNet pretrain, Epoch [148 / 300]: loss 0.3254, training auc: 0.8342, val_auc 0.7713\n",
      "2022-06-30 01:01:04,609 - NCNet pretrain, Epoch [149 / 300]: loss 0.3252, training auc: 0.8255, val_auc 0.7554\n",
      "2022-06-30 01:01:04,736 - NCNet pretrain, Epoch [150 / 300]: loss 0.3382, training auc: 0.8108, val_auc 0.7673\n",
      "2022-06-30 01:01:04,854 - NCNet pretrain, Epoch [151 / 300]: loss 0.3230, training auc: 0.8212, val_auc 0.7715\n",
      "2022-06-30 01:01:04,972 - NCNet pretrain, Epoch [152 / 300]: loss 0.3333, training auc: 0.8277, val_auc 0.7637\n",
      "2022-06-30 01:01:05,090 - NCNet pretrain, Epoch [153 / 300]: loss 0.3164, training auc: 0.8332, val_auc 0.7583\n",
      "2022-06-30 01:01:05,207 - NCNet pretrain, Epoch [154 / 300]: loss 0.3199, training auc: 0.8228, val_auc 0.7674\n",
      "2022-06-30 01:01:05,327 - NCNet pretrain, Epoch [155 / 300]: loss 0.3142, training auc: 0.8339, val_auc 0.7700\n",
      "2022-06-30 01:01:05,445 - NCNet pretrain, Epoch [156 / 300]: loss 0.3261, training auc: 0.8311, val_auc 0.7638\n",
      "2022-06-30 01:01:05,562 - NCNet pretrain, Epoch [157 / 300]: loss 0.3108, training auc: 0.8341, val_auc 0.7600\n",
      "2022-06-30 01:01:05,681 - NCNet pretrain, Epoch [158 / 300]: loss 0.3119, training auc: 0.8339, val_auc 0.7667\n",
      "2022-06-30 01:01:05,800 - NCNet pretrain, Epoch [159 / 300]: loss 0.3115, training auc: 0.8397, val_auc 0.7706\n",
      "2022-06-30 01:01:05,920 - NCNet pretrain, Epoch [160 / 300]: loss 0.3263, training auc: 0.8378, val_auc 0.7645\n",
      "2022-06-30 01:01:06,039 - NCNet pretrain, Epoch [161 / 300]: loss 0.3156, training auc: 0.8253, val_auc 0.7597\n",
      "2022-06-30 01:01:06,158 - NCNet pretrain, Epoch [162 / 300]: loss 0.3211, training auc: 0.8294, val_auc 0.7702\n",
      "2022-06-30 01:01:06,295 - NCNet pretrain, Epoch [163 / 300]: loss 0.3172, training auc: 0.8303, val_auc 0.7710\n",
      "2022-06-30 01:01:06,427 - NCNet pretrain, Epoch [164 / 300]: loss 0.3174, training auc: 0.8451, val_auc 0.7630\n",
      "2022-06-30 01:01:06,545 - NCNet pretrain, Epoch [165 / 300]: loss 0.3268, training auc: 0.8190, val_auc 0.7645\n",
      "2022-06-30 01:01:06,664 - NCNet pretrain, Epoch [166 / 300]: loss 0.3157, training auc: 0.8308, val_auc 0.7702\n",
      "2022-06-30 01:01:06,787 - NCNet pretrain, Epoch [167 / 300]: loss 0.3105, training auc: 0.8484, val_auc 0.7694\n",
      "2022-06-30 01:01:06,911 - NCNet pretrain, Epoch [168 / 300]: loss 0.3046, training auc: 0.8474, val_auc 0.7656\n",
      "2022-06-30 01:01:07,029 - NCNet pretrain, Epoch [169 / 300]: loss 0.3063, training auc: 0.8433, val_auc 0.7678\n",
      "2022-06-30 01:01:07,147 - NCNet pretrain, Epoch [170 / 300]: loss 0.2987, training auc: 0.8549, val_auc 0.7695\n",
      "2022-06-30 01:01:07,265 - NCNet pretrain, Epoch [171 / 300]: loss 0.3173, training auc: 0.8380, val_auc 0.7686\n",
      "2022-06-30 01:01:07,383 - NCNet pretrain, Epoch [172 / 300]: loss 0.3014, training auc: 0.8451, val_auc 0.7677\n",
      "2022-06-30 01:01:07,501 - NCNet pretrain, Epoch [173 / 300]: loss 0.3083, training auc: 0.8388, val_auc 0.7669\n",
      "2022-06-30 01:01:07,619 - NCNet pretrain, Epoch [174 / 300]: loss 0.3029, training auc: 0.8432, val_auc 0.7691\n",
      "2022-06-30 01:01:07,737 - NCNet pretrain, Epoch [175 / 300]: loss 0.3076, training auc: 0.8479, val_auc 0.7689\n",
      "2022-06-30 01:01:07,855 - NCNet pretrain, Epoch [176 / 300]: loss 0.3051, training auc: 0.8464, val_auc 0.7668\n",
      "2022-06-30 01:01:07,973 - NCNet pretrain, Epoch [177 / 300]: loss 0.3162, training auc: 0.8333, val_auc 0.7666\n",
      "2022-06-30 01:01:08,092 - NCNet pretrain, Epoch [178 / 300]: loss 0.3134, training auc: 0.8353, val_auc 0.7698\n",
      "2022-06-30 01:01:08,210 - NCNet pretrain, Epoch [179 / 300]: loss 0.3053, training auc: 0.8537, val_auc 0.7678\n",
      "2022-06-30 01:01:08,329 - NCNet pretrain, Epoch [180 / 300]: loss 0.3037, training auc: 0.8471, val_auc 0.7651\n",
      "2022-06-30 01:01:08,447 - NCNet pretrain, Epoch [181 / 300]: loss 0.3052, training auc: 0.8434, val_auc 0.7667\n",
      "2022-06-30 01:01:08,567 - NCNet pretrain, Epoch [182 / 300]: loss 0.3045, training auc: 0.8419, val_auc 0.7684\n",
      "2022-06-30 01:01:08,687 - NCNet pretrain, Epoch [183 / 300]: loss 0.3027, training auc: 0.8507, val_auc 0.7687\n",
      "2022-06-30 01:01:08,806 - NCNet pretrain, Epoch [184 / 300]: loss 0.3040, training auc: 0.8505, val_auc 0.7663\n",
      "2022-06-30 01:01:08,928 - NCNet pretrain, Epoch [185 / 300]: loss 0.3052, training auc: 0.8481, val_auc 0.7655\n",
      "2022-06-30 01:01:09,053 - NCNet pretrain, Epoch [186 / 300]: loss 0.3064, training auc: 0.8451, val_auc 0.7681\n",
      "2022-06-30 01:01:09,189 - NCNet pretrain, Epoch [187 / 300]: loss 0.3041, training auc: 0.8510, val_auc 0.7665\n",
      "2022-06-30 01:01:09,308 - NCNet pretrain, Epoch [188 / 300]: loss 0.2976, training auc: 0.8526, val_auc 0.7654\n",
      "2022-06-30 01:01:09,426 - NCNet pretrain, Epoch [189 / 300]: loss 0.3081, training auc: 0.8384, val_auc 0.7684\n",
      "2022-06-30 01:01:09,551 - NCNet pretrain, Epoch [190 / 300]: loss 0.3015, training auc: 0.8544, val_auc 0.7671\n",
      "2022-06-30 01:01:09,690 - NCNet pretrain, Epoch [191 / 300]: loss 0.3017, training auc: 0.8492, val_auc 0.7647\n",
      "2022-06-30 01:01:09,809 - NCNet pretrain, Epoch [192 / 300]: loss 0.3052, training auc: 0.8408, val_auc 0.7663\n",
      "2022-06-30 01:01:09,928 - NCNet pretrain, Epoch [193 / 300]: loss 0.3014, training auc: 0.8504, val_auc 0.7669\n",
      "2022-06-30 01:01:10,047 - NCNet pretrain, Epoch [194 / 300]: loss 0.3071, training auc: 0.8515, val_auc 0.7661\n",
      "2022-06-30 01:01:10,048 - Early stop!\n",
      "2022-06-30 01:01:10,049 - Best Test Results: auc 0.7911, ap 0.3984, f1 0.2842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 239958.77it/s]\n",
      "2022-06-30 01:01:11,227 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 01:01:11,409 - NCNet pretrain, Epoch [1 / 300]: loss 0.4650, training auc: 0.6895, val_auc 0.4635, test auc 0.4353\n",
      "2022-06-30 01:01:11,556 - NCNet pretrain, Epoch [2 / 300]: loss 0.4343, training auc: 0.4181, val_auc 0.4645, test auc 0.4375\n",
      "2022-06-30 01:01:11,674 - NCNet pretrain, Epoch [3 / 300]: loss 0.4472, training auc: 0.3862, val_auc 0.4635\n",
      "2022-06-30 01:01:11,821 - NCNet pretrain, Epoch [4 / 300]: loss 0.4358, training auc: 0.4017, val_auc 0.4672, test auc 0.4438\n",
      "2022-06-30 01:01:11,950 - NCNet pretrain, Epoch [5 / 300]: loss 0.4245, training auc: 0.4646, val_auc 0.4724, test auc 0.4507\n",
      "2022-06-30 01:01:12,079 - NCNet pretrain, Epoch [6 / 300]: loss 0.4255, training auc: 0.4398, val_auc 0.4776, test auc 0.4574\n",
      "2022-06-30 01:01:12,207 - NCNet pretrain, Epoch [7 / 300]: loss 0.4259, training auc: 0.4909, val_auc 0.4821, test auc 0.4623\n",
      "2022-06-30 01:01:12,356 - NCNet pretrain, Epoch [8 / 300]: loss 0.4265, training auc: 0.4607, val_auc 0.4867, test auc 0.4670\n",
      "2022-06-30 01:01:12,485 - NCNet pretrain, Epoch [9 / 300]: loss 0.4268, training auc: 0.4545, val_auc 0.4899, test auc 0.4696\n",
      "2022-06-30 01:01:12,604 - NCNet pretrain, Epoch [10 / 300]: loss 0.4232, training auc: 0.4861, val_auc 0.4880\n",
      "2022-06-30 01:01:12,724 - NCNet pretrain, Epoch [11 / 300]: loss 0.4231, training auc: 0.4722, val_auc 0.4873\n",
      "2022-06-30 01:01:12,846 - NCNet pretrain, Epoch [12 / 300]: loss 0.4228, training auc: 0.4610, val_auc 0.4873\n",
      "2022-06-30 01:01:12,965 - NCNet pretrain, Epoch [13 / 300]: loss 0.4230, training auc: 0.4619, val_auc 0.4898\n",
      "2022-06-30 01:01:13,090 - NCNet pretrain, Epoch [14 / 300]: loss 0.4230, training auc: 0.4544, val_auc 0.4922, test auc 0.4670\n",
      "2022-06-30 01:01:13,242 - NCNet pretrain, Epoch [15 / 300]: loss 0.4241, training auc: 0.4213, val_auc 0.4979, test auc 0.4732\n",
      "2022-06-30 01:01:13,391 - NCNet pretrain, Epoch [16 / 300]: loss 0.4216, training auc: 0.4661, val_auc 0.5092, test auc 0.4853\n",
      "2022-06-30 01:01:13,520 - NCNet pretrain, Epoch [17 / 300]: loss 0.4194, training auc: 0.4740, val_auc 0.5228, test auc 0.5003\n",
      "2022-06-30 01:01:13,648 - NCNet pretrain, Epoch [18 / 300]: loss 0.4186, training auc: 0.5017, val_auc 0.5342, test auc 0.5122\n",
      "2022-06-30 01:01:13,778 - NCNet pretrain, Epoch [19 / 300]: loss 0.4171, training auc: 0.5312, val_auc 0.5435, test auc 0.5195\n",
      "2022-06-30 01:01:13,907 - NCNet pretrain, Epoch [20 / 300]: loss 0.4126, training auc: 0.5771, val_auc 0.5530, test auc 0.5271\n",
      "2022-06-30 01:01:14,036 - NCNet pretrain, Epoch [21 / 300]: loss 0.4150, training auc: 0.5319, val_auc 0.5627, test auc 0.5354\n",
      "2022-06-30 01:01:14,164 - NCNet pretrain, Epoch [22 / 300]: loss 0.4159, training auc: 0.4929, val_auc 0.5867, test auc 0.5574\n",
      "2022-06-30 01:01:14,294 - NCNet pretrain, Epoch [23 / 300]: loss 0.4114, training auc: 0.5721, val_auc 0.6343, test auc 0.6028\n",
      "2022-06-30 01:01:14,424 - NCNet pretrain, Epoch [24 / 300]: loss 0.4056, training auc: 0.6520, val_auc 0.6907, test auc 0.6607\n",
      "2022-06-30 01:01:14,547 - NCNet pretrain, Epoch [25 / 300]: loss 0.4039, training auc: 0.7133, val_auc 0.6837\n",
      "2022-06-30 01:01:14,665 - NCNet pretrain, Epoch [26 / 300]: loss 0.4082, training auc: 0.5899, val_auc 0.6743\n",
      "2022-06-30 01:01:14,784 - NCNet pretrain, Epoch [27 / 300]: loss 0.4072, training auc: 0.6009, val_auc 0.6878\n",
      "2022-06-30 01:01:14,913 - NCNet pretrain, Epoch [28 / 300]: loss 0.3942, training auc: 0.7077, val_auc 0.7422, test auc 0.7206\n",
      "2022-06-30 01:01:15,044 - NCNet pretrain, Epoch [29 / 300]: loss 0.3964, training auc: 0.7170, val_auc 0.7616, test auc 0.7496\n",
      "2022-06-30 01:01:15,163 - NCNet pretrain, Epoch [30 / 300]: loss 0.3924, training auc: 0.7391, val_auc 0.7196\n",
      "2022-06-30 01:01:15,282 - NCNet pretrain, Epoch [31 / 300]: loss 0.3971, training auc: 0.6417, val_auc 0.7420\n",
      "2022-06-30 01:01:15,418 - NCNet pretrain, Epoch [32 / 300]: loss 0.3939, training auc: 0.6815, val_auc 0.7682, test auc 0.7629\n",
      "2022-06-30 01:01:15,537 - NCNet pretrain, Epoch [33 / 300]: loss 0.3804, training auc: 0.7654, val_auc 0.7589\n",
      "2022-06-30 01:01:15,656 - NCNet pretrain, Epoch [34 / 300]: loss 0.3838, training auc: 0.7334, val_auc 0.7541\n",
      "2022-06-30 01:01:15,775 - NCNet pretrain, Epoch [35 / 300]: loss 0.3728, training auc: 0.7555, val_auc 0.7453\n",
      "2022-06-30 01:01:15,895 - NCNet pretrain, Epoch [36 / 300]: loss 0.3832, training auc: 0.7255, val_auc 0.7616\n",
      "2022-06-30 01:01:16,034 - NCNet pretrain, Epoch [37 / 300]: loss 0.3790, training auc: 0.7590, val_auc 0.7506\n",
      "2022-06-30 01:01:16,153 - NCNet pretrain, Epoch [38 / 300]: loss 0.3719, training auc: 0.7416, val_auc 0.7515\n",
      "2022-06-30 01:01:16,271 - NCNet pretrain, Epoch [39 / 300]: loss 0.3776, training auc: 0.7334, val_auc 0.7644\n",
      "2022-06-30 01:01:16,392 - NCNet pretrain, Epoch [40 / 300]: loss 0.3767, training auc: 0.7358, val_auc 0.7575\n",
      "2022-06-30 01:01:16,510 - NCNet pretrain, Epoch [41 / 300]: loss 0.3667, training auc: 0.7427, val_auc 0.7621\n",
      "2022-06-30 01:01:16,628 - NCNet pretrain, Epoch [42 / 300]: loss 0.3734, training auc: 0.7373, val_auc 0.7673\n",
      "2022-06-30 01:01:16,746 - NCNet pretrain, Epoch [43 / 300]: loss 0.3584, training auc: 0.7854, val_auc 0.7661\n",
      "2022-06-30 01:01:16,884 - NCNet pretrain, Epoch [44 / 300]: loss 0.3448, training auc: 0.7874, val_auc 0.7659\n",
      "2022-06-30 01:01:17,013 - NCNet pretrain, Epoch [45 / 300]: loss 0.3604, training auc: 0.7709, val_auc 0.7742, test auc 0.7774\n",
      "2022-06-30 01:01:17,154 - NCNet pretrain, Epoch [46 / 300]: loss 0.3573, training auc: 0.7779, val_auc 0.7625\n",
      "2022-06-30 01:01:17,291 - NCNet pretrain, Epoch [47 / 300]: loss 0.3499, training auc: 0.7683, val_auc 0.7722\n",
      "2022-06-30 01:01:17,441 - NCNet pretrain, Epoch [48 / 300]: loss 0.3455, training auc: 0.7844, val_auc 0.7792, test auc 0.7863\n",
      "2022-06-30 01:01:17,585 - NCNet pretrain, Epoch [49 / 300]: loss 0.3560, training auc: 0.7864, val_auc 0.7602\n",
      "2022-06-30 01:01:17,724 - NCNet pretrain, Epoch [50 / 300]: loss 0.3550, training auc: 0.7629, val_auc 0.7632\n",
      "2022-06-30 01:01:17,861 - NCNet pretrain, Epoch [51 / 300]: loss 0.3536, training auc: 0.7603, val_auc 0.7806, test auc 0.7869\n",
      "2022-06-30 01:01:18,011 - NCNet pretrain, Epoch [52 / 300]: loss 0.3465, training auc: 0.8017, val_auc 0.7818, test auc 0.7886\n",
      "2022-06-30 01:01:18,129 - NCNet pretrain, Epoch [53 / 300]: loss 0.3456, training auc: 0.8022, val_auc 0.7672\n",
      "2022-06-30 01:01:18,245 - NCNet pretrain, Epoch [54 / 300]: loss 0.3484, training auc: 0.7783, val_auc 0.7651\n",
      "2022-06-30 01:01:18,363 - NCNet pretrain, Epoch [55 / 300]: loss 0.3466, training auc: 0.7841, val_auc 0.7792\n",
      "2022-06-30 01:01:18,511 - NCNet pretrain, Epoch [56 / 300]: loss 0.3358, training auc: 0.7994, val_auc 0.7852, test auc 0.7925\n",
      "2022-06-30 01:01:18,648 - NCNet pretrain, Epoch [57 / 300]: loss 0.3529, training auc: 0.8044, val_auc 0.7782\n",
      "2022-06-30 01:01:18,785 - NCNet pretrain, Epoch [58 / 300]: loss 0.3349, training auc: 0.7915, val_auc 0.7693\n",
      "2022-06-30 01:01:18,923 - NCNet pretrain, Epoch [59 / 300]: loss 0.3553, training auc: 0.7666, val_auc 0.7760\n",
      "2022-06-30 01:01:19,048 - NCNet pretrain, Epoch [60 / 300]: loss 0.3461, training auc: 0.7852, val_auc 0.7860, test auc 0.7919\n",
      "2022-06-30 01:01:19,167 - NCNet pretrain, Epoch [61 / 300]: loss 0.3427, training auc: 0.8138, val_auc 0.7831\n",
      "2022-06-30 01:01:19,286 - NCNet pretrain, Epoch [62 / 300]: loss 0.3360, training auc: 0.7944, val_auc 0.7752\n",
      "2022-06-30 01:01:19,404 - NCNet pretrain, Epoch [63 / 300]: loss 0.3367, training auc: 0.7958, val_auc 0.7754\n",
      "2022-06-30 01:01:19,523 - NCNet pretrain, Epoch [64 / 300]: loss 0.3299, training auc: 0.8063, val_auc 0.7828\n",
      "2022-06-30 01:01:19,676 - NCNet pretrain, Epoch [65 / 300]: loss 0.3406, training auc: 0.7931, val_auc 0.7872, test auc 0.7929\n",
      "2022-06-30 01:01:19,795 - NCNet pretrain, Epoch [66 / 300]: loss 0.3412, training auc: 0.8092, val_auc 0.7842\n",
      "2022-06-30 01:01:19,912 - NCNet pretrain, Epoch [67 / 300]: loss 0.3368, training auc: 0.8006, val_auc 0.7789\n",
      "2022-06-30 01:01:20,030 - NCNet pretrain, Epoch [68 / 300]: loss 0.3380, training auc: 0.7953, val_auc 0.7824\n",
      "2022-06-30 01:01:20,159 - NCNet pretrain, Epoch [69 / 300]: loss 0.3266, training auc: 0.8034, val_auc 0.7883, test auc 0.7939\n",
      "2022-06-30 01:01:20,277 - NCNet pretrain, Epoch [70 / 300]: loss 0.3270, training auc: 0.8149, val_auc 0.7879\n",
      "2022-06-30 01:01:20,396 - NCNet pretrain, Epoch [71 / 300]: loss 0.3195, training auc: 0.8262, val_auc 0.7837\n",
      "2022-06-30 01:01:20,515 - NCNet pretrain, Epoch [72 / 300]: loss 0.3300, training auc: 0.8078, val_auc 0.7832\n",
      "2022-06-30 01:01:20,633 - NCNet pretrain, Epoch [73 / 300]: loss 0.3271, training auc: 0.8122, val_auc 0.7873\n",
      "2022-06-30 01:01:20,762 - NCNet pretrain, Epoch [74 / 300]: loss 0.3233, training auc: 0.8172, val_auc 0.7900, test auc 0.7964\n",
      "2022-06-30 01:01:20,895 - NCNet pretrain, Epoch [75 / 300]: loss 0.3227, training auc: 0.8185, val_auc 0.7871\n",
      "2022-06-30 01:01:21,033 - NCNet pretrain, Epoch [76 / 300]: loss 0.3279, training auc: 0.8133, val_auc 0.7858\n",
      "2022-06-30 01:01:21,171 - NCNet pretrain, Epoch [77 / 300]: loss 0.3288, training auc: 0.8135, val_auc 0.7886\n",
      "2022-06-30 01:01:21,309 - NCNet pretrain, Epoch [78 / 300]: loss 0.3178, training auc: 0.8240, val_auc 0.7865\n",
      "2022-06-30 01:01:21,434 - NCNet pretrain, Epoch [79 / 300]: loss 0.3213, training auc: 0.8147, val_auc 0.7881\n",
      "2022-06-30 01:01:21,564 - NCNet pretrain, Epoch [80 / 300]: loss 0.3252, training auc: 0.8122, val_auc 0.7910, test auc 0.7992\n",
      "2022-06-30 01:01:21,702 - NCNet pretrain, Epoch [81 / 300]: loss 0.3129, training auc: 0.8345, val_auc 0.7890\n",
      "2022-06-30 01:01:21,824 - NCNet pretrain, Epoch [82 / 300]: loss 0.3228, training auc: 0.8174, val_auc 0.7880\n",
      "2022-06-30 01:01:21,942 - NCNet pretrain, Epoch [83 / 300]: loss 0.3218, training auc: 0.8266, val_auc 0.7908\n",
      "2022-06-30 01:01:22,070 - NCNet pretrain, Epoch [84 / 300]: loss 0.3232, training auc: 0.8205, val_auc 0.7914, test auc 0.8012\n",
      "2022-06-30 01:01:22,188 - NCNet pretrain, Epoch [85 / 300]: loss 0.3127, training auc: 0.8298, val_auc 0.7882\n",
      "2022-06-30 01:01:22,317 - NCNet pretrain, Epoch [86 / 300]: loss 0.3135, training auc: 0.8386, val_auc 0.7930, test auc 0.8045\n",
      "2022-06-30 01:01:22,436 - NCNet pretrain, Epoch [87 / 300]: loss 0.3179, training auc: 0.8332, val_auc 0.7924\n",
      "2022-06-30 01:01:22,555 - NCNet pretrain, Epoch [88 / 300]: loss 0.3114, training auc: 0.8364, val_auc 0.7874\n",
      "2022-06-30 01:01:22,684 - NCNet pretrain, Epoch [89 / 300]: loss 0.3177, training auc: 0.8307, val_auc 0.7941, test auc 0.8081\n",
      "2022-06-30 01:01:22,804 - NCNet pretrain, Epoch [90 / 300]: loss 0.3179, training auc: 0.8349, val_auc 0.7879\n",
      "2022-06-30 01:01:22,922 - NCNet pretrain, Epoch [91 / 300]: loss 0.3107, training auc: 0.8294, val_auc 0.7864\n",
      "2022-06-30 01:01:23,052 - NCNet pretrain, Epoch [92 / 300]: loss 0.3263, training auc: 0.8147, val_auc 0.7943, test auc 0.8103\n",
      "2022-06-30 01:01:23,171 - NCNet pretrain, Epoch [93 / 300]: loss 0.3169, training auc: 0.8472, val_auc 0.7887\n",
      "2022-06-30 01:01:23,290 - NCNet pretrain, Epoch [94 / 300]: loss 0.3074, training auc: 0.8373, val_auc 0.7838\n",
      "2022-06-30 01:01:23,409 - NCNet pretrain, Epoch [95 / 300]: loss 0.3123, training auc: 0.8359, val_auc 0.7916\n",
      "2022-06-30 01:01:23,535 - NCNet pretrain, Epoch [96 / 300]: loss 0.3116, training auc: 0.8443, val_auc 0.7926\n",
      "2022-06-30 01:01:23,655 - NCNet pretrain, Epoch [97 / 300]: loss 0.3080, training auc: 0.8520, val_auc 0.7841\n",
      "2022-06-30 01:01:23,773 - NCNet pretrain, Epoch [98 / 300]: loss 0.3121, training auc: 0.8343, val_auc 0.7856\n",
      "2022-06-30 01:01:23,897 - NCNet pretrain, Epoch [99 / 300]: loss 0.3091, training auc: 0.8450, val_auc 0.7941\n",
      "2022-06-30 01:01:24,024 - NCNet pretrain, Epoch [100 / 300]: loss 0.3191, training auc: 0.8520, val_auc 0.7867\n",
      "2022-06-30 01:01:24,149 - NCNet pretrain, Epoch [101 / 300]: loss 0.3139, training auc: 0.8334, val_auc 0.7871\n",
      "2022-06-30 01:01:24,266 - NCNet pretrain, Epoch [102 / 300]: loss 0.3110, training auc: 0.8339, val_auc 0.7930\n",
      "2022-06-30 01:01:24,384 - NCNet pretrain, Epoch [103 / 300]: loss 0.3095, training auc: 0.8509, val_auc 0.7909\n",
      "2022-06-30 01:01:24,502 - NCNet pretrain, Epoch [104 / 300]: loss 0.3012, training auc: 0.8472, val_auc 0.7874\n",
      "2022-06-30 01:01:24,620 - NCNet pretrain, Epoch [105 / 300]: loss 0.3024, training auc: 0.8415, val_auc 0.7913\n",
      "2022-06-30 01:01:24,739 - NCNet pretrain, Epoch [106 / 300]: loss 0.2976, training auc: 0.8530, val_auc 0.7932\n",
      "2022-06-30 01:01:24,858 - NCNet pretrain, Epoch [107 / 300]: loss 0.3017, training auc: 0.8586, val_auc 0.7889\n",
      "2022-06-30 01:01:24,976 - NCNet pretrain, Epoch [108 / 300]: loss 0.3060, training auc: 0.8416, val_auc 0.7910\n",
      "2022-06-30 01:01:25,094 - NCNet pretrain, Epoch [109 / 300]: loss 0.3043, training auc: 0.8411, val_auc 0.7927\n",
      "2022-06-30 01:01:25,227 - NCNet pretrain, Epoch [110 / 300]: loss 0.2973, training auc: 0.8602, val_auc 0.7897\n",
      "2022-06-30 01:01:25,370 - NCNet pretrain, Epoch [111 / 300]: loss 0.3037, training auc: 0.8493, val_auc 0.7893\n",
      "2022-06-30 01:01:25,514 - NCNet pretrain, Epoch [112 / 300]: loss 0.3029, training auc: 0.8466, val_auc 0.7926\n",
      "2022-06-30 01:01:25,659 - NCNet pretrain, Epoch [113 / 300]: loss 0.2935, training auc: 0.8610, val_auc 0.7895\n",
      "2022-06-30 01:01:25,795 - NCNet pretrain, Epoch [114 / 300]: loss 0.2934, training auc: 0.8545, val_auc 0.7877\n",
      "2022-06-30 01:01:25,937 - NCNet pretrain, Epoch [115 / 300]: loss 0.2983, training auc: 0.8489, val_auc 0.7917\n",
      "2022-06-30 01:01:26,078 - NCNet pretrain, Epoch [116 / 300]: loss 0.2987, training auc: 0.8600, val_auc 0.7889\n",
      "2022-06-30 01:01:26,215 - NCNet pretrain, Epoch [117 / 300]: loss 0.2961, training auc: 0.8526, val_auc 0.7856\n",
      "2022-06-30 01:01:26,352 - NCNet pretrain, Epoch [118 / 300]: loss 0.2951, training auc: 0.8547, val_auc 0.7907\n",
      "2022-06-30 01:01:26,490 - NCNet pretrain, Epoch [119 / 300]: loss 0.3051, training auc: 0.8484, val_auc 0.7867\n",
      "2022-06-30 01:01:26,627 - NCNet pretrain, Epoch [120 / 300]: loss 0.2976, training auc: 0.8519, val_auc 0.7910\n",
      "2022-06-30 01:01:26,765 - NCNet pretrain, Epoch [121 / 300]: loss 0.3013, training auc: 0.8551, val_auc 0.7850\n",
      "2022-06-30 01:01:26,901 - NCNet pretrain, Epoch [122 / 300]: loss 0.2990, training auc: 0.8537, val_auc 0.7897\n",
      "2022-06-30 01:01:27,043 - NCNet pretrain, Epoch [123 / 300]: loss 0.2886, training auc: 0.8646, val_auc 0.7907\n",
      "2022-06-30 01:01:27,185 - NCNet pretrain, Epoch [124 / 300]: loss 0.3072, training auc: 0.8495, val_auc 0.7843\n",
      "2022-06-30 01:01:27,316 - NCNet pretrain, Epoch [125 / 300]: loss 0.3015, training auc: 0.8475, val_auc 0.7916\n",
      "2022-06-30 01:01:27,447 - NCNet pretrain, Epoch [126 / 300]: loss 0.3027, training auc: 0.8571, val_auc 0.7849\n",
      "2022-06-30 01:01:27,587 - NCNet pretrain, Epoch [127 / 300]: loss 0.2917, training auc: 0.8586, val_auc 0.7866\n",
      "2022-06-30 01:01:27,722 - NCNet pretrain, Epoch [128 / 300]: loss 0.2889, training auc: 0.8601, val_auc 0.7912\n",
      "2022-06-30 01:01:27,861 - NCNet pretrain, Epoch [129 / 300]: loss 0.2965, training auc: 0.8660, val_auc 0.7768\n",
      "2022-06-30 01:01:28,005 - NCNet pretrain, Epoch [130 / 300]: loss 0.3121, training auc: 0.8488, val_auc 0.7896\n",
      "2022-06-30 01:01:28,156 - NCNet pretrain, Epoch [131 / 300]: loss 0.2941, training auc: 0.8592, val_auc 0.7892\n",
      "2022-06-30 01:01:28,326 - NCNet pretrain, Epoch [132 / 300]: loss 0.2909, training auc: 0.8638, val_auc 0.7792\n",
      "2022-06-30 01:01:28,466 - NCNet pretrain, Epoch [133 / 300]: loss 0.3036, training auc: 0.8566, val_auc 0.7883\n",
      "2022-06-30 01:01:28,606 - NCNet pretrain, Epoch [134 / 300]: loss 0.2876, training auc: 0.8670, val_auc 0.7883\n",
      "2022-06-30 01:01:28,746 - NCNet pretrain, Epoch [135 / 300]: loss 0.2883, training auc: 0.8743, val_auc 0.7753\n",
      "2022-06-30 01:01:28,866 - NCNet pretrain, Epoch [136 / 300]: loss 0.3069, training auc: 0.8409, val_auc 0.7855\n",
      "2022-06-30 01:01:28,986 - NCNet pretrain, Epoch [137 / 300]: loss 0.2891, training auc: 0.8658, val_auc 0.7874\n",
      "2022-06-30 01:01:29,107 - NCNet pretrain, Epoch [138 / 300]: loss 0.3199, training auc: 0.8684, val_auc 0.7636\n",
      "2022-06-30 01:01:29,228 - NCNet pretrain, Epoch [139 / 300]: loss 0.3302, training auc: 0.8374, val_auc 0.7704\n",
      "2022-06-30 01:01:29,348 - NCNet pretrain, Epoch [140 / 300]: loss 0.3262, training auc: 0.8408, val_auc 0.7855\n",
      "2022-06-30 01:01:29,488 - NCNet pretrain, Epoch [141 / 300]: loss 0.3359, training auc: 0.8658, val_auc 0.7842\n",
      "2022-06-30 01:01:29,612 - NCNet pretrain, Epoch [142 / 300]: loss 0.2858, training auc: 0.8655, val_auc 0.7726\n",
      "2022-06-30 01:01:29,613 - Early stop!\n",
      "2022-06-30 01:01:29,614 - Best Test Results: auc 0.8103, ap 0.4355, f1 0.3165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 247655.31it/s]\n",
      "2022-06-30 01:01:30,767 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 01:01:30,955 - NCNet pretrain, Epoch [1 / 300]: loss 0.8010, training auc: 0.4865, val_auc 0.4513, test auc 0.4185\n",
      "2022-06-30 01:01:31,090 - NCNet pretrain, Epoch [2 / 300]: loss 0.4892, training auc: 0.4130, val_auc 0.4558, test auc 0.4250\n",
      "2022-06-30 01:01:31,220 - NCNet pretrain, Epoch [3 / 300]: loss 0.4852, training auc: 0.4182, val_auc 0.4561, test auc 0.4260\n",
      "2022-06-30 01:01:31,339 - NCNet pretrain, Epoch [4 / 300]: loss 0.5163, training auc: 0.4008, val_auc 0.4556\n",
      "2022-06-30 01:01:31,470 - NCNet pretrain, Epoch [5 / 300]: loss 0.5023, training auc: 0.3895, val_auc 0.4562, test auc 0.4283\n",
      "2022-06-30 01:01:31,602 - NCNet pretrain, Epoch [6 / 300]: loss 0.4803, training auc: 0.3920, val_auc 0.4586, test auc 0.4322\n",
      "2022-06-30 01:01:31,738 - NCNet pretrain, Epoch [7 / 300]: loss 0.4560, training auc: 0.4719, val_auc 0.4635, test auc 0.4383\n",
      "2022-06-30 01:01:31,874 - NCNet pretrain, Epoch [8 / 300]: loss 0.4643, training auc: 0.4500, val_auc 0.4672, test auc 0.4429\n",
      "2022-06-30 01:01:32,006 - NCNet pretrain, Epoch [9 / 300]: loss 0.4632, training auc: 0.4721, val_auc 0.4689, test auc 0.4447\n",
      "2022-06-30 01:01:32,151 - NCNet pretrain, Epoch [10 / 300]: loss 0.4638, training auc: 0.4756, val_auc 0.4700, test auc 0.4454\n",
      "2022-06-30 01:01:32,288 - NCNet pretrain, Epoch [11 / 300]: loss 0.4591, training auc: 0.4567, val_auc 0.4703, test auc 0.4453\n",
      "2022-06-30 01:01:32,418 - NCNet pretrain, Epoch [12 / 300]: loss 0.4613, training auc: 0.4258, val_auc 0.4708, test auc 0.4451\n",
      "2022-06-30 01:01:32,552 - NCNet pretrain, Epoch [13 / 300]: loss 0.4657, training auc: 0.4245, val_auc 0.4720, test auc 0.4460\n",
      "2022-06-30 01:01:32,682 - NCNet pretrain, Epoch [14 / 300]: loss 0.4619, training auc: 0.4369, val_auc 0.4731, test auc 0.4472\n",
      "2022-06-30 01:01:32,813 - NCNet pretrain, Epoch [15 / 300]: loss 0.4620, training auc: 0.4358, val_auc 0.4744, test auc 0.4489\n",
      "2022-06-30 01:01:32,944 - NCNet pretrain, Epoch [16 / 300]: loss 0.4618, training auc: 0.4260, val_auc 0.4757, test auc 0.4509\n",
      "2022-06-30 01:01:33,074 - NCNet pretrain, Epoch [17 / 300]: loss 0.4487, training auc: 0.4793, val_auc 0.4772, test auc 0.4526\n",
      "2022-06-30 01:01:33,205 - NCNet pretrain, Epoch [18 / 300]: loss 0.4551, training auc: 0.4443, val_auc 0.4783, test auc 0.4543\n",
      "2022-06-30 01:01:33,349 - NCNet pretrain, Epoch [19 / 300]: loss 0.4593, training auc: 0.4263, val_auc 0.4796, test auc 0.4558\n",
      "2022-06-30 01:01:33,479 - NCNet pretrain, Epoch [20 / 300]: loss 0.4563, training auc: 0.4533, val_auc 0.4811, test auc 0.4572\n",
      "2022-06-30 01:01:33,630 - NCNet pretrain, Epoch [21 / 300]: loss 0.4572, training auc: 0.4407, val_auc 0.4827, test auc 0.4586\n",
      "2022-06-30 01:01:33,781 - NCNet pretrain, Epoch [22 / 300]: loss 0.4546, training auc: 0.4615, val_auc 0.4840, test auc 0.4596\n",
      "2022-06-30 01:01:33,912 - NCNet pretrain, Epoch [23 / 300]: loss 0.4518, training auc: 0.4563, val_auc 0.4854, test auc 0.4605\n",
      "2022-06-30 01:01:34,043 - NCNet pretrain, Epoch [24 / 300]: loss 0.4488, training auc: 0.4747, val_auc 0.4869, test auc 0.4615\n",
      "2022-06-30 01:01:34,176 - NCNet pretrain, Epoch [25 / 300]: loss 0.4453, training auc: 0.4838, val_auc 0.4885, test auc 0.4628\n",
      "2022-06-30 01:01:34,307 - NCNet pretrain, Epoch [26 / 300]: loss 0.4534, training auc: 0.4476, val_auc 0.4908, test auc 0.4646\n",
      "2022-06-30 01:01:34,438 - NCNet pretrain, Epoch [27 / 300]: loss 0.4478, training auc: 0.4553, val_auc 0.4930, test auc 0.4670\n",
      "2022-06-30 01:01:34,577 - NCNet pretrain, Epoch [28 / 300]: loss 0.4402, training auc: 0.5022, val_auc 0.4951, test auc 0.4693\n",
      "2022-06-30 01:01:34,707 - NCNet pretrain, Epoch [29 / 300]: loss 0.4446, training auc: 0.4665, val_auc 0.4976, test auc 0.4717\n",
      "2022-06-30 01:01:34,839 - NCNet pretrain, Epoch [30 / 300]: loss 0.4454, training auc: 0.4820, val_auc 0.5002, test auc 0.4744\n",
      "2022-06-30 01:01:34,969 - NCNet pretrain, Epoch [31 / 300]: loss 0.4462, training auc: 0.4563, val_auc 0.5038, test auc 0.4778\n",
      "2022-06-30 01:01:35,099 - NCNet pretrain, Epoch [32 / 300]: loss 0.4427, training auc: 0.4725, val_auc 0.5079, test auc 0.4822\n",
      "2022-06-30 01:01:35,229 - NCNet pretrain, Epoch [33 / 300]: loss 0.4398, training auc: 0.4843, val_auc 0.5128, test auc 0.4870\n",
      "2022-06-30 01:01:35,360 - NCNet pretrain, Epoch [34 / 300]: loss 0.4383, training auc: 0.4942, val_auc 0.5172, test auc 0.4910\n",
      "2022-06-30 01:01:35,490 - NCNet pretrain, Epoch [35 / 300]: loss 0.4435, training auc: 0.4689, val_auc 0.5221, test auc 0.4958\n",
      "2022-06-30 01:01:35,623 - NCNet pretrain, Epoch [36 / 300]: loss 0.4373, training auc: 0.4890, val_auc 0.5281, test auc 0.5013\n",
      "2022-06-30 01:01:35,754 - NCNet pretrain, Epoch [37 / 300]: loss 0.4313, training auc: 0.5204, val_auc 0.5345, test auc 0.5081\n",
      "2022-06-30 01:01:35,885 - NCNet pretrain, Epoch [38 / 300]: loss 0.4269, training auc: 0.5490, val_auc 0.5418, test auc 0.5154\n",
      "2022-06-30 01:01:36,015 - NCNet pretrain, Epoch [39 / 300]: loss 0.4306, training auc: 0.5237, val_auc 0.5514, test auc 0.5253\n",
      "2022-06-30 01:01:36,146 - NCNet pretrain, Epoch [40 / 300]: loss 0.4178, training auc: 0.5946, val_auc 0.5589, test auc 0.5326\n",
      "2022-06-30 01:01:36,278 - NCNet pretrain, Epoch [41 / 300]: loss 0.4220, training auc: 0.5658, val_auc 0.5672, test auc 0.5413\n",
      "2022-06-30 01:01:36,410 - NCNet pretrain, Epoch [42 / 300]: loss 0.4174, training auc: 0.5838, val_auc 0.5765, test auc 0.5507\n",
      "2022-06-30 01:01:36,540 - NCNet pretrain, Epoch [43 / 300]: loss 0.4200, training auc: 0.5671, val_auc 0.5890, test auc 0.5638\n",
      "2022-06-30 01:01:36,676 - NCNet pretrain, Epoch [44 / 300]: loss 0.4236, training auc: 0.5540, val_auc 0.6062, test auc 0.5816\n",
      "2022-06-30 01:01:36,811 - NCNet pretrain, Epoch [45 / 300]: loss 0.4182, training auc: 0.5759, val_auc 0.6201, test auc 0.5952\n",
      "2022-06-30 01:01:36,942 - NCNet pretrain, Epoch [46 / 300]: loss 0.4090, training auc: 0.6267, val_auc 0.6298, test auc 0.6042\n",
      "2022-06-30 01:01:37,072 - NCNet pretrain, Epoch [47 / 300]: loss 0.4110, training auc: 0.6195, val_auc 0.6368, test auc 0.6101\n",
      "2022-06-30 01:01:37,202 - NCNet pretrain, Epoch [48 / 300]: loss 0.4128, training auc: 0.5976, val_auc 0.6545, test auc 0.6271\n",
      "2022-06-30 01:01:37,332 - NCNet pretrain, Epoch [49 / 300]: loss 0.4094, training auc: 0.6153, val_auc 0.6792, test auc 0.6531\n",
      "2022-06-30 01:01:37,481 - NCNet pretrain, Epoch [50 / 300]: loss 0.4037, training auc: 0.6580, val_auc 0.6880, test auc 0.6615\n",
      "2022-06-30 01:01:37,601 - NCNet pretrain, Epoch [51 / 300]: loss 0.4004, training auc: 0.6669, val_auc 0.6835\n",
      "2022-06-30 01:01:37,726 - NCNet pretrain, Epoch [52 / 300]: loss 0.3962, training auc: 0.6651, val_auc 0.6883, test auc 0.6602\n",
      "2022-06-30 01:01:37,858 - NCNet pretrain, Epoch [53 / 300]: loss 0.3935, training auc: 0.6737, val_auc 0.7054, test auc 0.6816\n",
      "2022-06-30 01:01:37,989 - NCNet pretrain, Epoch [54 / 300]: loss 0.3989, training auc: 0.6556, val_auc 0.7199, test auc 0.7017\n",
      "2022-06-30 01:01:38,109 - NCNet pretrain, Epoch [55 / 300]: loss 0.3884, training auc: 0.7095, val_auc 0.7136\n",
      "2022-06-30 01:01:38,229 - NCNet pretrain, Epoch [56 / 300]: loss 0.3848, training auc: 0.6975, val_auc 0.7106\n",
      "2022-06-30 01:01:38,360 - NCNet pretrain, Epoch [57 / 300]: loss 0.3849, training auc: 0.6936, val_auc 0.7303, test auc 0.7151\n",
      "2022-06-30 01:01:38,493 - NCNet pretrain, Epoch [58 / 300]: loss 0.3776, training auc: 0.7235, val_auc 0.7381, test auc 0.7252\n",
      "2022-06-30 01:01:38,614 - NCNet pretrain, Epoch [59 / 300]: loss 0.3812, training auc: 0.7341, val_auc 0.7289\n",
      "2022-06-30 01:01:38,735 - NCNet pretrain, Epoch [60 / 300]: loss 0.3737, training auc: 0.7236, val_auc 0.7262\n",
      "2022-06-30 01:01:38,871 - NCNet pretrain, Epoch [61 / 300]: loss 0.3812, training auc: 0.7107, val_auc 0.7393, test auc 0.7262\n",
      "2022-06-30 01:01:39,002 - NCNet pretrain, Epoch [62 / 300]: loss 0.3727, training auc: 0.7316, val_auc 0.7492, test auc 0.7408\n",
      "2022-06-30 01:01:39,131 - NCNet pretrain, Epoch [63 / 300]: loss 0.3823, training auc: 0.7382, val_auc 0.7429\n",
      "2022-06-30 01:01:39,271 - NCNet pretrain, Epoch [64 / 300]: loss 0.3655, training auc: 0.7525, val_auc 0.7350\n",
      "2022-06-30 01:01:39,390 - NCNet pretrain, Epoch [65 / 300]: loss 0.3738, training auc: 0.7233, val_auc 0.7457\n",
      "2022-06-30 01:01:39,521 - NCNet pretrain, Epoch [66 / 300]: loss 0.3635, training auc: 0.7501, val_auc 0.7548, test auc 0.7484\n",
      "2022-06-30 01:01:39,641 - NCNet pretrain, Epoch [67 / 300]: loss 0.3666, training auc: 0.7631, val_auc 0.7510\n",
      "2022-06-30 01:01:39,766 - NCNet pretrain, Epoch [68 / 300]: loss 0.3659, training auc: 0.7516, val_auc 0.7487\n",
      "2022-06-30 01:01:39,885 - NCNet pretrain, Epoch [69 / 300]: loss 0.3603, training auc: 0.7535, val_auc 0.7519\n",
      "2022-06-30 01:01:40,015 - NCNet pretrain, Epoch [70 / 300]: loss 0.3612, training auc: 0.7503, val_auc 0.7583, test auc 0.7524\n",
      "2022-06-30 01:01:40,137 - NCNet pretrain, Epoch [71 / 300]: loss 0.3626, training auc: 0.7592, val_auc 0.7582\n",
      "2022-06-30 01:01:40,276 - NCNet pretrain, Epoch [72 / 300]: loss 0.3595, training auc: 0.7617, val_auc 0.7553\n",
      "2022-06-30 01:01:40,407 - NCNet pretrain, Epoch [73 / 300]: loss 0.3685, training auc: 0.7437, val_auc 0.7596, test auc 0.7534\n",
      "2022-06-30 01:01:40,539 - NCNet pretrain, Epoch [74 / 300]: loss 0.3701, training auc: 0.7414, val_auc 0.7632, test auc 0.7588\n",
      "2022-06-30 01:01:40,674 - NCNet pretrain, Epoch [75 / 300]: loss 0.3573, training auc: 0.7709, val_auc 0.7639, test auc 0.7596\n",
      "2022-06-30 01:01:40,794 - NCNet pretrain, Epoch [76 / 300]: loss 0.3593, training auc: 0.7626, val_auc 0.7625\n",
      "2022-06-30 01:01:40,927 - NCNet pretrain, Epoch [77 / 300]: loss 0.3465, training auc: 0.7766, val_auc 0.7658, test auc 0.7622\n",
      "2022-06-30 01:01:41,066 - NCNet pretrain, Epoch [78 / 300]: loss 0.3520, training auc: 0.7718, val_auc 0.7695, test auc 0.7686\n",
      "2022-06-30 01:01:41,186 - NCNet pretrain, Epoch [79 / 300]: loss 0.3543, training auc: 0.7743, val_auc 0.7688\n",
      "2022-06-30 01:01:41,306 - NCNet pretrain, Epoch [80 / 300]: loss 0.3578, training auc: 0.7669, val_auc 0.7600\n",
      "2022-06-30 01:01:41,454 - NCNet pretrain, Epoch [81 / 300]: loss 0.3526, training auc: 0.7716, val_auc 0.7702, test auc 0.7690\n",
      "2022-06-30 01:01:41,582 - NCNet pretrain, Epoch [82 / 300]: loss 0.3353, training auc: 0.8035, val_auc 0.7758, test auc 0.7797\n",
      "2022-06-30 01:01:41,699 - NCNet pretrain, Epoch [83 / 300]: loss 0.3587, training auc: 0.7833, val_auc 0.7667\n",
      "2022-06-30 01:01:41,817 - NCNet pretrain, Epoch [84 / 300]: loss 0.3470, training auc: 0.7818, val_auc 0.7638\n",
      "2022-06-30 01:01:41,960 - NCNet pretrain, Epoch [85 / 300]: loss 0.3450, training auc: 0.7926, val_auc 0.7758, test auc 0.7785\n",
      "2022-06-30 01:01:42,091 - NCNet pretrain, Epoch [86 / 300]: loss 0.3482, training auc: 0.7833, val_auc 0.7781, test auc 0.7825\n",
      "2022-06-30 01:01:42,208 - NCNet pretrain, Epoch [87 / 300]: loss 0.3434, training auc: 0.8075, val_auc 0.7646\n",
      "2022-06-30 01:01:42,326 - NCNet pretrain, Epoch [88 / 300]: loss 0.3422, training auc: 0.7896, val_auc 0.7722\n",
      "2022-06-30 01:01:42,455 - NCNet pretrain, Epoch [89 / 300]: loss 0.3408, training auc: 0.7987, val_auc 0.7787, test auc 0.7848\n",
      "2022-06-30 01:01:42,593 - NCNet pretrain, Epoch [90 / 300]: loss 0.3431, training auc: 0.8137, val_auc 0.7711\n",
      "2022-06-30 01:01:42,730 - NCNet pretrain, Epoch [91 / 300]: loss 0.3354, training auc: 0.8066, val_auc 0.7670\n",
      "2022-06-30 01:01:42,871 - NCNet pretrain, Epoch [92 / 300]: loss 0.3434, training auc: 0.7867, val_auc 0.7739\n",
      "2022-06-30 01:01:42,990 - NCNet pretrain, Epoch [93 / 300]: loss 0.3398, training auc: 0.7991, val_auc 0.7771\n",
      "2022-06-30 01:01:43,108 - NCNet pretrain, Epoch [94 / 300]: loss 0.3468, training auc: 0.7977, val_auc 0.7711\n",
      "2022-06-30 01:01:43,229 - NCNet pretrain, Epoch [95 / 300]: loss 0.3390, training auc: 0.8053, val_auc 0.7710\n",
      "2022-06-30 01:01:43,351 - NCNet pretrain, Epoch [96 / 300]: loss 0.3339, training auc: 0.8055, val_auc 0.7774\n",
      "2022-06-30 01:01:43,468 - NCNet pretrain, Epoch [97 / 300]: loss 0.3352, training auc: 0.8037, val_auc 0.7780\n",
      "2022-06-30 01:01:43,586 - NCNet pretrain, Epoch [98 / 300]: loss 0.3408, training auc: 0.8073, val_auc 0.7689\n",
      "2022-06-30 01:01:43,706 - NCNet pretrain, Epoch [99 / 300]: loss 0.3404, training auc: 0.7946, val_auc 0.7753\n",
      "2022-06-30 01:01:43,835 - NCNet pretrain, Epoch [100 / 300]: loss 0.3287, training auc: 0.8099, val_auc 0.7800, test auc 0.7911\n",
      "2022-06-30 01:01:43,953 - NCNet pretrain, Epoch [101 / 300]: loss 0.3378, training auc: 0.8170, val_auc 0.7753\n",
      "2022-06-30 01:01:44,070 - NCNet pretrain, Epoch [102 / 300]: loss 0.3338, training auc: 0.8142, val_auc 0.7742\n",
      "2022-06-30 01:01:44,207 - NCNet pretrain, Epoch [103 / 300]: loss 0.3363, training auc: 0.8100, val_auc 0.7796\n",
      "2022-06-30 01:01:44,325 - NCNet pretrain, Epoch [104 / 300]: loss 0.3259, training auc: 0.8273, val_auc 0.7791\n",
      "2022-06-30 01:01:44,442 - NCNet pretrain, Epoch [105 / 300]: loss 0.3273, training auc: 0.8133, val_auc 0.7722\n",
      "2022-06-30 01:01:44,561 - NCNet pretrain, Epoch [106 / 300]: loss 0.3271, training auc: 0.8122, val_auc 0.7768\n",
      "2022-06-30 01:01:44,678 - NCNet pretrain, Epoch [107 / 300]: loss 0.3283, training auc: 0.8176, val_auc 0.7800\n",
      "2022-06-30 01:01:44,796 - NCNet pretrain, Epoch [108 / 300]: loss 0.3219, training auc: 0.8272, val_auc 0.7741\n",
      "2022-06-30 01:01:44,915 - NCNet pretrain, Epoch [109 / 300]: loss 0.3264, training auc: 0.8150, val_auc 0.7790\n",
      "2022-06-30 01:01:45,043 - NCNet pretrain, Epoch [110 / 300]: loss 0.3299, training auc: 0.8087, val_auc 0.7806, test auc 0.7936\n",
      "2022-06-30 01:01:45,164 - NCNet pretrain, Epoch [111 / 300]: loss 0.3318, training auc: 0.8212, val_auc 0.7776\n",
      "2022-06-30 01:01:45,282 - NCNet pretrain, Epoch [112 / 300]: loss 0.3248, training auc: 0.8262, val_auc 0.7765\n",
      "2022-06-30 01:01:45,412 - NCNet pretrain, Epoch [113 / 300]: loss 0.3314, training auc: 0.8081, val_auc 0.7811, test auc 0.7969\n",
      "2022-06-30 01:01:45,538 - NCNet pretrain, Epoch [114 / 300]: loss 0.3265, training auc: 0.8325, val_auc 0.7785\n",
      "2022-06-30 01:01:45,655 - NCNet pretrain, Epoch [115 / 300]: loss 0.3196, training auc: 0.8220, val_auc 0.7718\n",
      "2022-06-30 01:01:45,775 - NCNet pretrain, Epoch [116 / 300]: loss 0.3296, training auc: 0.8142, val_auc 0.7794\n",
      "2022-06-30 01:01:45,893 - NCNet pretrain, Epoch [117 / 300]: loss 0.3209, training auc: 0.8321, val_auc 0.7798\n",
      "2022-06-30 01:01:46,011 - NCNet pretrain, Epoch [118 / 300]: loss 0.3279, training auc: 0.8186, val_auc 0.7760\n",
      "2022-06-30 01:01:46,148 - NCNet pretrain, Epoch [119 / 300]: loss 0.3284, training auc: 0.8160, val_auc 0.7768\n",
      "2022-06-30 01:01:46,265 - NCNet pretrain, Epoch [120 / 300]: loss 0.3222, training auc: 0.8189, val_auc 0.7799\n",
      "2022-06-30 01:01:46,383 - NCNet pretrain, Epoch [121 / 300]: loss 0.3223, training auc: 0.8291, val_auc 0.7787\n",
      "2022-06-30 01:01:46,500 - NCNet pretrain, Epoch [122 / 300]: loss 0.3132, training auc: 0.8355, val_auc 0.7759\n",
      "2022-06-30 01:01:46,618 - NCNet pretrain, Epoch [123 / 300]: loss 0.3182, training auc: 0.8269, val_auc 0.7791\n",
      "2022-06-30 01:01:46,739 - NCNet pretrain, Epoch [124 / 300]: loss 0.3190, training auc: 0.8240, val_auc 0.7803\n",
      "2022-06-30 01:01:46,857 - NCNet pretrain, Epoch [125 / 300]: loss 0.3180, training auc: 0.8351, val_auc 0.7779\n",
      "2022-06-30 01:01:46,976 - NCNet pretrain, Epoch [126 / 300]: loss 0.3178, training auc: 0.8313, val_auc 0.7751\n",
      "2022-06-30 01:01:47,094 - NCNet pretrain, Epoch [127 / 300]: loss 0.3162, training auc: 0.8301, val_auc 0.7809\n",
      "2022-06-30 01:01:47,233 - NCNet pretrain, Epoch [128 / 300]: loss 0.3202, training auc: 0.8371, val_auc 0.7801\n",
      "2022-06-30 01:01:47,370 - NCNet pretrain, Epoch [129 / 300]: loss 0.3203, training auc: 0.8329, val_auc 0.7755\n",
      "2022-06-30 01:01:47,494 - NCNet pretrain, Epoch [130 / 300]: loss 0.3205, training auc: 0.8251, val_auc 0.7806\n",
      "2022-06-30 01:01:47,617 - NCNet pretrain, Epoch [131 / 300]: loss 0.3174, training auc: 0.8298, val_auc 0.7809\n",
      "2022-06-30 01:01:47,740 - NCNet pretrain, Epoch [132 / 300]: loss 0.3134, training auc: 0.8384, val_auc 0.7788\n",
      "2022-06-30 01:01:47,859 - NCNet pretrain, Epoch [133 / 300]: loss 0.3273, training auc: 0.8207, val_auc 0.7770\n",
      "2022-06-30 01:01:47,977 - NCNet pretrain, Epoch [134 / 300]: loss 0.3096, training auc: 0.8408, val_auc 0.7808\n",
      "2022-06-30 01:01:48,094 - NCNet pretrain, Epoch [135 / 300]: loss 0.3061, training auc: 0.8462, val_auc 0.7797\n",
      "2022-06-30 01:01:48,212 - NCNet pretrain, Epoch [136 / 300]: loss 0.3114, training auc: 0.8361, val_auc 0.7784\n",
      "2022-06-30 01:01:48,329 - NCNet pretrain, Epoch [137 / 300]: loss 0.3139, training auc: 0.8257, val_auc 0.7781\n",
      "2022-06-30 01:01:48,447 - NCNet pretrain, Epoch [138 / 300]: loss 0.3128, training auc: 0.8402, val_auc 0.7800\n",
      "2022-06-30 01:01:48,565 - NCNet pretrain, Epoch [139 / 300]: loss 0.3104, training auc: 0.8386, val_auc 0.7802\n",
      "2022-06-30 01:01:48,683 - NCNet pretrain, Epoch [140 / 300]: loss 0.3070, training auc: 0.8434, val_auc 0.7789\n",
      "2022-06-30 01:01:48,802 - NCNet pretrain, Epoch [141 / 300]: loss 0.3069, training auc: 0.8434, val_auc 0.7798\n",
      "2022-06-30 01:01:48,922 - NCNet pretrain, Epoch [142 / 300]: loss 0.3147, training auc: 0.8369, val_auc 0.7768\n",
      "2022-06-30 01:01:49,040 - NCNet pretrain, Epoch [143 / 300]: loss 0.3070, training auc: 0.8429, val_auc 0.7794\n",
      "2022-06-30 01:01:49,177 - NCNet pretrain, Epoch [144 / 300]: loss 0.3045, training auc: 0.8415, val_auc 0.7787\n",
      "2022-06-30 01:01:49,314 - NCNet pretrain, Epoch [145 / 300]: loss 0.3012, training auc: 0.8517, val_auc 0.7782\n",
      "2022-06-30 01:01:49,452 - NCNet pretrain, Epoch [146 / 300]: loss 0.3093, training auc: 0.8421, val_auc 0.7760\n",
      "2022-06-30 01:01:49,588 - NCNet pretrain, Epoch [147 / 300]: loss 0.3042, training auc: 0.8496, val_auc 0.7784\n",
      "2022-06-30 01:01:49,719 - NCNet pretrain, Epoch [148 / 300]: loss 0.3062, training auc: 0.8503, val_auc 0.7742\n",
      "2022-06-30 01:01:49,861 - NCNet pretrain, Epoch [149 / 300]: loss 0.3096, training auc: 0.8388, val_auc 0.7780\n",
      "2022-06-30 01:01:49,978 - NCNet pretrain, Epoch [150 / 300]: loss 0.3066, training auc: 0.8487, val_auc 0.7776\n",
      "2022-06-30 01:01:50,096 - NCNet pretrain, Epoch [151 / 300]: loss 0.3055, training auc: 0.8420, val_auc 0.7754\n",
      "2022-06-30 01:01:50,214 - NCNet pretrain, Epoch [152 / 300]: loss 0.3076, training auc: 0.8425, val_auc 0.7773\n",
      "2022-06-30 01:01:50,351 - NCNet pretrain, Epoch [153 / 300]: loss 0.3001, training auc: 0.8536, val_auc 0.7769\n",
      "2022-06-30 01:01:50,488 - NCNet pretrain, Epoch [154 / 300]: loss 0.3061, training auc: 0.8525, val_auc 0.7739\n",
      "2022-06-30 01:01:50,606 - NCNet pretrain, Epoch [155 / 300]: loss 0.3060, training auc: 0.8454, val_auc 0.7776\n",
      "2022-06-30 01:01:50,745 - NCNet pretrain, Epoch [156 / 300]: loss 0.3038, training auc: 0.8569, val_auc 0.7773\n",
      "2022-06-30 01:01:50,863 - NCNet pretrain, Epoch [157 / 300]: loss 0.3080, training auc: 0.8460, val_auc 0.7775\n",
      "2022-06-30 01:01:50,982 - NCNet pretrain, Epoch [158 / 300]: loss 0.2997, training auc: 0.8560, val_auc 0.7744\n",
      "2022-06-30 01:01:51,100 - NCNet pretrain, Epoch [159 / 300]: loss 0.2993, training auc: 0.8529, val_auc 0.7772\n",
      "2022-06-30 01:01:51,219 - NCNet pretrain, Epoch [160 / 300]: loss 0.3014, training auc: 0.8579, val_auc 0.7717\n",
      "2022-06-30 01:01:51,338 - NCNet pretrain, Epoch [161 / 300]: loss 0.3095, training auc: 0.8383, val_auc 0.7769\n",
      "2022-06-30 01:01:51,457 - NCNet pretrain, Epoch [162 / 300]: loss 0.3046, training auc: 0.8580, val_auc 0.7693\n",
      "2022-06-30 01:01:51,577 - NCNet pretrain, Epoch [163 / 300]: loss 0.3081, training auc: 0.8482, val_auc 0.7756\n",
      "2022-06-30 01:01:51,578 - Early stop!\n",
      "2022-06-30 01:01:51,580 - Best Test Results: auc 0.7969, ap 0.4031, f1 0.2415\n"
     ]
    }
   ],
   "source": [
    "for rate in rates:\n",
    "        auc_res, ap_res = [], []\n",
    "        for _ in range(20):\n",
    "            auc, ap = run(dataset, rate, name=f'{dataset}_{method}_{rate}_{rnn}', baseline=baseline, gnnlayer_type=method, rnnlayer_type=rnn, device=device)\n",
    "            auc_res.append(auc)\n",
    "            ap_res.append(ap)\n",
    "        with open(f'ELANDe2e_{dataset}_{method}_{rate}_{rnn}_try.txt', 'a') as f:\n",
    "            f.write(f'auc: {np.mean(auc_res)} +- {np.std(auc_res)}, ap: {np.mean(ap_res)} +- {np.std(ap_res)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd41461c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7815773090163335,\n",
       " 0.800525263939898,\n",
       " 0.7993128145567171,\n",
       " 0.815706385828337,\n",
       " 0.7999445438469828,\n",
       " 0.8092262558725973,\n",
       " 0.7970272101369662,\n",
       " 0.8121704021094265,\n",
       " 0.7854572778353265,\n",
       " 0.7997640497640498,\n",
       " 0.8031574693160058,\n",
       " 0.8126929194002366,\n",
       " 0.8037211863431375,\n",
       " 0.7922094568436032,\n",
       " 0.7854075765661132,\n",
       " 0.8040534000899855,\n",
       " 0.8062206370133198,\n",
       " 0.7910624038672819,\n",
       " 0.8102804459511777,\n",
       " 0.7969376170595682]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92d07287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3835175775414199,\n",
       " 0.4188607136460357,\n",
       " 0.4096221560568864,\n",
       " 0.44033756451677947,\n",
       " 0.42489229519446203,\n",
       " 0.4293152467335025,\n",
       " 0.40993789257391433,\n",
       " 0.436776757984043,\n",
       " 0.4022491584864704,\n",
       " 0.4143178366148888,\n",
       " 0.41319479676545195,\n",
       " 0.44085022617260056,\n",
       " 0.4166372442776068,\n",
       " 0.4087923448209431,\n",
       " 0.389768553307066,\n",
       " 0.433510374732659,\n",
       " 0.4276592625626203,\n",
       " 0.39842701710859607,\n",
       " 0.4354548495427667,\n",
       " 0.40314497268356725]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e073e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 832 positive users and 6000 labeled users\n",
      "Train: total of  1200 users with   174 pos users and  1026 neg users\n",
      "Val:   total of  1200 users with   166 pos users and  1034 neg users\n",
      "Test:  total of  3600 users with   492 pos users and  3108 neg users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178841it [00:00, 252819.70it/s]\n",
      "2022-06-30 01:18:32,911 - Parameters: {'base_pred': 30, 'device': device(type='cuda', index=0), 'bmloss_type': 'mse', 'alpha': 0.05, 'pretrain_nc': 300, 'pretrain_bm': 25, 'rnn_type': 'gru', 'gnnlayer_type': 'gcn', 'name': 'reddit_gcn_0.3_gru', 'log': True, 'tensorboard': False, 'dropout': 0.4, 'weight_decay': 1e-05, 'lr': 0.01, 'seed': -1, 'epochs': 400, 'n_layers': 2, 'hidden_size': 128, 'cuda': 0, 'dim_feats': 300}\n",
      "2022-06-30 01:18:33,101 - NCNet pretrain, Epoch [1 / 300]: loss 0.4542, training auc: 0.5758, val_auc 0.4818, test auc 0.4613\n",
      "2022-06-30 01:18:33,225 - NCNet pretrain, Epoch [2 / 300]: loss 0.4429, training auc: 0.4414, val_auc 0.4816\n",
      "2022-06-30 01:18:33,372 - NCNet pretrain, Epoch [3 / 300]: loss 0.4369, training auc: 0.4597, val_auc 0.4912, test auc 0.4755\n",
      "2022-06-30 01:18:33,505 - NCNet pretrain, Epoch [4 / 300]: loss 0.4294, training auc: 0.4867, val_auc 0.4988, test auc 0.4864\n",
      "2022-06-30 01:18:33,642 - NCNet pretrain, Epoch [5 / 300]: loss 0.4261, training auc: 0.5149, val_auc 0.5037, test auc 0.4949\n",
      "2022-06-30 01:18:33,782 - NCNet pretrain, Epoch [6 / 300]: loss 0.4307, training auc: 0.4864, val_auc 0.5051, test auc 0.4960\n",
      "2022-06-30 01:18:33,904 - NCNet pretrain, Epoch [7 / 300]: loss 0.4312, training auc: 0.4884, val_auc 0.5037\n",
      "2022-06-30 01:18:34,025 - NCNet pretrain, Epoch [8 / 300]: loss 0.4291, training auc: 0.4856, val_auc 0.5008\n",
      "2022-06-30 01:18:34,146 - NCNet pretrain, Epoch [9 / 300]: loss 0.4315, training auc: 0.4250, val_auc 0.4988\n",
      "2022-06-30 01:18:34,267 - NCNet pretrain, Epoch [10 / 300]: loss 0.4262, training auc: 0.4748, val_auc 0.4982\n",
      "2022-06-30 01:18:34,388 - NCNet pretrain, Epoch [11 / 300]: loss 0.4266, training auc: 0.4644, val_auc 0.4990\n",
      "2022-06-30 01:18:34,507 - NCNet pretrain, Epoch [12 / 300]: loss 0.4261, training auc: 0.4625, val_auc 0.4998\n",
      "2022-06-30 01:18:34,627 - NCNet pretrain, Epoch [13 / 300]: loss 0.4234, training auc: 0.4855, val_auc 0.5034\n",
      "2022-06-30 01:18:34,772 - NCNet pretrain, Epoch [14 / 300]: loss 0.4179, training auc: 0.5589, val_auc 0.5082, test auc 0.4841\n",
      "2022-06-30 01:18:34,897 - NCNet pretrain, Epoch [15 / 300]: loss 0.4170, training auc: 0.5456, val_auc 0.5109, test auc 0.4861\n",
      "2022-06-30 01:18:35,023 - NCNet pretrain, Epoch [16 / 300]: loss 0.4173, training auc: 0.5345, val_auc 0.5125, test auc 0.4864\n",
      "2022-06-30 01:18:35,149 - NCNet pretrain, Epoch [17 / 300]: loss 0.4164, training auc: 0.5426, val_auc 0.5181, test auc 0.4907\n",
      "2022-06-30 01:18:35,280 - NCNet pretrain, Epoch [18 / 300]: loss 0.4189, training auc: 0.4975, val_auc 0.5266, test auc 0.4979\n",
      "2022-06-30 01:18:35,412 - NCNet pretrain, Epoch [19 / 300]: loss 0.4118, training auc: 0.5726, val_auc 0.5417, test auc 0.5111\n",
      "2022-06-30 01:18:35,544 - NCNet pretrain, Epoch [20 / 300]: loss 0.4226, training auc: 0.4522, val_auc 0.5658, test auc 0.5320\n",
      "2022-06-30 01:18:35,696 - NCNet pretrain, Epoch [21 / 300]: loss 0.4174, training auc: 0.4956, val_auc 0.5904, test auc 0.5536\n",
      "2022-06-30 01:18:35,840 - NCNet pretrain, Epoch [22 / 300]: loss 0.4110, training auc: 0.5829, val_auc 0.6106, test auc 0.5699\n",
      "2022-06-30 01:18:35,970 - NCNet pretrain, Epoch [23 / 300]: loss 0.4097, training auc: 0.5988, val_auc 0.6191, test auc 0.5774\n",
      "2022-06-30 01:18:36,101 - NCNet pretrain, Epoch [24 / 300]: loss 0.4135, training auc: 0.5436, val_auc 0.6344, test auc 0.5911\n",
      "2022-06-30 01:18:36,231 - NCNet pretrain, Epoch [25 / 300]: loss 0.4081, training auc: 0.6102, val_auc 0.6559, test auc 0.6106\n",
      "2022-06-30 01:18:36,362 - NCNet pretrain, Epoch [26 / 300]: loss 0.4038, training auc: 0.6257, val_auc 0.6945, test auc 0.6479\n",
      "2022-06-30 01:18:36,493 - NCNet pretrain, Epoch [27 / 300]: loss 0.4040, training auc: 0.6146, val_auc 0.7236, test auc 0.6811\n",
      "2022-06-30 01:18:36,634 - NCNet pretrain, Epoch [28 / 300]: loss 0.4026, training auc: 0.6122, val_auc 0.7311, test auc 0.6903\n",
      "2022-06-30 01:18:36,753 - NCNet pretrain, Epoch [29 / 300]: loss 0.3995, training auc: 0.6369, val_auc 0.7259\n",
      "2022-06-30 01:18:36,885 - NCNet pretrain, Epoch [30 / 300]: loss 0.3924, training auc: 0.6844, val_auc 0.7491, test auc 0.7144\n",
      "2022-06-30 01:18:37,006 - NCNet pretrain, Epoch [31 / 300]: loss 0.3912, training auc: 0.7120, val_auc 0.7444\n",
      "2022-06-30 01:18:37,138 - NCNet pretrain, Epoch [32 / 300]: loss 0.3865, training auc: 0.6984, val_auc 0.7557, test auc 0.7255\n",
      "2022-06-30 01:18:37,270 - NCNet pretrain, Epoch [33 / 300]: loss 0.3838, training auc: 0.7159, val_auc 0.7710, test auc 0.7521\n",
      "2022-06-30 01:18:37,391 - NCNet pretrain, Epoch [34 / 300]: loss 0.3793, training auc: 0.7544, val_auc 0.7559\n",
      "2022-06-30 01:18:37,543 - NCNet pretrain, Epoch [35 / 300]: loss 0.3777, training auc: 0.7214, val_auc 0.7789, test auc 0.7679\n",
      "2022-06-30 01:18:37,663 - NCNet pretrain, Epoch [36 / 300]: loss 0.3745, training auc: 0.7591, val_auc 0.7526\n",
      "2022-06-30 01:18:37,796 - NCNet pretrain, Epoch [37 / 300]: loss 0.3630, training auc: 0.7505, val_auc 0.7571\n",
      "2022-06-30 01:18:37,924 - NCNet pretrain, Epoch [38 / 300]: loss 0.3689, training auc: 0.7399, val_auc 0.7775\n",
      "2022-06-30 01:18:38,044 - NCNet pretrain, Epoch [39 / 300]: loss 0.3709, training auc: 0.7669, val_auc 0.7626\n",
      "2022-06-30 01:18:38,162 - NCNet pretrain, Epoch [40 / 300]: loss 0.3622, training auc: 0.7552, val_auc 0.7555\n",
      "2022-06-30 01:18:38,280 - NCNet pretrain, Epoch [41 / 300]: loss 0.3649, training auc: 0.7460, val_auc 0.7688\n",
      "2022-06-30 01:18:38,400 - NCNet pretrain, Epoch [42 / 300]: loss 0.3570, training auc: 0.7774, val_auc 0.7710\n",
      "2022-06-30 01:18:38,518 - NCNet pretrain, Epoch [43 / 300]: loss 0.3591, training auc: 0.7646, val_auc 0.7600\n",
      "2022-06-30 01:18:38,639 - NCNet pretrain, Epoch [44 / 300]: loss 0.3564, training auc: 0.7493, val_auc 0.7655\n",
      "2022-06-30 01:18:38,758 - NCNet pretrain, Epoch [45 / 300]: loss 0.3494, training auc: 0.7724, val_auc 0.7748\n",
      "2022-06-30 01:18:38,876 - NCNet pretrain, Epoch [46 / 300]: loss 0.3571, training auc: 0.7714, val_auc 0.7713\n",
      "2022-06-30 01:18:38,995 - NCNet pretrain, Epoch [47 / 300]: loss 0.3504, training auc: 0.7774, val_auc 0.7687\n",
      "2022-06-30 01:18:39,114 - NCNet pretrain, Epoch [48 / 300]: loss 0.3494, training auc: 0.7775, val_auc 0.7785\n",
      "2022-06-30 01:18:39,232 - NCNet pretrain, Epoch [49 / 300]: loss 0.3495, training auc: 0.7918, val_auc 0.7764\n",
      "2022-06-30 01:18:39,369 - NCNet pretrain, Epoch [50 / 300]: loss 0.3539, training auc: 0.7676, val_auc 0.7726\n",
      "2022-06-30 01:18:39,499 - NCNet pretrain, Epoch [51 / 300]: loss 0.3389, training auc: 0.7827, val_auc 0.7814, test auc 0.7831\n",
      "2022-06-30 01:18:39,633 - NCNet pretrain, Epoch [52 / 300]: loss 0.3404, training auc: 0.7949, val_auc 0.7841, test auc 0.7871\n",
      "2022-06-30 01:18:39,752 - NCNet pretrain, Epoch [53 / 300]: loss 0.3417, training auc: 0.7907, val_auc 0.7779\n",
      "2022-06-30 01:18:39,870 - NCNet pretrain, Epoch [54 / 300]: loss 0.3411, training auc: 0.7844, val_auc 0.7834\n",
      "2022-06-30 01:18:40,005 - NCNet pretrain, Epoch [55 / 300]: loss 0.3303, training auc: 0.8049, val_auc 0.7893, test auc 0.7951\n",
      "2022-06-30 01:18:40,124 - NCNet pretrain, Epoch [56 / 300]: loss 0.3410, training auc: 0.8114, val_auc 0.7797\n",
      "2022-06-30 01:18:40,241 - NCNet pretrain, Epoch [57 / 300]: loss 0.3387, training auc: 0.7878, val_auc 0.7844\n",
      "2022-06-30 01:18:40,374 - NCNet pretrain, Epoch [58 / 300]: loss 0.3353, training auc: 0.7997, val_auc 0.7914, test auc 0.7978\n",
      "2022-06-30 01:18:40,493 - NCNet pretrain, Epoch [59 / 300]: loss 0.3386, training auc: 0.8112, val_auc 0.7790\n",
      "2022-06-30 01:18:40,611 - NCNet pretrain, Epoch [60 / 300]: loss 0.3421, training auc: 0.7908, val_auc 0.7821\n",
      "2022-06-30 01:18:40,742 - NCNet pretrain, Epoch [61 / 300]: loss 0.3403, training auc: 0.8011, val_auc 0.7930, test auc 0.8008\n",
      "2022-06-30 01:18:40,859 - NCNet pretrain, Epoch [62 / 300]: loss 0.3382, training auc: 0.8241, val_auc 0.7899\n",
      "2022-06-30 01:18:40,978 - NCNet pretrain, Epoch [63 / 300]: loss 0.3289, training auc: 0.8117, val_auc 0.7791\n",
      "2022-06-30 01:18:41,115 - NCNet pretrain, Epoch [64 / 300]: loss 0.3384, training auc: 0.7953, val_auc 0.7857\n",
      "2022-06-30 01:18:41,240 - NCNet pretrain, Epoch [65 / 300]: loss 0.3275, training auc: 0.8072, val_auc 0.7940, test auc 0.8007\n",
      "2022-06-30 01:18:41,358 - NCNet pretrain, Epoch [66 / 300]: loss 0.3375, training auc: 0.8124, val_auc 0.7924\n",
      "2022-06-30 01:18:41,496 - NCNet pretrain, Epoch [67 / 300]: loss 0.3237, training auc: 0.8175, val_auc 0.7860\n",
      "2022-06-30 01:18:41,615 - NCNet pretrain, Epoch [68 / 300]: loss 0.3288, training auc: 0.8138, val_auc 0.7888\n",
      "2022-06-30 01:18:41,736 - NCNet pretrain, Epoch [69 / 300]: loss 0.3261, training auc: 0.8164, val_auc 0.7923\n",
      "2022-06-30 01:18:41,855 - NCNet pretrain, Epoch [70 / 300]: loss 0.3247, training auc: 0.8272, val_auc 0.7929\n",
      "2022-06-30 01:18:41,975 - NCNet pretrain, Epoch [71 / 300]: loss 0.3199, training auc: 0.8343, val_auc 0.7883\n",
      "2022-06-30 01:18:42,097 - NCNet pretrain, Epoch [72 / 300]: loss 0.3260, training auc: 0.8142, val_auc 0.7930\n",
      "2022-06-30 01:18:42,218 - NCNet pretrain, Epoch [73 / 300]: loss 0.3166, training auc: 0.8275, val_auc 0.7933\n",
      "2022-06-30 01:18:42,336 - NCNet pretrain, Epoch [74 / 300]: loss 0.3200, training auc: 0.8317, val_auc 0.7903\n",
      "2022-06-30 01:18:42,454 - NCNet pretrain, Epoch [75 / 300]: loss 0.3180, training auc: 0.8304, val_auc 0.7920\n",
      "2022-06-30 01:18:42,583 - NCNet pretrain, Epoch [76 / 300]: loss 0.3164, training auc: 0.8342, val_auc 0.7947, test auc 0.8033\n",
      "2022-06-30 01:18:42,701 - NCNet pretrain, Epoch [77 / 300]: loss 0.3160, training auc: 0.8374, val_auc 0.7938\n",
      "2022-06-30 01:18:42,818 - NCNet pretrain, Epoch [78 / 300]: loss 0.3131, training auc: 0.8351, val_auc 0.7905\n",
      "2022-06-30 01:18:42,946 - NCNet pretrain, Epoch [79 / 300]: loss 0.3184, training auc: 0.8193, val_auc 0.7957, test auc 0.8059\n",
      "2022-06-30 01:18:43,084 - NCNet pretrain, Epoch [80 / 300]: loss 0.3102, training auc: 0.8456, val_auc 0.7924\n",
      "2022-06-30 01:18:43,222 - NCNet pretrain, Epoch [81 / 300]: loss 0.3102, training auc: 0.8338, val_auc 0.7944\n",
      "2022-06-30 01:18:43,340 - NCNet pretrain, Epoch [82 / 300]: loss 0.3087, training auc: 0.8431, val_auc 0.7950\n",
      "2022-06-30 01:18:43,459 - NCNet pretrain, Epoch [83 / 300]: loss 0.3148, training auc: 0.8365, val_auc 0.7922\n",
      "2022-06-30 01:18:43,578 - NCNet pretrain, Epoch [84 / 300]: loss 0.3145, training auc: 0.8287, val_auc 0.7947\n",
      "2022-06-30 01:18:43,699 - NCNet pretrain, Epoch [85 / 300]: loss 0.3128, training auc: 0.8383, val_auc 0.7888\n",
      "2022-06-30 01:18:43,817 - NCNet pretrain, Epoch [86 / 300]: loss 0.3125, training auc: 0.8291, val_auc 0.7939\n",
      "2022-06-30 01:18:43,936 - NCNet pretrain, Epoch [87 / 300]: loss 0.3121, training auc: 0.8433, val_auc 0.7909\n",
      "2022-06-30 01:18:44,054 - NCNet pretrain, Epoch [88 / 300]: loss 0.3042, training auc: 0.8409, val_auc 0.7932\n",
      "2022-06-30 01:18:44,196 - NCNet pretrain, Epoch [89 / 300]: loss 0.3117, training auc: 0.8414, val_auc 0.7950\n",
      "2022-06-30 01:18:44,320 - NCNet pretrain, Epoch [90 / 300]: loss 0.3066, training auc: 0.8457, val_auc 0.7864\n",
      "2022-06-30 01:18:44,438 - NCNet pretrain, Epoch [91 / 300]: loss 0.3109, training auc: 0.8391, val_auc 0.7946\n",
      "2022-06-30 01:18:44,574 - NCNet pretrain, Epoch [92 / 300]: loss 0.3131, training auc: 0.8446, val_auc 0.7928\n",
      "2022-06-30 01:18:44,695 - NCNet pretrain, Epoch [93 / 300]: loss 0.3087, training auc: 0.8463, val_auc 0.7856\n",
      "2022-06-30 01:18:44,812 - NCNet pretrain, Epoch [94 / 300]: loss 0.3120, training auc: 0.8353, val_auc 0.7946\n",
      "2022-06-30 01:18:44,949 - NCNet pretrain, Epoch [95 / 300]: loss 0.3150, training auc: 0.8445, val_auc 0.7867\n",
      "2022-06-30 01:18:45,085 - NCNet pretrain, Epoch [96 / 300]: loss 0.3143, training auc: 0.8345, val_auc 0.7917\n",
      "2022-06-30 01:18:45,203 - NCNet pretrain, Epoch [97 / 300]: loss 0.2969, training auc: 0.8518, val_auc 0.7931\n",
      "2022-06-30 01:18:45,321 - NCNet pretrain, Epoch [98 / 300]: loss 0.3059, training auc: 0.8551, val_auc 0.7824\n",
      "2022-06-30 01:18:45,460 - NCNet pretrain, Epoch [99 / 300]: loss 0.3213, training auc: 0.8348, val_auc 0.7931\n",
      "2022-06-30 01:18:45,578 - NCNet pretrain, Epoch [100 / 300]: loss 0.3073, training auc: 0.8565, val_auc 0.7886\n",
      "2022-06-30 01:18:45,696 - NCNet pretrain, Epoch [101 / 300]: loss 0.2960, training auc: 0.8539, val_auc 0.7852\n",
      "2022-06-30 01:18:45,814 - NCNet pretrain, Epoch [102 / 300]: loss 0.3099, training auc: 0.8405, val_auc 0.7935\n",
      "2022-06-30 01:18:45,934 - NCNet pretrain, Epoch [103 / 300]: loss 0.3126, training auc: 0.8506, val_auc 0.7922\n",
      "2022-06-30 01:18:46,053 - NCNet pretrain, Epoch [104 / 300]: loss 0.2976, training auc: 0.8540, val_auc 0.7880\n",
      "2022-06-30 01:18:46,172 - NCNet pretrain, Epoch [105 / 300]: loss 0.3041, training auc: 0.8483, val_auc 0.7938\n",
      "2022-06-30 01:18:46,295 - NCNet pretrain, Epoch [106 / 300]: loss 0.2997, training auc: 0.8576, val_auc 0.7933\n",
      "2022-06-30 01:18:46,420 - NCNet pretrain, Epoch [107 / 300]: loss 0.3032, training auc: 0.8503, val_auc 0.7897\n",
      "2022-06-30 01:18:46,539 - NCNet pretrain, Epoch [108 / 300]: loss 0.2979, training auc: 0.8442, val_auc 0.7912\n",
      "2022-06-30 01:18:46,676 - NCNet pretrain, Epoch [109 / 300]: loss 0.2918, training auc: 0.8585, val_auc 0.7934\n",
      "2022-06-30 01:18:46,795 - NCNet pretrain, Epoch [110 / 300]: loss 0.2983, training auc: 0.8582, val_auc 0.7918\n",
      "2022-06-30 01:18:46,912 - NCNet pretrain, Epoch [111 / 300]: loss 0.3011, training auc: 0.8496, val_auc 0.7887\n",
      "2022-06-30 01:18:47,030 - NCNet pretrain, Epoch [112 / 300]: loss 0.2931, training auc: 0.8536, val_auc 0.7933\n",
      "2022-06-30 01:18:47,149 - NCNet pretrain, Epoch [113 / 300]: loss 0.3014, training auc: 0.8637, val_auc 0.7894\n",
      "2022-06-30 01:18:47,267 - NCNet pretrain, Epoch [114 / 300]: loss 0.2926, training auc: 0.8473, val_auc 0.7910\n",
      "2022-06-30 01:18:47,385 - NCNet pretrain, Epoch [115 / 300]: loss 0.2990, training auc: 0.8520, val_auc 0.7916\n",
      "2022-06-30 01:18:47,522 - NCNet pretrain, Epoch [116 / 300]: loss 0.2944, training auc: 0.8626, val_auc 0.7887\n",
      "2022-06-30 01:18:47,641 - NCNet pretrain, Epoch [117 / 300]: loss 0.2952, training auc: 0.8588, val_auc 0.7908\n",
      "2022-06-30 01:18:47,761 - NCNet pretrain, Epoch [118 / 300]: loss 0.2926, training auc: 0.8630, val_auc 0.7836\n",
      "2022-06-30 01:18:47,880 - NCNet pretrain, Epoch [119 / 300]: loss 0.2933, training auc: 0.8521, val_auc 0.7902\n",
      "2022-06-30 01:18:47,999 - NCNet pretrain, Epoch [120 / 300]: loss 0.2944, training auc: 0.8689, val_auc 0.7843\n",
      "2022-06-30 01:18:48,119 - NCNet pretrain, Epoch [121 / 300]: loss 0.2922, training auc: 0.8557, val_auc 0.7845\n",
      "2022-06-30 01:18:48,248 - NCNet pretrain, Epoch [122 / 300]: loss 0.2893, training auc: 0.8602, val_auc 0.7902\n",
      "2022-06-30 01:18:48,367 - NCNet pretrain, Epoch [123 / 300]: loss 0.3040, training auc: 0.8670, val_auc 0.7685\n",
      "2022-06-30 01:18:48,493 - NCNet pretrain, Epoch [124 / 300]: loss 0.3292, training auc: 0.8363, val_auc 0.7900\n",
      "2022-06-30 01:18:48,611 - NCNet pretrain, Epoch [125 / 300]: loss 0.2995, training auc: 0.8693, val_auc 0.7893\n",
      "2022-06-30 01:18:48,732 - NCNet pretrain, Epoch [126 / 300]: loss 0.2824, training auc: 0.8807, val_auc 0.7766\n",
      "2022-06-30 01:18:48,850 - NCNet pretrain, Epoch [127 / 300]: loss 0.3072, training auc: 0.8563, val_auc 0.7898\n",
      "2022-06-30 01:18:48,967 - NCNet pretrain, Epoch [128 / 300]: loss 0.2867, training auc: 0.8798, val_auc 0.7894\n",
      "2022-06-30 01:18:49,104 - NCNet pretrain, Epoch [129 / 300]: loss 0.2932, training auc: 0.8707, val_auc 0.7803\n",
      "2022-06-30 01:18:49,105 - Early stop!\n",
      "2022-06-30 01:18:49,106 - Best Test Results: auc 0.8059, ap 0.4290, f1 0.2747\n"
     ]
    }
   ],
   "source": [
    "baseline = 'True'\n",
    "for rate in rates:\n",
    "        auc_res, ap_res = [], []\n",
    "        for _ in range(1):\n",
    "            auc, ap = run(dataset, rate, name=f'{dataset}_{method}_{rate}_{rnn}', baseline=baseline, gnnlayer_type=method, rnnlayer_type=rnn, device=device)\n",
    "            auc_res.append(auc)\n",
    "            ap_res.append(ap)\n",
    "        with open(f'ELANDe2e_{dataset}_{method}_{rate}_{rnn}_try.txt', 'a') as f:\n",
    "            f.write(f'auc: {np.mean(auc_res)} +- {np.std(auc_res)}, ap: {np.mean(ap_res)} +- {np.std(ap_res)}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-eland-venv",
   "language": "python",
   "name": "kedro-eland-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b87a4b",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c8f01",
   "metadata": {},
   "source": [
    "# This notebook is prepared to show eland model training results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9ffe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/anomaly-detection-spatial-temporal-data/src/kedro-eland-venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import pickle as pk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss, CosineEmbeddingLoss\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd29201",
   "metadata": {},
   "source": [
    "## loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95bfdcd",
   "metadata": {},
   "source": [
    "### user label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55b1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_label = pd.read_csv(\"../../data/02_intermediate/user_behavior/user_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90dc87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultimatt42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jonknee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dons</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jedravent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burtonmkz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sblinn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WebZen</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doodahdei</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tack122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author  label\n",
       "0    ultimatt42      0\n",
       "1       jonknee      0\n",
       "2          dons      0\n",
       "3     Jedravent      0\n",
       "4     burtonmkz      0\n",
       "5  pavel_lishin      0\n",
       "6        sblinn      0\n",
       "7        WebZen      0\n",
       "8     doodahdei      0\n",
       "9       Tack122      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_label.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4817b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb6f9722",
   "metadata": {},
   "source": [
    "## user and subreddit topic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa92583",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/u2index.pkl\",\"rb\") as f:\n",
    "    u2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46add0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0_o': 0,\n",
       " '138': 1,\n",
       " '13ren': 2,\n",
       " '1812overture': 3,\n",
       " '1esproc': 4,\n",
       " '315was_an_inside_job': 5,\n",
       " '43P04T34': 6,\n",
       " '7oby': 7,\n",
       " 'AAjax': 8,\n",
       " 'ABabyAteMyDingo': 9,\n",
       " 'ANSICL': 10,\n",
       " 'AbouBenAdhem': 11,\n",
       " 'Aerik': 12,\n",
       " 'Ajenthavoc': 13,\n",
       " 'AliasHandler': 14,\n",
       " 'AmericanGoyBlog': 15,\n",
       " 'AngelaMotorman': 16,\n",
       " 'AngledLuffa': 17,\n",
       " 'Anonymous7777': 18,\n",
       " 'AnteChronos': 19,\n",
       " 'ApostrophePosse': 20,\n",
       " 'ArcticCelt': 21,\n",
       " 'Bagel': 22,\n",
       " 'Battleloser': 23,\n",
       " 'BedtimeForSheeple': 24,\n",
       " 'BeetleB': 25,\n",
       " 'Benny_Lava': 26,\n",
       " 'Bensch': 27,\n",
       " 'Bixie': 28,\n",
       " 'Bloodlustt': 29,\n",
       " 'Bloody_Eye': 30,\n",
       " 'BlueBeard': 31,\n",
       " 'BobGaffney': 32,\n",
       " 'BraveSirRobin': 33,\n",
       " 'BrianBoyko': 34,\n",
       " 'Browzer': 35,\n",
       " 'Burlapin': 36,\n",
       " 'Busybyeski': 37,\n",
       " 'CampusTour': 38,\n",
       " 'CannedMango': 39,\n",
       " 'Captain-Obliviouss': 40,\n",
       " 'Chirp08': 41,\n",
       " 'ChunkyLaFunga': 42,\n",
       " 'Ciserus': 43,\n",
       " 'Clothos': 44,\n",
       " 'CodeMonkey1': 45,\n",
       " 'Codebender': 46,\n",
       " 'ColdSnickersBar': 47,\n",
       " 'Cookie': 48,\n",
       " 'CrackIsGoodForYou': 49,\n",
       " 'CrimsonSun99': 50,\n",
       " 'D-Style': 51,\n",
       " 'DCGaymer': 52,\n",
       " 'DOGA': 53,\n",
       " 'DaDibbel': 54,\n",
       " 'Dafuzz': 55,\n",
       " 'Dallas442': 56,\n",
       " 'Dark-Dx': 57,\n",
       " 'DarkSideofOZ': 58,\n",
       " 'Darkmeerkat': 59,\n",
       " 'Dax420': 60,\n",
       " 'Deacon': 61,\n",
       " 'Deestan': 62,\n",
       " 'Dildozer': 63,\n",
       " 'Doeke': 64,\n",
       " 'Doomdoomkittydoom': 65,\n",
       " 'Doomed': 66,\n",
       " 'DoorFrame': 67,\n",
       " 'Dr-No': 68,\n",
       " 'Drevor': 69,\n",
       " 'DudeAsInCool': 70,\n",
       " 'Dzazter': 71,\n",
       " 'EFG': 72,\n",
       " 'Eijin': 73,\n",
       " 'El_Guapo': 74,\n",
       " 'EndymionAwake': 75,\n",
       " 'Entropy': 76,\n",
       " 'Erudecorp': 77,\n",
       " 'Etab': 78,\n",
       " 'EvilPigeon': 79,\n",
       " 'ExplodingBob': 80,\n",
       " 'FANGO': 81,\n",
       " 'FMERCURY': 82,\n",
       " 'Farsay': 83,\n",
       " 'FeedMePlease': 84,\n",
       " 'FenPhen': 85,\n",
       " 'Fidodo': 86,\n",
       " 'Filmore': 87,\n",
       " 'FionaSarah': 88,\n",
       " 'FlySwat': 89,\n",
       " 'Flyen': 90,\n",
       " 'FrancisC': 91,\n",
       " 'Fulltangviper': 92,\n",
       " 'G_Morgan': 93,\n",
       " 'Gargilius': 94,\n",
       " 'GeorgeWBush': 95,\n",
       " 'GetToTheKarateChoppa': 96,\n",
       " 'Grimalkin': 97,\n",
       " 'Grue': 98,\n",
       " 'GrumpySimon': 99,\n",
       " 'GunnerMcGrath': 100,\n",
       " 'Guybrush_Threepwood': 101,\n",
       " 'HardwareLust': 102,\n",
       " 'Haroshia': 103,\n",
       " 'Haven': 104,\n",
       " 'HerbertMcSherbert': 105,\n",
       " 'Hubso': 106,\n",
       " 'HumanSockPuppet': 107,\n",
       " 'HunterTV': 108,\n",
       " 'IConrad': 109,\n",
       " 'I_AM_A_NEOCON': 110,\n",
       " 'Icanhazreddit': 111,\n",
       " 'Indyhouse': 112,\n",
       " 'InkyChan': 113,\n",
       " 'Ioewe': 114,\n",
       " 'James_Johnson': 115,\n",
       " 'JarvisCocker': 116,\n",
       " 'JasonDJ': 117,\n",
       " 'Jedravent': 118,\n",
       " 'JimDabell': 119,\n",
       " 'Jimmy': 120,\n",
       " 'Jivlain': 121,\n",
       " 'Johny_Cash': 122,\n",
       " 'JulianMorrison': 123,\n",
       " 'Kardlonoc': 124,\n",
       " 'KazamaSmokers': 125,\n",
       " 'Kestral': 126,\n",
       " 'Klowner': 127,\n",
       " 'LeviDon': 128,\n",
       " 'Lizard': 129,\n",
       " 'Lukifer': 130,\n",
       " 'Lystrodom': 131,\n",
       " 'MadScientist420': 132,\n",
       " 'MarkByers': 133,\n",
       " 'MarlonBain': 134,\n",
       " 'MarshallBanana': 135,\n",
       " 'MaximumBob': 136,\n",
       " 'McGuirk': 137,\n",
       " 'Midwest_Product': 138,\n",
       " 'MisterEggs': 139,\n",
       " 'MrFlesh': 140,\n",
       " 'MrKlaatu': 141,\n",
       " 'Mr_Smartypants': 142,\n",
       " 'MyaloMark': 143,\n",
       " 'Mythrilfan': 144,\n",
       " 'NSMike': 145,\n",
       " 'NadsatBrat': 146,\n",
       " 'NancyGracesTesticles': 147,\n",
       " 'NastyConde': 148,\n",
       " 'Nate_W': 149,\n",
       " 'Nefelia': 150,\n",
       " 'NoControl': 151,\n",
       " 'NoMoreNicksLeft': 152,\n",
       " 'NoSalt': 153,\n",
       " 'Notmyrealname': 154,\n",
       " 'OMouse': 155,\n",
       " 'Oak': 156,\n",
       " 'OlympicPirate': 157,\n",
       " 'Orangutan': 158,\n",
       " 'OriginalSyn': 159,\n",
       " 'Oryx': 160,\n",
       " 'Osmanthus': 161,\n",
       " 'Othello': 162,\n",
       " 'Papper': 163,\n",
       " 'Petrarch1603': 164,\n",
       " 'Philluminati': 165,\n",
       " 'Pikajabroni': 166,\n",
       " 'Pilebsa': 167,\n",
       " 'PlasmaWhore': 168,\n",
       " 'Poromenos': 169,\n",
       " 'Poultry_In_Motion': 170,\n",
       " 'ProximaC': 171,\n",
       " 'Prysorra': 172,\n",
       " 'Purp': 173,\n",
       " 'Pxorp': 174,\n",
       " 'Qubed': 175,\n",
       " 'Quel': 176,\n",
       " 'QuinnFazigu': 177,\n",
       " 'RKBA': 178,\n",
       " 'RainmadeMan': 179,\n",
       " 'Recoil42': 180,\n",
       " 'RedDyeNumber4': 181,\n",
       " 'ReiToei': 182,\n",
       " 'ReligionOfPeace': 183,\n",
       " 'Resilience': 184,\n",
       " 'RevLoveJoy': 185,\n",
       " 'RexManningDay': 186,\n",
       " 'Richeh': 187,\n",
       " 'RonObvious': 188,\n",
       " 'RonPaulTouchedMe': 189,\n",
       " 'RonaldFuckingPaul': 190,\n",
       " 'RugerRedhawk': 191,\n",
       " 'S7evyn': 192,\n",
       " 'Saiing': 193,\n",
       " 'SamHealer': 194,\n",
       " 'Sangermaine': 195,\n",
       " 'Sarm': 196,\n",
       " 'Satanscock': 197,\n",
       " 'Saydrah': 198,\n",
       " 'Scarker': 199,\n",
       " 'ScornForSega': 200,\n",
       " 'Shaper_pmp': 201,\n",
       " 'SirEdmund': 202,\n",
       " 'SkyMarshal': 203,\n",
       " 'Sle': 204,\n",
       " 'Slipgrid': 205,\n",
       " 'SlvrEagle23': 206,\n",
       " 'Smight': 207,\n",
       " 'SodiumKPump': 208,\n",
       " 'Spacksack': 209,\n",
       " 'Spazsquatch': 210,\n",
       " 'SpikeWolfwood': 211,\n",
       " 'Spudders': 212,\n",
       " 'Squarsh': 213,\n",
       " 'Sqwirl': 214,\n",
       " 'Stingray88': 215,\n",
       " 'StoneMe': 216,\n",
       " 'Stormflux': 217,\n",
       " 'SuperKing': 218,\n",
       " 'Tack122': 219,\n",
       " 'Taladar': 220,\n",
       " 'TheColonel': 221,\n",
       " 'TheKorn': 222,\n",
       " 'TheSOB88': 223,\n",
       " 'The_Ultimate_Reality': 224,\n",
       " 'Thimble': 225,\n",
       " 'ThisIsDave': 226,\n",
       " 'ThrasherC': 227,\n",
       " 'Tommah': 228,\n",
       " 'TripMaster_Monkey': 229,\n",
       " 'Twisted': 230,\n",
       " 'TwoToke': 231,\n",
       " 'UncleOxidant': 232,\n",
       " 'Unfair': 233,\n",
       " 'UntakenUsername': 234,\n",
       " 'Vash265': 235,\n",
       " 'VincentVega12': 236,\n",
       " 'VnlaThndr775': 237,\n",
       " 'VoodooIdol': 238,\n",
       " 'Vreep-eep': 239,\n",
       " 'WebZen': 240,\n",
       " 'Whisper': 241,\n",
       " 'Winoria': 242,\n",
       " 'WipeHandsOnPants': 243,\n",
       " 'Wo1ke': 244,\n",
       " 'XS4Me': 245,\n",
       " 'Xiphorian': 246,\n",
       " 'Yst': 247,\n",
       " 'Zweben': 248,\n",
       " 'a_little_perspective': 249,\n",
       " 'abrahamsen': 250,\n",
       " 'adaminc': 251,\n",
       " 'adleym': 252,\n",
       " 'admiralteal': 253,\n",
       " 'adremeaux': 254,\n",
       " 'adrianmonk': 255,\n",
       " 'aedes': 256,\n",
       " 'ajrw': 257,\n",
       " 'akatherder': 258,\n",
       " 'akatsukix': 259,\n",
       " 'akdas': 260,\n",
       " 'aktufe': 261,\n",
       " 'alaskamiller': 262,\n",
       " 'aletoledo': 263,\n",
       " 'allhands': 264,\n",
       " 'alllie': 265,\n",
       " 'alphabeat': 266,\n",
       " 'amstrdamordeath': 267,\n",
       " 'anachronic': 268,\n",
       " 'andrewd': 269,\n",
       " 'andrewnorris': 270,\n",
       " 'anescient': 271,\n",
       " 'anions': 272,\n",
       " 'anonymous-coward': 273,\n",
       " 'anthropology_nerd': 274,\n",
       " 'antifolkhero': 275,\n",
       " 'apathy': 276,\n",
       " 'api': 277,\n",
       " 'aradil': 278,\n",
       " 'argeaux': 279,\n",
       " 'argoff': 280,\n",
       " 'artman': 281,\n",
       " 'asaturn': 282,\n",
       " 'aschapm': 283,\n",
       " 'assteroid': 284,\n",
       " 'atomicthumbs': 285,\n",
       " 'aussie_bob': 286,\n",
       " 'automatedresponse': 287,\n",
       " 'axord': 288,\n",
       " 'ayrnieu': 289,\n",
       " 'b34nz': 290,\n",
       " 'b3mus3d': 291,\n",
       " 'bad_llama': 292,\n",
       " 'bamobrien': 293,\n",
       " 'bananahead': 294,\n",
       " 'barryicide': 295,\n",
       " 'bart2019': 296,\n",
       " 'baxyjr': 297,\n",
       " 'bazoople': 298,\n",
       " 'bbqribs': 299,\n",
       " 'bcash': 300,\n",
       " 'bebnet': 301,\n",
       " 'benihana': 302,\n",
       " 'blaze4metal': 303,\n",
       " 'bobcat': 304,\n",
       " 'bobpaul': 305,\n",
       " 'boredzo': 306,\n",
       " 'brad-walker': 307,\n",
       " 'brainburger': 308,\n",
       " 'break99': 309,\n",
       " 'breakneckridge': 310,\n",
       " 'brennen': 311,\n",
       " 'broohaha': 312,\n",
       " 'brosephius': 313,\n",
       " 'brufleth': 314,\n",
       " 'btl': 315,\n",
       " 'buffi': 316,\n",
       " 'burtonmkz': 317,\n",
       " 'bushwakko': 318,\n",
       " 'busytigger': 319,\n",
       " 'bw1870': 320,\n",
       " 'cabbit': 321,\n",
       " 'cajolingwilhelm': 322,\n",
       " 'calantorntain': 323,\n",
       " 'captainAwesomePants': 324,\n",
       " 'casicatracha': 325,\n",
       " 'casual_observer': 326,\n",
       " 'cbroberts': 327,\n",
       " 'cefm': 328,\n",
       " 'ch00f': 329,\n",
       " 'chall85': 330,\n",
       " 'christianjb': 331,\n",
       " 'chunky_bacon': 332,\n",
       " 'cocorobot': 333,\n",
       " 'codepoet': 334,\n",
       " 'commonslip': 335,\n",
       " 'contrarian': 336,\n",
       " 'corentin': 337,\n",
       " 'cov': 338,\n",
       " 'cracell': 339,\n",
       " 'crawfishsoul': 340,\n",
       " 'crazybones': 341,\n",
       " 'crusoe': 342,\n",
       " 'cryptoz': 343,\n",
       " 'cuddles666': 344,\n",
       " 'cum_pussy_tits_ass': 345,\n",
       " 'cup': 346,\n",
       " 'cweaver': 347,\n",
       " 'cwzwarich': 348,\n",
       " 'cyber_rigger': 349,\n",
       " 'cyks': 350,\n",
       " 'cypherx': 351,\n",
       " 'd07c0m': 352,\n",
       " 'd42': 353,\n",
       " 'daisy0808': 354,\n",
       " 'danweber': 355,\n",
       " 'darkgatherer': 356,\n",
       " 'davidreiss666': 357,\n",
       " 'db2': 358,\n",
       " 'dead_ed': 359,\n",
       " 'deadsoon': 360,\n",
       " 'defproc': 361,\n",
       " 'dellis': 362,\n",
       " 'derefr': 363,\n",
       " 'desrosiers': 364,\n",
       " 'deuteros': 365,\n",
       " 'dfranke': 366,\n",
       " 'dghughes': 367,\n",
       " 'dhusk': 368,\n",
       " 'diamond': 369,\n",
       " 'dicey': 370,\n",
       " 'digg_suxx_bigg': 371,\n",
       " 'digital': 372,\n",
       " 'diogames': 373,\n",
       " 'diversionmary': 374,\n",
       " 'dlds': 375,\n",
       " 'dmd': 376,\n",
       " 'dmiff': 377,\n",
       " 'dodus': 378,\n",
       " 'dons': 379,\n",
       " 'doodahdei': 380,\n",
       " 'doody': 381,\n",
       " 'dora_explorer': 382,\n",
       " 'dotrob': 383,\n",
       " 'downdiagonal': 384,\n",
       " 'doxiegrl1': 385,\n",
       " 'doyoulikeworms': 386,\n",
       " 'dpzdpz': 387,\n",
       " 'dsk': 388,\n",
       " 'duhblow7': 389,\n",
       " 'duus': 390,\n",
       " 'eOgas': 391,\n",
       " 'earthboundkid': 392,\n",
       " 'eaturbrainz': 393,\n",
       " 'eddie964': 394,\n",
       " 'edguy': 395,\n",
       " 'einexile': 396,\n",
       " 'ejp1082': 397,\n",
       " 'el_pinata': 398,\n",
       " 'elasticsoul': 399,\n",
       " 'elissa1959': 400,\n",
       " 'elquesogrande': 401,\n",
       " 'epicRelic': 402,\n",
       " 'epsilona01': 403,\n",
       " 'eroverton': 404,\n",
       " 'erulabs': 405,\n",
       " 'eusephus': 406,\n",
       " 'executivemonkey': 407,\n",
       " 'f0nd004u': 408,\n",
       " 'fangolo': 409,\n",
       " 'fapman': 410,\n",
       " 'ffualo': 411,\n",
       " 'fingers': 412,\n",
       " 'finix': 413,\n",
       " 'fjhqjv': 414,\n",
       " 'flydog2': 415,\n",
       " 'foonly': 416,\n",
       " 'formido': 417,\n",
       " 'free_man': 418,\n",
       " 'frickindeal': 419,\n",
       " 'frogking': 420,\n",
       " 'frukt': 421,\n",
       " 'frutiger': 422,\n",
       " 'fstorino': 423,\n",
       " 'fuckbuddy': 424,\n",
       " 'fujimitsu': 425,\n",
       " 'fun1ne': 426,\n",
       " 'furry8': 427,\n",
       " 'fwork': 428,\n",
       " 'g0taclue': 429,\n",
       " 'garg': 430,\n",
       " 'garyp714': 431,\n",
       " 'garyr_h': 432,\n",
       " 'geeeeoffff': 433,\n",
       " 'generic_handle': 434,\n",
       " 'geoff422': 435,\n",
       " 'gfixler': 436,\n",
       " 'gid13': 437,\n",
       " 'gigaquack': 438,\n",
       " 'gimeit': 439,\n",
       " 'glmory': 440,\n",
       " 'gmick': 441,\n",
       " 'godlesspinko': 442,\n",
       " 'gonorrhea': 443,\n",
       " 'goodbyeworld': 444,\n",
       " 'grauenwolf': 445,\n",
       " 'gregK': 446,\n",
       " 'greginnj': 447,\n",
       " 'grignr': 448,\n",
       " 'growinglotus': 449,\n",
       " 'guntotingliberal': 450,\n",
       " 'guriboysf': 451,\n",
       " 'gvsteve': 452,\n",
       " 'h0dg3s': 453,\n",
       " 'habbadash': 454,\n",
       " 'happysinger': 455,\n",
       " 'harryballsagna': 456,\n",
       " 'haywire': 457,\n",
       " 'haywire9000': 458,\n",
       " 'hellsbelles': 459,\n",
       " 'hiS_oWn': 460,\n",
       " 'hibryd': 461,\n",
       " 'hiredgoon': 462,\n",
       " 'hitler_rape_abortion': 463,\n",
       " 'hitmonval': 464,\n",
       " 'homeworld': 465,\n",
       " 'hongnanhai': 466,\n",
       " 'honus': 467,\n",
       " 'hotwingbias': 468,\n",
       " 'hpymondays': 469,\n",
       " 'hs4x': 470,\n",
       " 'hypo11': 471,\n",
       " 'illuminatedwax': 472,\n",
       " 'indycysive': 473,\n",
       " 'inferno0000': 474,\n",
       " 'infinite': 475,\n",
       " 'infinityvortex': 476,\n",
       " 'innocentbystander': 477,\n",
       " 'intellectual': 478,\n",
       " 'interstate': 479,\n",
       " 'j2d2': 480,\n",
       " 'jaalin': 481,\n",
       " 'jaggederest': 482,\n",
       " 'jamesallen74': 483,\n",
       " 'jamierc': 484,\n",
       " 'jax9999': 485,\n",
       " 'jayssite': 486,\n",
       " 'je255j': 487,\n",
       " 'jerf': 488,\n",
       " 'jesus4u': 489,\n",
       " 'jfowler27': 490,\n",
       " 'jingo04': 491,\n",
       " 'jjrs': 492,\n",
       " 'jodv': 493,\n",
       " 'joe90210': 494,\n",
       " 'johnfn': 495,\n",
       " 'jon_k': 496,\n",
       " 'jon_titor': 497,\n",
       " 'jones77': 498,\n",
       " 'jonknee': 499,\n",
       " 'jordanlund': 500,\n",
       " 'joshfern': 501,\n",
       " 'jotaroh': 502,\n",
       " 'joyork': 503,\n",
       " 'jozzas': 504,\n",
       " 'judgej2': 505,\n",
       " 'jumpyg1258': 506,\n",
       " 'junkeee999': 507,\n",
       " 'justinhj': 508,\n",
       " 'kalazar': 509,\n",
       " 'kanuk876': 510,\n",
       " 'kareems': 511,\n",
       " 'katsi': 512,\n",
       " 'kdraper': 513,\n",
       " 'kelmr2003': 514,\n",
       " 'kermityfrog': 515,\n",
       " 'ketralnis': 516,\n",
       " 'khafra': 517,\n",
       " 'khoury': 518,\n",
       " 'killerstorm': 519,\n",
       " 'kingkilr': 520,\n",
       " 'kjartanelli': 521,\n",
       " 'knylok': 522,\n",
       " 'kolm': 523,\n",
       " 'kousi': 524,\n",
       " 'kraemahz': 525,\n",
       " 'kraftmatic': 526,\n",
       " 'krizo': 527,\n",
       " 'krugerlive': 528,\n",
       " 'leemy': 529,\n",
       " 'liber8US': 530,\n",
       " 'linkedlist': 531,\n",
       " 'llanor': 532,\n",
       " 'locke2002': 533,\n",
       " 'lockhart000': 534,\n",
       " 'lugfish': 535,\n",
       " 'lylia': 536,\n",
       " 'm1ss1ontomars2k4': 537,\n",
       " 'machrider': 538,\n",
       " 'mackprime': 539,\n",
       " 'madmax_br5': 540,\n",
       " 'malcontent': 541,\n",
       " 'manthrax': 542,\n",
       " 'maqr': 543,\n",
       " 'marthirial': 544,\n",
       " 'martoo': 545,\n",
       " 'masklinn': 546,\n",
       " 'mcantelon': 547,\n",
       " 'mccoyn': 548,\n",
       " 'mchrisneglia': 549,\n",
       " 'me_so_porny': 550,\n",
       " 'megablahblah': 551,\n",
       " 'mexicodoug': 552,\n",
       " 'mindbleach': 553,\n",
       " 'mipadi': 554,\n",
       " 'miparasito': 555,\n",
       " 'mistermoxy': 556,\n",
       " 'miyakohouou': 557,\n",
       " 'mkrfctr': 558,\n",
       " 'mmazing': 559,\n",
       " 'moddestmouse': 560,\n",
       " 'moneyprinter': 561,\n",
       " 'moogle516': 562,\n",
       " 'mooglor': 563,\n",
       " 'moonman': 564,\n",
       " 'moonzilla': 565,\n",
       " 'moriya': 566,\n",
       " 'mothereffingtheresa': 567,\n",
       " 'movzx': 568,\n",
       " 'moxy': 569,\n",
       " 'mrbroom': 570,\n",
       " 'mschaef': 571,\n",
       " 'msdesireeg': 572,\n",
       " 'mturk': 573,\n",
       " 'muhfuhkuh': 574,\n",
       " 'mutatron': 575,\n",
       " 'mynameisdave': 576,\n",
       " 'mynameishere': 577,\n",
       " 'myotheralt': 578,\n",
       " 'mystery_guest': 579,\n",
       " 'natrius': 580,\n",
       " 'neat_stuff': 581,\n",
       " 'neoabraxas': 582,\n",
       " 'neoform3': 583,\n",
       " 'neonic': 584,\n",
       " 'neuquino': 585,\n",
       " 'nevesis': 586,\n",
       " 'nextofpumpkin': 587,\n",
       " 'nfulton': 588,\n",
       " 'ngngboone': 589,\n",
       " 'ngroot': 590,\n",
       " 'nickatnite101': 591,\n",
       " 'nicolaslloyd': 592,\n",
       " 'nihilite': 593,\n",
       " 'nixonrichard': 594,\n",
       " 'njharman': 595,\n",
       " 'nmcyall': 596,\n",
       " 'nooneelse': 597,\n",
       " 'noseeme': 598,\n",
       " 'notor': 599,\n",
       " 'ntr0p3': 600,\n",
       " 'numb3rb0y': 601,\n",
       " 'number6': 602,\n",
       " 'numlok': 603,\n",
       " 'oblivious_human': 604,\n",
       " 'oddmanout': 605,\n",
       " 'officemonkey': 606,\n",
       " 'old_gill': 607,\n",
       " 'ondal': 608,\n",
       " 'onebit': 609,\n",
       " 'oniony': 610,\n",
       " 'orthogonality': 611,\n",
       " 'otakucode': 612,\n",
       " 'otatop': 613,\n",
       " 'otterdam': 614,\n",
       " 'ouroborosity': 615,\n",
       " 'p3do': 616,\n",
       " 'pandemic': 617,\n",
       " 'paro': 618,\n",
       " 'pastanoose': 619,\n",
       " 'pavel_lishin': 620,\n",
       " 'pelirrojo': 621,\n",
       " 'pillage': 622,\n",
       " 'pilto': 623,\n",
       " 'pingish': 624,\n",
       " 'pozorvlak': 625,\n",
       " 'pradador': 626,\n",
       " 'protoopus': 627,\n",
       " 'psdtocode': 628,\n",
       " 'psychometry': 629,\n",
       " 'psyne': 630,\n",
       " 'qarl': 631,\n",
       " 'qgyh2': 632,\n",
       " 'quack': 633,\n",
       " 'quiller': 634,\n",
       " 'r2002': 635,\n",
       " 'raedix': 636,\n",
       " 'rainman_104': 637,\n",
       " 'ralphwiggum': 638,\n",
       " 'randomb0y': 639,\n",
       " 'raouldukeesq': 640,\n",
       " 'rats99ass': 641,\n",
       " 'readdit7': 642,\n",
       " 'rebop': 643,\n",
       " 'reddit_user13': 644,\n",
       " 'redditcensoredme': 645,\n",
       " 'redditrasberry': 646,\n",
       " 'redninja2000': 647,\n",
       " 'rek': 648,\n",
       " 'relic2279': 649,\n",
       " 'reverendfrag4': 650,\n",
       " 'ride': 651,\n",
       " 'rieux': 652,\n",
       " 'ristin': 653,\n",
       " 'rjcarr': 654,\n",
       " 'rmuser': 655,\n",
       " 'robdag2': 656,\n",
       " 'robodale': 657,\n",
       " 'robotevil': 658,\n",
       " 'robotfuel': 659,\n",
       " 'rockicon82': 660,\n",
       " 'rogerssucks': 661,\n",
       " 'roland19d': 662,\n",
       " 'ropers': 663,\n",
       " 'rory096': 664,\n",
       " 'rosshettel': 665,\n",
       " 'rthomas6': 666,\n",
       " 'ryanissuper': 667,\n",
       " 'ryanknapper': 668,\n",
       " 'ryanx27': 669,\n",
       " 'sakebomb69': 670,\n",
       " 'sam512': 671,\n",
       " 'san1ty': 672,\n",
       " 'sarahfrancesca': 673,\n",
       " 'satx': 674,\n",
       " 'sblinn': 675,\n",
       " 'scared1': 676,\n",
       " 'schizobullet': 677,\n",
       " 'sense': 678,\n",
       " 'sfultong': 679,\n",
       " 'shadowsurge': 680,\n",
       " 'shaurz': 681,\n",
       " 'shazbaz': 682,\n",
       " 'sheeprevolution': 683,\n",
       " 'shiner_man': 684,\n",
       " 'shitcovereddick': 685,\n",
       " 'shmi': 686,\n",
       " 'silence_hr': 687,\n",
       " 'sinsyder': 688,\n",
       " 'sketerpot': 689,\n",
       " 'skibybadoowap': 690,\n",
       " 'skippy17': 691,\n",
       " 'skitzh0': 692,\n",
       " 'slenderdog': 693,\n",
       " 'slidinglight': 694,\n",
       " 'slydee': 695,\n",
       " 'smitisme': 696,\n",
       " 'sn0re': 697,\n",
       " 'snair': 698,\n",
       " 'snifty': 699,\n",
       " 'snookums': 700,\n",
       " 'socks': 701,\n",
       " 'somn': 702,\n",
       " 'spaceghoti': 703,\n",
       " 'spez': 704,\n",
       " 'spif': 705,\n",
       " 'spookyvision': 706,\n",
       " 'squidboots': 707,\n",
       " 'squigs': 708,\n",
       " 'srika': 709,\n",
       " 'st_gulik': 710,\n",
       " 'stacecom': 711,\n",
       " 'state_of_alert': 712,\n",
       " 'stesch': 713,\n",
       " 'stomicron': 714,\n",
       " 'stopit': 715,\n",
       " 'stopmotionsunrise': 716,\n",
       " 'stunt_penguin': 717,\n",
       " 'sunshine-x': 718,\n",
       " 'supaphly42': 719,\n",
       " 'swagohome': 720,\n",
       " 'swede': 721,\n",
       " 'sylvan': 722,\n",
       " 'synthpop': 723,\n",
       " 'syroncoda': 724,\n",
       " 'tbotcotw': 725,\n",
       " 'tdehnel': 726,\n",
       " 'thatguydr': 727,\n",
       " 'thax': 728,\n",
       " 'theDrWho': 729,\n",
       " 'the_big_wedding': 730,\n",
       " 'thecompletegeek': 731,\n",
       " 'theeth': 732,\n",
       " 'thekrone': 733,\n",
       " 'thepensivepoet': 734,\n",
       " 'thrakhath': 735,\n",
       " 'throwaway': 736,\n",
       " 'tjdziuba': 737,\n",
       " 'tomjen': 738,\n",
       " 'trackerbishop': 739,\n",
       " 'treebright': 740,\n",
       " 'trenchfever': 741,\n",
       " 'tridentgum': 742,\n",
       " 'trivial': 743,\n",
       " 'troglodyte': 744,\n",
       " 'tryk48s': 745,\n",
       " 'tsteele93': 746,\n",
       " 'tuoder': 747,\n",
       " 'turkourjurbs': 748,\n",
       " 'tvshopceo': 749,\n",
       " 'ubernostrum': 750,\n",
       " 'ubitendo': 751,\n",
       " 'ultimatt42': 752,\n",
       " 'username223': 753,\n",
       " 'ustgblerkvusrd': 754,\n",
       " 'uteunawaytay': 755,\n",
       " 'utexaspunk': 756,\n",
       " 'uwjames': 757,\n",
       " 'vajav': 758,\n",
       " 'valeriepieris': 759,\n",
       " 'vanostran': 760,\n",
       " 'vapblack': 761,\n",
       " 'veracity024': 762,\n",
       " 'veritaze': 763,\n",
       " 'wbeavis': 764,\n",
       " 'wejash': 765,\n",
       " 'whatwedo': 766,\n",
       " 'willis77': 767,\n",
       " 'windmilltheory': 768,\n",
       " 'windynights': 769,\n",
       " 'wolfzero': 770,\n",
       " 'woodsier': 771,\n",
       " 'wurtis16': 772,\n",
       " 'wwabc': 773,\n",
       " 'xMadxScientistx': 774,\n",
       " 'xenmate': 775,\n",
       " 'xinhoj': 776,\n",
       " 'xutopia': 777,\n",
       " 'xyphus': 778,\n",
       " 'yellowking': 779,\n",
       " 'zem': 780,\n",
       " 'zepolen': 781,\n",
       " 'zombieaynrand': 782,\n",
       " 'zombiewat': 783,\n",
       " 'zoomzoom83': 784,\n",
       " 'zorno': 785,\n",
       " 'zyzzogeton': 786}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a55d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/p2index.pkl\",\"rb\") as f:\n",
    "    p2index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db891b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AskReddit': 0,\n",
       " 'Drugs': 1,\n",
       " 'Economics': 2,\n",
       " 'Music': 3,\n",
       " 'WTF': 4,\n",
       " 'apple': 5,\n",
       " 'area51': 6,\n",
       " 'atheism': 7,\n",
       " 'bestof': 8,\n",
       " 'business': 9,\n",
       " 'canada': 10,\n",
       " 'cogsci': 11,\n",
       " 'comics': 12,\n",
       " 'entertainment': 13,\n",
       " 'environment': 14,\n",
       " 'funny': 15,\n",
       " 'gadgets': 16,\n",
       " 'gaming': 17,\n",
       " 'geek': 18,\n",
       " 'happy': 19,\n",
       " 'lgbt': 20,\n",
       " 'linux': 21,\n",
       " 'lolcats': 22,\n",
       " 'math': 23,\n",
       " 'netsec': 24,\n",
       " 'nsfw': 25,\n",
       " 'obama': 26,\n",
       " 'offbeat': 27,\n",
       " 'philosophy': 28,\n",
       " 'photography': 29,\n",
       " 'pics': 30,\n",
       " 'politics': 31,\n",
       " 'programming': 32,\n",
       " 'psychology': 33,\n",
       " 'reddit.com': 34,\n",
       " 'science': 35,\n",
       " 'scifi': 36,\n",
       " 'self': 37,\n",
       " 'sex': 38,\n",
       " 'software': 39,\n",
       " 'sports': 40,\n",
       " 'technology': 41,\n",
       " 'videos': 42,\n",
       " 'web_design': 43,\n",
       " 'worldnews': 44,\n",
       " 'xkcd': 45,\n",
       " 'yourweek': 46}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5e245",
   "metadata": {},
   "source": [
    "## edge list data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f26c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_df = pd.read_csv(\"../../data/02_intermediate/user_behavior/edge_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a73c883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>retrieved_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ultimatt42</td>\n",
       "      <td>science</td>\n",
       "      <td>1425846806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jonknee</td>\n",
       "      <td>programming</td>\n",
       "      <td>1425846807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>burtonmkz</td>\n",
       "      <td>science</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pavel_lishin</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sblinn</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dons</td>\n",
       "      <td>programming</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jedravent</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WebZen</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doodahdei</td>\n",
       "      <td>politics</td>\n",
       "      <td>1425846812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author    subreddit  retrieved_on\n",
       "0    ultimatt42      science    1425846806\n",
       "1       jonknee  programming    1425846807\n",
       "2     burtonmkz      science    1425846810\n",
       "3  pavel_lishin   reddit.com    1425846810\n",
       "4  pavel_lishin   reddit.com    1425846810\n",
       "5        sblinn     politics    1425846810\n",
       "6          dons  programming    1425846811\n",
       "7     Jedravent     politics    1425846811\n",
       "8        WebZen     politics    1425846811\n",
       "9     doodahdei     politics    1425846812"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgelist_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76695257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "def process_edgelist(edge_list, u2index, p2index):\n",
    "    \"\"\" Load edge list and construct a graph \"\"\"\n",
    "    edges = Counter()\n",
    "\n",
    "    for i, row in edge_list.iterrows():\n",
    "        #u = row[0]\n",
    "        #p = row[1]\n",
    "        #t = row[2]\n",
    "        u = row['author']\n",
    "        p = row['subreddit']\n",
    "        t = row['retrieved_on']\n",
    "\n",
    "        if i<1:\n",
    "            print(u, p, t)\n",
    "        edges[(u2index[u], p2index[p])] += 1\n",
    "    # Construct the graph\n",
    "    row = []\n",
    "    col = []\n",
    "    entry = []\n",
    "    for edge, w in edges.items():\n",
    "        #print(w)\n",
    "        i, j = edge\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "        entry.append(w)\n",
    "    graph = csr_matrix(\n",
    "        (entry, (row, col)), \n",
    "        shape=(len(u2index), len(p2index))\n",
    "    )   \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0a8ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultimatt42 science 1425846806\n"
     ]
    }
   ],
   "source": [
    "graph = process_edgelist(edgelist_df, u2index, p2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7060536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ceb14",
   "metadata": {},
   "source": [
    "## train/validation/test id split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d106857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/02_intermediate/user_behavior/data_tvt.pkl\",\"rb\") as f:\n",
    "    tvt_idx = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d214eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = tvt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d9f78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195,), (198,), (393,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train.shape, idx_val.shape, idx_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9e93d",
   "metadata": {},
   "source": [
    "### convert label format (to numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82163725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(labels: pd.DataFrame) -> np.array:\n",
    "    \"\"\"process label information\"\"\"\n",
    "    u_all = set()\n",
    "    pos_uids = set()\n",
    "    labeled_uids = set()\n",
    "    #convert a dataframe to an numpy array, array index being mapped indexes from u2index\n",
    "    for i,row in labels.iterrows():\n",
    "        author = row['author']\n",
    "        author_label = row['label']\n",
    "        u_all.add(author)\n",
    "        if author_label == 1:\n",
    "            pos_uids.add(author)\n",
    "            labeled_uids.add(author)\n",
    "        elif author_label == 0:\n",
    "            labeled_uids.add(author)\n",
    "    print(f'loaded labels, total of {len(pos_uids)} positive users and {len(labeled_uids)} labeled users')\n",
    "    labels = np.zeros(len(u2index))\n",
    "    for u in u2index:\n",
    "        if u in pos_uids:\n",
    "            labels[u2index[u]] = 1\n",
    "    labels = labels.astype(int)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54392c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 327 positive users and 787 labeled users\n"
     ]
    }
   ],
   "source": [
    "labels = process_label(user_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8be08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: total of   195 users with    79 pos users and   116 neg users\n",
      "Val:   total of   198 users with    88 pos users and   110 neg users\n",
      "Test:  total of   393 users with   159 pos users and   234 neg users\n"
     ]
    }
   ],
   "source": [
    "print('Train: total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_train), \n",
    "    np.sum(labels[idx_train]), \n",
    "    len(idx_train)-np.sum(labels[idx_train]))\n",
    "     )\n",
    "print('Val:   total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_val), \n",
    "    np.sum(labels[idx_val]), \n",
    "    len(idx_val)-np.sum(labels[idx_val]))\n",
    "     )\n",
    "print('Test:  total of {:5} users with {:5} pos users and {:5} neg users'.format(\n",
    "    len(idx_test), \n",
    "    np.sum(labels[idx_test]), \n",
    "    len(idx_test)-np.sum(labels[idx_test]))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d71dd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = np.load(\"../../data/02_intermediate/user_behavior/user2vec_npy.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d32ed891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_features['data'].shape #787 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9515fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = np.load(\"../../data/02_intermediate/user_behavior/prod2vec_npy.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5060e7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features['data'].shape #47 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7088ded",
   "metadata": {},
   "source": [
    "## setting up the model trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3576269",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/ec2-user/SageMaker/anomaly-detection-spatial-temporal-data/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b06f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomaly_detection_spatial_temporal_data.model.data_loader import DynamicGraphWNFDataSet, DynamicGraphWNodeFeatDatasetLoader\n",
    "from anomaly_detection_spatial_temporal_data.model.dynamic_graph import Eland_e2e\n",
    "from anomaly_detection_spatial_temporal_data.model.model_config import ElandConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8889329",
   "metadata": {},
   "source": [
    "### set up dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7d304ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded labels, total of 327 positive users and 787 labeled users\n",
      "Train: total of   195 users with    79 pos users and   116 neg users\n",
      "Val:   total of   198 users with    88 pos users and   110 neg users\n",
      "Test:  total of   393 users with   159 pos users and   234 neg users\n"
     ]
    }
   ],
   "source": [
    "data_loader = DynamicGraphWNodeFeatDatasetLoader(\n",
    "    user_label, \n",
    "    u2index, \n",
    "    p2index, \n",
    "    edgelist_df, \n",
    "    tvt_idx, \n",
    "    user_features['data'], \n",
    "    item_features['data']\n",
    ")\n",
    "\n",
    "#sequential data loader\n",
    "dataset = DynamicGraphWNFDataSet(p2index, item_features['data'], edgelist_df)\n",
    "lstm_dataloader = DataLoader(dataset, batch_size=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0df27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "        'graph': data_loader.graph, \n",
    "        'lstm_dataloader': lstm_dataloader,\n",
    "        'user_features': data_loader.user_features,\n",
    "        'item_features': data_loader.item_features,\n",
    "        'labels': data_loader.labels,\n",
    "        'tvt_nids': data_loader.tvt_idx,\n",
    "        'u2index': data_loader.u2index,\n",
    "        'p2index': data_loader.p2index\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37361e92",
   "metadata": {},
   "source": [
    "### load model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95450437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36f23773",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config_file = '../../conf/base/parameters/eland.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b95a8d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eland_data_load_options': {'dataset': 'reddit', 'baseline': 'store_true', 'batch_size': 300}, 'eland_model_options': {'dim_feats': 300, 'cuda': 0, 'hidden_size': 128, 'n_layers': 2, 'epochs': 50, 'batch_size': 300, 'seed': -1, 'lr': 0.0001, 'log': True, 'weight_decay': 1e-06, 'dropout': 0.4, 'tensorboard': False, 'name': 'debug', 'gnnlayer_type': 'gcn', 'rnn_type': 'lstm', 'pretrain_bm': 25, 'pretrain_nc': 200, 'alpha': 0.05, 'bmloss_type': 'mse', 'device': 'cpu', 'base_pred': 400, 'save_directory': 'data/07_model_output/user_behavior'}}\n"
     ]
    }
   ],
   "source": [
    "with open(model_config_file, \"r\") as stream:\n",
    "    try:\n",
    "        mode_config=yaml.safe_load(stream)\n",
    "        print(mode_config)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "402ff841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open a log directory for notebook training session \n",
    "from pathlib import Path\n",
    "log_dir = Path('logs/')\n",
    "log_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f88e6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eland_config = ElandConfig(mode_config['eland_model_options'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f894b48",
   "metadata": {},
   "source": [
    "#### adjust model directory for notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eedaab5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/07_model_output/user_behavior'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eland_config.save_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "836b7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "eland_config.save_directory = '../../data/07_model_output/user_behavior/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1e1956e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/07_model_output/user_behavior/'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eland_config.save_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd7d0e",
   "metadata": {},
   "source": [
    "## kick off model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2537dc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 05:31:09,794 - Parameters: {'dim_feats': 300, 'hidden_size': 128, 'n_layers': 2, 'lr': 0.0001, 'weight_decay': 1e-06, 'dropout': 0.4, 'gnnlayer_type': 'gcn', 'rnn_type': 'lstm', 'bmloss_type': 'mse'}\n",
      "2022-07-29 05:31:14,642 - BM Module pretrain, Epoch 1/25: loss 104.74526374\n",
      "2022-07-29 05:31:18,809 - BM Module pretrain, Epoch 2/25: loss 98.92015425\n",
      "2022-07-29 05:31:22,629 - BM Module pretrain, Epoch 3/25: loss 91.10625966\n",
      "2022-07-29 05:31:26,521 - BM Module pretrain, Epoch 4/25: loss 79.72125181\n",
      "2022-07-29 05:31:30,115 - BM Module pretrain, Epoch 5/25: loss 65.84392325\n",
      "2022-07-29 05:31:33,951 - BM Module pretrain, Epoch 6/25: loss 53.09317303\n",
      "2022-07-29 05:31:37,540 - BM Module pretrain, Epoch 7/25: loss 43.98713462\n",
      "2022-07-29 05:31:41,065 - BM Module pretrain, Epoch 8/25: loss 37.43398778\n",
      "2022-07-29 05:31:44,768 - BM Module pretrain, Epoch 9/25: loss 32.04992612\n",
      "2022-07-29 05:31:48,674 - BM Module pretrain, Epoch 10/25: loss 27.67630625\n",
      "2022-07-29 05:31:52,118 - BM Module pretrain, Epoch 11/25: loss 24.29518843\n",
      "2022-07-29 05:31:55,538 - BM Module pretrain, Epoch 12/25: loss 21.98845482\n",
      "2022-07-29 05:31:59,105 - BM Module pretrain, Epoch 13/25: loss 20.30629365\n",
      "2022-07-29 05:32:02,932 - BM Module pretrain, Epoch 14/25: loss 19.05024465\n",
      "2022-07-29 05:32:06,538 - BM Module pretrain, Epoch 15/25: loss 18.05483381\n",
      "2022-07-29 05:32:09,981 - BM Module pretrain, Epoch 16/25: loss 17.35620928\n",
      "2022-07-29 05:32:13,774 - BM Module pretrain, Epoch 17/25: loss 16.81980022\n",
      "2022-07-29 05:32:17,526 - BM Module pretrain, Epoch 18/25: loss 16.49741999\n",
      "2022-07-29 05:32:21,078 - BM Module pretrain, Epoch 19/25: loss 16.33620286\n",
      "2022-07-29 05:32:24,880 - BM Module pretrain, Epoch 20/25: loss 16.24601865\n",
      "2022-07-29 05:32:28,293 - BM Module pretrain, Epoch 21/25: loss 16.20349065\n",
      "2022-07-29 05:32:32,128 - BM Module pretrain, Epoch 22/25: loss 16.16305876\n",
      "2022-07-29 05:32:35,593 - BM Module pretrain, Epoch 23/25: loss 16.11087743\n",
      "2022-07-29 05:32:39,008 - BM Module pretrain, Epoch 24/25: loss 16.06783772\n",
      "2022-07-29 05:32:42,434 - BM Module pretrain, Epoch 25/25: loss 16.05125729\n",
      "2022-07-29 05:32:42,473 - NCNet pretrain, Epoch [1 / 200]: loss 0.6747, training auc: 0.5259, val_auc 0.8374, test auc 0.8718\n",
      "2022-07-29 05:32:42,506 - NCNet pretrain, Epoch [2 / 200]: loss 0.6650, training auc: 0.6505, val_auc 0.8555, test auc 0.8922\n",
      "2022-07-29 05:32:42,538 - NCNet pretrain, Epoch [3 / 200]: loss 0.6534, training auc: 0.7774, val_auc 0.8653, test auc 0.9086\n",
      "2022-07-29 05:32:42,571 - NCNet pretrain, Epoch [4 / 200]: loss 0.6570, training auc: 0.7520, val_auc 0.8759, test auc 0.9213\n",
      "2022-07-29 05:32:42,603 - NCNet pretrain, Epoch [5 / 200]: loss 0.6508, training auc: 0.8457, val_auc 0.8919, test auc 0.9312\n",
      "2022-07-29 05:32:42,636 - NCNet pretrain, Epoch [6 / 200]: loss 0.6571, training auc: 0.7681, val_auc 0.8995, test auc 0.9400\n",
      "2022-07-29 05:32:42,669 - NCNet pretrain, Epoch [7 / 200]: loss 0.6543, training auc: 0.8380, val_auc 0.9071, test auc 0.9459\n",
      "2022-07-29 05:32:42,704 - NCNet pretrain, Epoch [8 / 200]: loss 0.6542, training auc: 0.7874, val_auc 0.9139, test auc 0.9526\n",
      "2022-07-29 05:32:42,737 - NCNet pretrain, Epoch [9 / 200]: loss 0.6547, training auc: 0.8207, val_auc 0.9176, test auc 0.9562\n",
      "2022-07-29 05:32:42,771 - NCNet pretrain, Epoch [10 / 200]: loss 0.6485, training auc: 0.8462, val_auc 0.9218, test auc 0.9598\n",
      "2022-07-29 05:32:42,805 - NCNet pretrain, Epoch [11 / 200]: loss 0.6525, training auc: 0.8612, val_auc 0.9254, test auc 0.9636\n",
      "2022-07-29 05:32:42,840 - NCNet pretrain, Epoch [12 / 200]: loss 0.6469, training auc: 0.8754, val_auc 0.9320, test auc 0.9659\n",
      "2022-07-29 05:32:42,874 - NCNet pretrain, Epoch [13 / 200]: loss 0.6498, training auc: 0.8747, val_auc 0.9365, test auc 0.9676\n",
      "2022-07-29 05:32:42,906 - NCNet pretrain, Epoch [14 / 200]: loss 0.6479, training auc: 0.8685, val_auc 0.9389, test auc 0.9692\n",
      "2022-07-29 05:32:42,939 - NCNet pretrain, Epoch [15 / 200]: loss 0.6440, training auc: 0.9020, val_auc 0.9405, test auc 0.9705\n",
      "2022-07-29 05:32:42,971 - NCNet pretrain, Epoch [16 / 200]: loss 0.6441, training auc: 0.9026, val_auc 0.9419, test auc 0.9720\n",
      "2022-07-29 05:32:43,004 - NCNet pretrain, Epoch [17 / 200]: loss 0.6399, training auc: 0.9067, val_auc 0.9434, test auc 0.9735\n",
      "2022-07-29 05:32:43,037 - NCNet pretrain, Epoch [18 / 200]: loss 0.6419, training auc: 0.9265, val_auc 0.9442, test auc 0.9748\n",
      "2022-07-29 05:32:43,069 - NCNet pretrain, Epoch [19 / 200]: loss 0.6397, training auc: 0.9059, val_auc 0.9450, test auc 0.9758\n",
      "2022-07-29 05:32:43,102 - NCNet pretrain, Epoch [20 / 200]: loss 0.6351, training auc: 0.9318, val_auc 0.9464, test auc 0.9763\n",
      "2022-07-29 05:32:43,135 - NCNet pretrain, Epoch [21 / 200]: loss 0.6280, training auc: 0.9453, val_auc 0.9477, test auc 0.9771\n",
      "2022-07-29 05:32:43,168 - NCNet pretrain, Epoch [22 / 200]: loss 0.6382, training auc: 0.9226, val_auc 0.9489, test auc 0.9780\n",
      "2022-07-29 05:32:43,201 - NCNet pretrain, Epoch [23 / 200]: loss 0.6334, training auc: 0.9274, val_auc 0.9518, test auc 0.9786\n",
      "2022-07-29 05:32:43,233 - NCNet pretrain, Epoch [24 / 200]: loss 0.6289, training auc: 0.9351, val_auc 0.9538, test auc 0.9792\n",
      "2022-07-29 05:32:43,266 - NCNet pretrain, Epoch [25 / 200]: loss 0.6363, training auc: 0.9176, val_auc 0.9541, test auc 0.9796\n",
      "2022-07-29 05:32:43,299 - NCNet pretrain, Epoch [26 / 200]: loss 0.6267, training auc: 0.9454, val_auc 0.9545, test auc 0.9801\n",
      "2022-07-29 05:32:43,331 - NCNet pretrain, Epoch [27 / 200]: loss 0.6336, training auc: 0.9464, val_auc 0.9554, test auc 0.9807\n",
      "2022-07-29 05:32:43,364 - NCNet pretrain, Epoch [28 / 200]: loss 0.6303, training auc: 0.9398, val_auc 0.9576, test auc 0.9815\n",
      "2022-07-29 05:32:43,397 - NCNet pretrain, Epoch [29 / 200]: loss 0.6327, training auc: 0.9199, val_auc 0.9586, test auc 0.9820\n",
      "2022-07-29 05:32:43,430 - NCNet pretrain, Epoch [30 / 200]: loss 0.6254, training auc: 0.9522, val_auc 0.9590, test auc 0.9823\n",
      "2022-07-29 05:32:43,458 - NCNet pretrain, Epoch [31 / 200]: loss 0.6236, training auc: 0.9409, val_auc 0.9590\n",
      "2022-07-29 05:32:43,492 - NCNet pretrain, Epoch [32 / 200]: loss 0.6237, training auc: 0.9536, val_auc 0.9592, test auc 0.9830\n",
      "2022-07-29 05:32:43,520 - NCNet pretrain, Epoch [33 / 200]: loss 0.6225, training auc: 0.9610, val_auc 0.9592\n",
      "2022-07-29 05:32:43,553 - NCNet pretrain, Epoch [34 / 200]: loss 0.6204, training auc: 0.9538, val_auc 0.9596, test auc 0.9838\n",
      "2022-07-29 05:32:43,586 - NCNet pretrain, Epoch [35 / 200]: loss 0.6229, training auc: 0.9494, val_auc 0.9597, test auc 0.9840\n",
      "2022-07-29 05:32:43,619 - NCNet pretrain, Epoch [36 / 200]: loss 0.6182, training auc: 0.9557, val_auc 0.9599, test auc 0.9841\n",
      "2022-07-29 05:32:43,654 - NCNet pretrain, Epoch [37 / 200]: loss 0.6161, training auc: 0.9535, val_auc 0.9600, test auc 0.9842\n",
      "2022-07-29 05:32:43,692 - NCNet pretrain, Epoch [38 / 200]: loss 0.6189, training auc: 0.9556, val_auc 0.9602, test auc 0.9843\n",
      "2022-07-29 05:32:43,726 - NCNet pretrain, Epoch [39 / 200]: loss 0.6145, training auc: 0.9574, val_auc 0.9603, test auc 0.9844\n",
      "2022-07-29 05:32:43,760 - NCNet pretrain, Epoch [40 / 200]: loss 0.6192, training auc: 0.9508, val_auc 0.9606, test auc 0.9844\n",
      "2022-07-29 05:32:43,795 - NCNet pretrain, Epoch [41 / 200]: loss 0.6173, training auc: 0.9489, val_auc 0.9610, test auc 0.9845\n",
      "2022-07-29 05:32:43,827 - NCNet pretrain, Epoch [42 / 200]: loss 0.6094, training auc: 0.9497, val_auc 0.9610\n",
      "2022-07-29 05:32:43,860 - NCNet pretrain, Epoch [43 / 200]: loss 0.6161, training auc: 0.9494, val_auc 0.9613, test auc 0.9848\n",
      "2022-07-29 05:32:43,893 - NCNet pretrain, Epoch [44 / 200]: loss 0.6194, training auc: 0.9697, val_auc 0.9616, test auc 0.9849\n",
      "2022-07-29 05:32:43,925 - NCNet pretrain, Epoch [45 / 200]: loss 0.6121, training auc: 0.9390, val_auc 0.9619, test auc 0.9849\n",
      "2022-07-29 05:32:43,953 - NCNet pretrain, Epoch [46 / 200]: loss 0.6112, training auc: 0.9550, val_auc 0.9619\n",
      "2022-07-29 05:32:43,986 - NCNet pretrain, Epoch [47 / 200]: loss 0.6076, training auc: 0.9614, val_auc 0.9626, test auc 0.9851\n",
      "2022-07-29 05:32:44,028 - NCNet pretrain, Epoch [48 / 200]: loss 0.6061, training auc: 0.9540, val_auc 0.9643, test auc 0.9852\n",
      "2022-07-29 05:32:44,071 - NCNet pretrain, Epoch [49 / 200]: loss 0.6019, training auc: 0.9489, val_auc 0.9648, test auc 0.9853\n",
      "2022-07-29 05:32:44,104 - NCNet pretrain, Epoch [50 / 200]: loss 0.6065, training auc: 0.9566, val_auc 0.9650, test auc 0.9854\n",
      "2022-07-29 05:32:44,137 - NCNet pretrain, Epoch [51 / 200]: loss 0.6070, training auc: 0.9620, val_auc 0.9651, test auc 0.9855\n",
      "2022-07-29 05:32:44,170 - NCNet pretrain, Epoch [52 / 200]: loss 0.6104, training auc: 0.9588, val_auc 0.9652, test auc 0.9856\n",
      "2022-07-29 05:32:44,200 - NCNet pretrain, Epoch [53 / 200]: loss 0.6041, training auc: 0.9580, val_auc 0.9655, test auc 0.9855\n",
      "2022-07-29 05:32:44,228 - NCNet pretrain, Epoch [54 / 200]: loss 0.6031, training auc: 0.9569, val_auc 0.9655\n",
      "2022-07-29 05:32:44,256 - NCNet pretrain, Epoch [55 / 200]: loss 0.5992, training auc: 0.9554, val_auc 0.9655\n",
      "2022-07-29 05:32:44,288 - NCNet pretrain, Epoch [56 / 200]: loss 0.6002, training auc: 0.9604, val_auc 0.9656, test auc 0.9857\n",
      "2022-07-29 05:32:44,316 - NCNet pretrain, Epoch [57 / 200]: loss 0.5984, training auc: 0.9642, val_auc 0.9656\n",
      "2022-07-29 05:32:44,344 - NCNet pretrain, Epoch [58 / 200]: loss 0.5991, training auc: 0.9574, val_auc 0.9655\n",
      "2022-07-29 05:32:44,372 - NCNet pretrain, Epoch [59 / 200]: loss 0.6025, training auc: 0.9529, val_auc 0.9655\n",
      "2022-07-29 05:32:44,400 - NCNet pretrain, Epoch [60 / 200]: loss 0.5964, training auc: 0.9507, val_auc 0.9655\n",
      "2022-07-29 05:32:44,427 - NCNet pretrain, Epoch [61 / 200]: loss 0.6003, training auc: 0.9577, val_auc 0.9654\n",
      "2022-07-29 05:32:44,460 - NCNet pretrain, Epoch [62 / 200]: loss 0.5999, training auc: 0.9605, val_auc 0.9657, test auc 0.9861\n",
      "2022-07-29 05:32:44,490 - NCNet pretrain, Epoch [63 / 200]: loss 0.5970, training auc: 0.9622, val_auc 0.9657\n",
      "2022-07-29 05:32:44,526 - NCNet pretrain, Epoch [64 / 200]: loss 0.6040, training auc: 0.9481, val_auc 0.9657\n",
      "2022-07-29 05:32:44,566 - NCNet pretrain, Epoch [65 / 200]: loss 0.5897, training auc: 0.9626, val_auc 0.9658, test auc 0.9862\n",
      "2022-07-29 05:32:44,608 - NCNet pretrain, Epoch [66 / 200]: loss 0.5916, training auc: 0.9648, val_auc 0.9659, test auc 0.9862\n",
      "2022-07-29 05:32:44,644 - NCNet pretrain, Epoch [67 / 200]: loss 0.5954, training auc: 0.9638, val_auc 0.9659\n",
      "2022-07-29 05:32:44,672 - NCNet pretrain, Epoch [68 / 200]: loss 0.5907, training auc: 0.9535, val_auc 0.9659\n",
      "2022-07-29 05:32:44,701 - NCNet pretrain, Epoch [69 / 200]: loss 0.5954, training auc: 0.9696, val_auc 0.9659\n",
      "2022-07-29 05:32:44,731 - NCNet pretrain, Epoch [70 / 200]: loss 0.5891, training auc: 0.9633, val_auc 0.9659\n",
      "2022-07-29 05:32:44,759 - NCNet pretrain, Epoch [71 / 200]: loss 0.5912, training auc: 0.9682, val_auc 0.9659\n",
      "2022-07-29 05:32:44,792 - NCNet pretrain, Epoch [72 / 200]: loss 0.5939, training auc: 0.9645, val_auc 0.9660, test auc 0.9863\n",
      "2022-07-29 05:32:44,820 - NCNet pretrain, Epoch [73 / 200]: loss 0.5925, training auc: 0.9630, val_auc 0.9660\n",
      "2022-07-29 05:32:44,847 - NCNet pretrain, Epoch [74 / 200]: loss 0.5881, training auc: 0.9598, val_auc 0.9660\n",
      "2022-07-29 05:32:44,874 - NCNet pretrain, Epoch [75 / 200]: loss 0.5910, training auc: 0.9586, val_auc 0.9660\n",
      "2022-07-29 05:32:44,901 - NCNet pretrain, Epoch [76 / 200]: loss 0.5915, training auc: 0.9614, val_auc 0.9660\n",
      "2022-07-29 05:32:44,929 - NCNet pretrain, Epoch [77 / 200]: loss 0.5868, training auc: 0.9675, val_auc 0.9660\n",
      "2022-07-29 05:32:44,956 - NCNet pretrain, Epoch [78 / 200]: loss 0.5847, training auc: 0.9667, val_auc 0.9660\n",
      "2022-07-29 05:32:44,988 - NCNet pretrain, Epoch [79 / 200]: loss 0.5840, training auc: 0.9588, val_auc 0.9661, test auc 0.9864\n",
      "2022-07-29 05:32:45,020 - NCNet pretrain, Epoch [80 / 200]: loss 0.5859, training auc: 0.9478, val_auc 0.9662, test auc 0.9865\n",
      "2022-07-29 05:32:45,049 - NCNet pretrain, Epoch [81 / 200]: loss 0.5825, training auc: 0.9569, val_auc 0.9663, test auc 0.9865\n",
      "2022-07-29 05:32:45,076 - NCNet pretrain, Epoch [82 / 200]: loss 0.5847, training auc: 0.9614, val_auc 0.9663\n",
      "2022-07-29 05:32:45,105 - NCNet pretrain, Epoch [83 / 200]: loss 0.5852, training auc: 0.9604, val_auc 0.9665, test auc 0.9865\n",
      "2022-07-29 05:32:45,133 - NCNet pretrain, Epoch [84 / 200]: loss 0.5816, training auc: 0.9616, val_auc 0.9665\n",
      "2022-07-29 05:32:45,160 - NCNet pretrain, Epoch [85 / 200]: loss 0.5847, training auc: 0.9596, val_auc 0.9665\n",
      "2022-07-29 05:32:45,186 - NCNet pretrain, Epoch [86 / 200]: loss 0.5780, training auc: 0.9686, val_auc 0.9665\n",
      "2022-07-29 05:32:45,213 - NCNet pretrain, Epoch [87 / 200]: loss 0.5861, training auc: 0.9590, val_auc 0.9665\n",
      "2022-07-29 05:32:45,239 - NCNet pretrain, Epoch [88 / 200]: loss 0.5740, training auc: 0.9679, val_auc 0.9665\n",
      "2022-07-29 05:32:45,270 - NCNet pretrain, Epoch [89 / 200]: loss 0.5846, training auc: 0.9509, val_auc 0.9666, test auc 0.9865\n",
      "2022-07-29 05:32:45,301 - NCNet pretrain, Epoch [90 / 200]: loss 0.5736, training auc: 0.9665, val_auc 0.9667, test auc 0.9865\n",
      "2022-07-29 05:32:45,327 - NCNet pretrain, Epoch [91 / 200]: loss 0.5813, training auc: 0.9663, val_auc 0.9667\n",
      "2022-07-29 05:32:45,355 - NCNet pretrain, Epoch [92 / 200]: loss 0.5791, training auc: 0.9572, val_auc 0.9668, test auc 0.9865\n",
      "2022-07-29 05:32:45,381 - NCNet pretrain, Epoch [93 / 200]: loss 0.5770, training auc: 0.9664, val_auc 0.9668\n",
      "2022-07-29 05:32:45,408 - NCNet pretrain, Epoch [94 / 200]: loss 0.5796, training auc: 0.9637, val_auc 0.9668\n",
      "2022-07-29 05:32:45,434 - NCNet pretrain, Epoch [95 / 200]: loss 0.5756, training auc: 0.9534, val_auc 0.9668\n",
      "2022-07-29 05:32:45,460 - NCNet pretrain, Epoch [96 / 200]: loss 0.5763, training auc: 0.9640, val_auc 0.9668\n",
      "2022-07-29 05:32:45,492 - NCNet pretrain, Epoch [97 / 200]: loss 0.5721, training auc: 0.9631, val_auc 0.9669, test auc 0.9867\n",
      "2022-07-29 05:32:45,519 - NCNet pretrain, Epoch [98 / 200]: loss 0.5727, training auc: 0.9655, val_auc 0.9669\n",
      "2022-07-29 05:32:45,545 - NCNet pretrain, Epoch [99 / 200]: loss 0.5744, training auc: 0.9583, val_auc 0.9669\n",
      "2022-07-29 05:32:45,571 - NCNet pretrain, Epoch [100 / 200]: loss 0.5781, training auc: 0.9686, val_auc 0.9669\n",
      "2022-07-29 05:32:45,597 - NCNet pretrain, Epoch [101 / 200]: loss 0.5774, training auc: 0.9631, val_auc 0.9669\n",
      "2022-07-29 05:32:45,624 - NCNet pretrain, Epoch [102 / 200]: loss 0.5770, training auc: 0.9627, val_auc 0.9669\n",
      "2022-07-29 05:32:45,650 - NCNet pretrain, Epoch [103 / 200]: loss 0.5749, training auc: 0.9560, val_auc 0.9669\n",
      "2022-07-29 05:32:45,677 - NCNet pretrain, Epoch [104 / 200]: loss 0.5667, training auc: 0.9646, val_auc 0.9669\n",
      "2022-07-29 05:32:45,703 - NCNet pretrain, Epoch [105 / 200]: loss 0.5730, training auc: 0.9630, val_auc 0.9669\n",
      "2022-07-29 05:32:45,730 - NCNet pretrain, Epoch [106 / 200]: loss 0.5750, training auc: 0.9670, val_auc 0.9669\n",
      "2022-07-29 05:32:45,756 - NCNet pretrain, Epoch [107 / 200]: loss 0.5601, training auc: 0.9645, val_auc 0.9669\n",
      "2022-07-29 05:32:45,784 - NCNet pretrain, Epoch [108 / 200]: loss 0.5724, training auc: 0.9612, val_auc 0.9669\n",
      "2022-07-29 05:32:45,812 - NCNet pretrain, Epoch [109 / 200]: loss 0.5815, training auc: 0.9628, val_auc 0.9669\n",
      "2022-07-29 05:32:45,845 - NCNet pretrain, Epoch [110 / 200]: loss 0.5644, training auc: 0.9692, val_auc 0.9670, test auc 0.9868\n",
      "2022-07-29 05:32:45,874 - NCNet pretrain, Epoch [111 / 200]: loss 0.5714, training auc: 0.9555, val_auc 0.9669\n",
      "2022-07-29 05:32:45,902 - NCNet pretrain, Epoch [112 / 200]: loss 0.5686, training auc: 0.9686, val_auc 0.9670\n",
      "2022-07-29 05:32:45,931 - NCNet pretrain, Epoch [113 / 200]: loss 0.5620, training auc: 0.9637, val_auc 0.9669\n",
      "2022-07-29 05:32:45,958 - NCNet pretrain, Epoch [114 / 200]: loss 0.5697, training auc: 0.9627, val_auc 0.9669\n",
      "2022-07-29 05:32:45,984 - NCNet pretrain, Epoch [115 / 200]: loss 0.5643, training auc: 0.9617, val_auc 0.9669\n",
      "2022-07-29 05:32:46,012 - NCNet pretrain, Epoch [116 / 200]: loss 0.5711, training auc: 0.9684, val_auc 0.9669\n",
      "2022-07-29 05:32:46,039 - NCNet pretrain, Epoch [117 / 200]: loss 0.5726, training auc: 0.9632, val_auc 0.9670\n",
      "2022-07-29 05:32:46,066 - NCNet pretrain, Epoch [118 / 200]: loss 0.5580, training auc: 0.9699, val_auc 0.9670\n",
      "2022-07-29 05:32:46,092 - NCNet pretrain, Epoch [119 / 200]: loss 0.5613, training auc: 0.9696, val_auc 0.9670\n",
      "2022-07-29 05:32:46,119 - NCNet pretrain, Epoch [120 / 200]: loss 0.5647, training auc: 0.9622, val_auc 0.9670\n",
      "2022-07-29 05:32:46,147 - NCNet pretrain, Epoch [121 / 200]: loss 0.5620, training auc: 0.9616, val_auc 0.9670\n",
      "2022-07-29 05:32:46,174 - NCNet pretrain, Epoch [122 / 200]: loss 0.5633, training auc: 0.9682, val_auc 0.9670\n",
      "2022-07-29 05:32:46,201 - NCNet pretrain, Epoch [123 / 200]: loss 0.5607, training auc: 0.9592, val_auc 0.9670\n",
      "2022-07-29 05:32:46,227 - NCNet pretrain, Epoch [124 / 200]: loss 0.5617, training auc: 0.9615, val_auc 0.9670\n",
      "2022-07-29 05:32:46,253 - NCNet pretrain, Epoch [125 / 200]: loss 0.5639, training auc: 0.9624, val_auc 0.9669\n",
      "2022-07-29 05:32:46,280 - NCNet pretrain, Epoch [126 / 200]: loss 0.5584, training auc: 0.9665, val_auc 0.9669\n",
      "2022-07-29 05:32:46,306 - NCNet pretrain, Epoch [127 / 200]: loss 0.5566, training auc: 0.9674, val_auc 0.9669\n",
      "2022-07-29 05:32:46,332 - NCNet pretrain, Epoch [128 / 200]: loss 0.5623, training auc: 0.9637, val_auc 0.9669\n",
      "2022-07-29 05:32:46,358 - NCNet pretrain, Epoch [129 / 200]: loss 0.5614, training auc: 0.9636, val_auc 0.9669\n",
      "2022-07-29 05:32:46,384 - NCNet pretrain, Epoch [130 / 200]: loss 0.5596, training auc: 0.9653, val_auc 0.9669\n",
      "2022-07-29 05:32:46,410 - NCNet pretrain, Epoch [131 / 200]: loss 0.5578, training auc: 0.9636, val_auc 0.9669\n",
      "2022-07-29 05:32:46,436 - NCNet pretrain, Epoch [132 / 200]: loss 0.5522, training auc: 0.9576, val_auc 0.9669\n",
      "2022-07-29 05:32:46,462 - NCNet pretrain, Epoch [133 / 200]: loss 0.5577, training auc: 0.9593, val_auc 0.9669\n",
      "2022-07-29 05:32:46,488 - NCNet pretrain, Epoch [134 / 200]: loss 0.5596, training auc: 0.9579, val_auc 0.9669\n",
      "2022-07-29 05:32:46,514 - NCNet pretrain, Epoch [135 / 200]: loss 0.5555, training auc: 0.9609, val_auc 0.9669\n",
      "2022-07-29 05:32:46,540 - NCNet pretrain, Epoch [136 / 200]: loss 0.5594, training auc: 0.9637, val_auc 0.9668\n",
      "2022-07-29 05:32:46,566 - NCNet pretrain, Epoch [137 / 200]: loss 0.5534, training auc: 0.9613, val_auc 0.9668\n",
      "2022-07-29 05:32:46,592 - NCNet pretrain, Epoch [138 / 200]: loss 0.5555, training auc: 0.9698, val_auc 0.9668\n",
      "2022-07-29 05:32:46,618 - NCNet pretrain, Epoch [139 / 200]: loss 0.5515, training auc: 0.9607, val_auc 0.9668\n",
      "2022-07-29 05:32:46,644 - NCNet pretrain, Epoch [140 / 200]: loss 0.5536, training auc: 0.9554, val_auc 0.9668\n",
      "2022-07-29 05:32:46,670 - NCNet pretrain, Epoch [141 / 200]: loss 0.5541, training auc: 0.9598, val_auc 0.9668\n",
      "2022-07-29 05:32:46,696 - NCNet pretrain, Epoch [142 / 200]: loss 0.5523, training auc: 0.9550, val_auc 0.9669\n",
      "2022-07-29 05:32:46,723 - NCNet pretrain, Epoch [143 / 200]: loss 0.5506, training auc: 0.9622, val_auc 0.9669\n",
      "2022-07-29 05:32:46,751 - NCNet pretrain, Epoch [144 / 200]: loss 0.5534, training auc: 0.9604, val_auc 0.9669\n",
      "2022-07-29 05:32:46,777 - NCNet pretrain, Epoch [145 / 200]: loss 0.5539, training auc: 0.9605, val_auc 0.9669\n",
      "2022-07-29 05:32:46,805 - NCNet pretrain, Epoch [146 / 200]: loss 0.5474, training auc: 0.9664, val_auc 0.9669\n",
      "2022-07-29 05:32:46,834 - NCNet pretrain, Epoch [147 / 200]: loss 0.5495, training auc: 0.9626, val_auc 0.9669\n",
      "2022-07-29 05:32:46,861 - NCNet pretrain, Epoch [148 / 200]: loss 0.5499, training auc: 0.9600, val_auc 0.9669\n",
      "2022-07-29 05:32:46,889 - NCNet pretrain, Epoch [149 / 200]: loss 0.5511, training auc: 0.9572, val_auc 0.9669\n",
      "2022-07-29 05:32:46,916 - NCNet pretrain, Epoch [150 / 200]: loss 0.5473, training auc: 0.9666, val_auc 0.9669\n",
      "2022-07-29 05:32:46,944 - NCNet pretrain, Epoch [151 / 200]: loss 0.5466, training auc: 0.9658, val_auc 0.9668\n",
      "2022-07-29 05:32:46,972 - NCNet pretrain, Epoch [152 / 200]: loss 0.5467, training auc: 0.9673, val_auc 0.9668\n",
      "2022-07-29 05:32:47,000 - NCNet pretrain, Epoch [153 / 200]: loss 0.5458, training auc: 0.9618, val_auc 0.9668\n",
      "2022-07-29 05:32:47,027 - NCNet pretrain, Epoch [154 / 200]: loss 0.5455, training auc: 0.9642, val_auc 0.9668\n",
      "2022-07-29 05:32:47,054 - NCNet pretrain, Epoch [155 / 200]: loss 0.5381, training auc: 0.9638, val_auc 0.9668\n",
      "2022-07-29 05:32:47,081 - NCNet pretrain, Epoch [156 / 200]: loss 0.5433, training auc: 0.9616, val_auc 0.9668\n",
      "2022-07-29 05:32:47,108 - NCNet pretrain, Epoch [157 / 200]: loss 0.5468, training auc: 0.9556, val_auc 0.9668\n",
      "2022-07-29 05:32:47,135 - NCNet pretrain, Epoch [158 / 200]: loss 0.5476, training auc: 0.9600, val_auc 0.9668\n",
      "2022-07-29 05:32:47,162 - NCNet pretrain, Epoch [159 / 200]: loss 0.5375, training auc: 0.9597, val_auc 0.9668\n",
      "2022-07-29 05:32:47,189 - NCNet pretrain, Epoch [160 / 200]: loss 0.5408, training auc: 0.9690, val_auc 0.9668\n",
      "2022-07-29 05:32:47,190 - Early stop!\n",
      "2022-07-29 05:32:47,190 - Best Test Results: auc 0.9868, ap 0.9868, f1 0.0000\n",
      "2022-07-29 05:33:08,197 - Eland Training, Epoch [1/50]: loss 2.9421, train_auc: 0.9658, val_auc 0.9755, test_auc 0.9866\n",
      "2022-07-29 05:33:30,358 - Eland Training, Epoch [2/50]: loss 2.9428, train_auc: 0.9660, val_auc 0.9741\n",
      "2022-07-29 05:33:51,425 - Eland Training, Epoch [3/50]: loss 2.9444, train_auc: 0.9692, val_auc 0.9775, test_auc 0.9879\n",
      "2022-07-29 05:34:13,530 - Eland Training, Epoch [4/50]: loss 2.9272, train_auc: 0.9632, val_auc 0.9736\n",
      "2022-07-29 05:34:34,868 - Eland Training, Epoch [5/50]: loss 2.9283, train_auc: 0.9674, val_auc 0.9759\n",
      "2022-07-29 05:34:57,218 - Eland Training, Epoch [6/50]: loss 2.9268, train_auc: 0.9714, val_auc 0.9755\n",
      "2022-07-29 05:35:19,663 - Eland Training, Epoch [7/50]: loss 2.9347, train_auc: 0.9744, val_auc 0.9755\n",
      "2022-07-29 05:35:42,214 - Eland Training, Epoch [8/50]: loss 2.9253, train_auc: 0.9793, val_auc 0.9756\n",
      "2022-07-29 05:36:05,512 - Eland Training, Epoch [9/50]: loss 2.9207, train_auc: 0.9668, val_auc 0.9729\n",
      "2022-07-29 05:36:54,229 - Eland Training, Epoch [10/50]: loss 2.9401, train_auc: 0.9674, val_auc 0.9748\n",
      "2022-07-29 05:37:16,699 - Eland Training, Epoch [11/50]: loss 2.9336, train_auc: 0.9630, val_auc 0.9771\n",
      "2022-07-29 05:37:38,386 - Eland Training, Epoch [12/50]: loss 2.9170, train_auc: 0.9620, val_auc 0.9727\n",
      "2022-07-29 05:38:00,259 - Eland Training, Epoch [13/50]: loss 2.9196, train_auc: 0.9689, val_auc 0.9753\n",
      "2022-07-29 05:38:23,920 - Eland Training, Epoch [14/50]: loss 2.9289, train_auc: 0.9637, val_auc 0.9750\n",
      "2022-07-29 05:38:45,766 - Eland Training, Epoch [15/50]: loss 2.9153, train_auc: 0.9686, val_auc 0.9754\n",
      "2022-07-29 05:39:11,636 - Eland Training, Epoch [16/50]: loss 2.9087, train_auc: 0.9698, val_auc 0.9752\n",
      "2022-07-29 05:39:35,512 - Eland Training, Epoch [17/50]: loss 2.9114, train_auc: 0.9708, val_auc 0.9765\n",
      "2022-07-29 05:39:58,938 - Eland Training, Epoch [18/50]: loss 2.9236, train_auc: 0.9658, val_auc 0.9755\n",
      "2022-07-29 05:40:22,695 - Eland Training, Epoch [19/50]: loss 2.9090, train_auc: 0.9679, val_auc 0.9749\n",
      "2022-07-29 05:40:49,962 - Eland Training, Epoch [20/50]: loss 2.9132, train_auc: 0.9674, val_auc 0.9737\n",
      "2022-07-29 05:41:19,768 - Eland Training, Epoch [21/50]: loss 2.9017, train_auc: 0.9740, val_auc 0.9754\n",
      "2022-07-29 05:41:44,360 - Eland Training, Epoch [22/50]: loss 2.9068, train_auc: 0.9657, val_auc 0.9738\n",
      "2022-07-29 05:42:09,776 - Eland Training, Epoch [23/50]: loss 2.8948, train_auc: 0.9673, val_auc 0.9770\n",
      "2022-07-29 05:42:35,265 - Eland Training, Epoch [24/50]: loss 2.9100, train_auc: 0.9689, val_auc 0.9755\n",
      "2022-07-29 05:43:00,539 - Eland Training, Epoch [25/50]: loss 2.8956, train_auc: 0.9704, val_auc 0.9745\n",
      "2022-07-29 05:43:23,513 - Eland Training, Epoch [26/50]: loss 2.9047, train_auc: 0.9687, val_auc 0.9757\n",
      "2022-07-29 05:43:48,141 - Eland Training, Epoch [27/50]: loss 2.9032, train_auc: 0.9682, val_auc 0.9747\n",
      "2022-07-29 05:44:12,538 - Eland Training, Epoch [28/50]: loss 2.8954, train_auc: 0.9736, val_auc 0.9746\n",
      "2022-07-29 05:44:38,484 - Eland Training, Epoch [29/50]: loss 2.8924, train_auc: 0.9650, val_auc 0.9749\n",
      "2022-07-29 05:45:03,977 - Eland Training, Epoch [30/50]: loss 2.8995, train_auc: 0.9689, val_auc 0.9749\n",
      "2022-07-29 05:45:30,531 - Eland Training, Epoch [31/50]: loss 2.8928, train_auc: 0.9715, val_auc 0.9744\n",
      "2022-07-29 05:45:58,952 - Eland Training, Epoch [32/50]: loss 2.8881, train_auc: 0.9729, val_auc 0.9756\n",
      "2022-07-29 05:46:24,053 - Eland Training, Epoch [33/50]: loss 2.8781, train_auc: 0.9724, val_auc 0.9747\n",
      "2022-07-29 05:46:49,226 - Eland Training, Epoch [34/50]: loss 2.8824, train_auc: 0.9650, val_auc 0.9762\n",
      "2022-07-29 05:47:16,885 - Eland Training, Epoch [35/50]: loss 2.8814, train_auc: 0.9667, val_auc 0.9768\n",
      "2022-07-29 05:47:45,539 - Eland Training, Epoch [36/50]: loss 2.8711, train_auc: 0.9691, val_auc 0.9752\n",
      "2022-07-29 05:48:11,769 - Eland Training, Epoch [37/50]: loss 2.8905, train_auc: 0.9737, val_auc 0.9752\n",
      "2022-07-29 05:48:37,815 - Eland Training, Epoch [38/50]: loss 2.8765, train_auc: 0.9736, val_auc 0.9759\n",
      "2022-07-29 05:49:03,236 - Eland Training, Epoch [39/50]: loss 2.8753, train_auc: 0.9745, val_auc 0.9754\n",
      "2022-07-29 05:49:31,150 - Eland Training, Epoch [40/50]: loss 2.8878, train_auc: 0.9663, val_auc 0.9744\n",
      "2022-07-29 05:49:54,527 - Eland Training, Epoch [41/50]: loss 2.8827, train_auc: 0.9610, val_auc 0.9758\n",
      "2022-07-29 05:50:19,563 - Eland Training, Epoch [42/50]: loss 2.8775, train_auc: 0.9639, val_auc 0.9763\n",
      "2022-07-29 05:50:44,652 - Eland Training, Epoch [43/50]: loss 2.8687, train_auc: 0.9621, val_auc 0.9759\n",
      "2022-07-29 05:51:08,151 - Eland Training, Epoch [44/50]: loss 2.8626, train_auc: 0.9649, val_auc 0.9749\n",
      "2022-07-29 05:51:30,564 - Eland Training, Epoch [45/50]: loss 2.8694, train_auc: 0.9697, val_auc 0.9732\n",
      "2022-07-29 05:51:52,515 - Eland Training, Epoch [46/50]: loss 2.8643, train_auc: 0.9705, val_auc 0.9756\n",
      "2022-07-29 05:52:14,124 - Eland Training, Epoch [47/50]: loss 2.8553, train_auc: 0.9732, val_auc 0.9777, test_auc 0.9897\n",
      "2022-07-29 05:52:36,946 - Eland Training, Epoch [48/50]: loss 2.8547, train_auc: 0.9723, val_auc 0.9738\n",
      "2022-07-29 05:52:59,313 - Eland Training, Epoch [49/50]: loss 2.8656, train_auc: 0.9717, val_auc 0.9760\n",
      "2022-07-29 05:53:22,979 - Eland Training, Epoch [50/50]: loss 2.8511, train_auc: 0.9690, val_auc 0.9756\n",
      "2022-07-29 05:53:22,980 - Best Test Results: auc 0.9897, ap 0.9902, f1 0.6750\n"
     ]
    }
   ],
   "source": [
    "model_obj = Eland_e2e(\n",
    "    data_dict['graph'], \n",
    "    data_dict['lstm_dataloader'], \n",
    "    data_dict['user_features'],\n",
    "    data_dict['item_features'], \n",
    "    data_dict['labels'], \n",
    "    data_dict['tvt_nids'], \n",
    "    data_dict['u2index'],\n",
    "    data_dict['p2index'], \n",
    "    data_dict['item_features'], \n",
    "    eland_config\n",
    ")\n",
    "training_result,save_model_path = model_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd57209f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'train_auc': 0.9658446093408992, 'val_auc': 0.9755165289256199},\n",
       " 2: {'train_auc': 0.9659537319947622, 'val_auc': 0.9740702479338843},\n",
       " 3: {'train_auc': 0.9692274116106504, 'val_auc': 0.9774793388429752},\n",
       " 4: {'train_auc': 0.9632256656481886, 'val_auc': 0.9735537190082645},\n",
       " 5: {'train_auc': 0.9673723264949804, 'val_auc': 0.9759297520661157},\n",
       " 6: {'train_auc': 0.9714098646879092, 'val_auc': 0.9755165289256199},\n",
       " 7: {'train_auc': 0.9743561763422086, 'val_auc': 0.9755165289256198},\n",
       " 8: {'train_auc': 0.9792666957660412, 'val_auc': 0.9756198347107438},\n",
       " 9: {'train_auc': 0.9668267132256656, 'val_auc': 0.9729338842975207},\n",
       " 10: {'train_auc': 0.9673723264949804, 'val_auc': 0.9747933884297522},\n",
       " 11: {'train_auc': 0.9630074203404626, 'val_auc': 0.9770661157024795},\n",
       " 12: {'train_auc': 0.9620253164556961, 'val_auc': 0.9727272727272727},\n",
       " 13: {'train_auc': 0.9689000436490616, 'val_auc': 0.9753099173553719},\n",
       " 14: {'train_auc': 0.9636621562636404, 'val_auc': 0.975},\n",
       " 15: {'train_auc': 0.9685726756874726, 'val_auc': 0.9754132231404959},\n",
       " 16: {'train_auc': 0.9697730248799651, 'val_auc': 0.9752066115702479},\n",
       " 17: {'train_auc': 0.9707551287647316, 'val_auc': 0.9765495867768595},\n",
       " 18: {'train_auc': 0.9658446093408992, 'val_auc': 0.9755165289256198},\n",
       " 19: {'train_auc': 0.9679179397642951, 'val_auc': 0.974896694214876},\n",
       " 20: {'train_auc': 0.9673723264949804, 'val_auc': 0.9736570247933884},\n",
       " 21: {'train_auc': 0.9740288083806199, 'val_auc': 0.9754132231404958},\n",
       " 22: {'train_auc': 0.9657354866870362, 'val_auc': 0.9737603305785124},\n",
       " 23: {'train_auc': 0.9672632038411174, 'val_auc': 0.9769628099173554},\n",
       " 24: {'train_auc': 0.9689000436490616, 'val_auc': 0.9755165289256199},\n",
       " 25: {'train_auc': 0.9704277608031427, 'val_auc': 0.9744834710743802},\n",
       " 26: {'train_auc': 0.9686817983413357, 'val_auc': 0.9757231404958677},\n",
       " 27: {'train_auc': 0.9682453077258839, 'val_auc': 0.9746900826446281},\n",
       " 28: {'train_auc': 0.9735923177651682, 'val_auc': 0.9745867768595041},\n",
       " 29: {'train_auc': 0.9649716281099956, 'val_auc': 0.9748966942148759},\n",
       " 30: {'train_auc': 0.9689000436490616, 'val_auc': 0.974896694214876},\n",
       " 31: {'train_auc': 0.9715189873417722, 'val_auc': 0.9743801652892563},\n",
       " 32: {'train_auc': 0.9729375818419903, 'val_auc': 0.9756198347107439},\n",
       " 33: {'train_auc': 0.9723919685726756, 'val_auc': 0.9746900826446282},\n",
       " 34: {'train_auc': 0.9649716281099957, 'val_auc': 0.9762396694214875},\n",
       " 35: {'train_auc': 0.9667175905718027, 'val_auc': 0.9767561983471075},\n",
       " 36: {'train_auc': 0.9691182889567875, 'val_auc': 0.9752066115702479},\n",
       " 37: {'train_auc': 0.973701440419031, 'val_auc': 0.9752066115702479},\n",
       " 38: {'train_auc': 0.9735923177651681, 'val_auc': 0.9759297520661158},\n",
       " 39: {'train_auc': 0.9744652989960717, 'val_auc': 0.9754132231404958},\n",
       " 40: {'train_auc': 0.966281099956351, 'val_auc': 0.9743801652892563},\n",
       " 41: {'train_auc': 0.9610432125709297, 'val_auc': 0.9758264462809918},\n",
       " 42: {'train_auc': 0.9638804015713662, 'val_auc': 0.9763429752066116},\n",
       " 43: {'train_auc': 0.9621344391095591, 'val_auc': 0.9759297520661158},\n",
       " 44: {'train_auc': 0.9648625054561326, 'val_auc': 0.9748966942148759},\n",
       " 45: {'train_auc': 0.9696639022261022, 'val_auc': 0.9732438016528926},\n",
       " 46: {'train_auc': 0.9705368834570057, 'val_auc': 0.9756198347107438},\n",
       " 47: {'train_auc': 0.9731558271497163, 'val_auc': 0.9776859504132231},\n",
       " 48: {'train_auc': 0.9722828459188129, 'val_auc': 0.9737603305785124},\n",
       " 49: {'train_auc': 0.9717372326494981, 'val_auc': 0.9760330578512396},\n",
       " 50: {'train_auc': 0.9690091663029246, 'val_auc': 0.9756198347107439}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a75b3",
   "metadata": {},
   "source": [
    "## read in kedro pipeline training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6025e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/07_model_output/user_behavior/train_result.pkl\",\"rb\") as f:\n",
    "    train_result_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8da38ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'train_auc': 0.9604743083003953, 'val_auc': 1.0},\n",
       " 2: {'train_auc': 0.9727849143610012, 'val_auc': 1.0},\n",
       " 3: {'train_auc': 0.9725790513833992, 'val_auc': 1.0},\n",
       " 4: {'train_auc': 0.9724967061923584, 'val_auc': 1.0},\n",
       " 5: {'train_auc': 0.9749258893280633, 'val_auc': 1.0},\n",
       " 6: {'train_auc': 0.9697381422924901, 'val_auc': 1.0},\n",
       " 7: {'train_auc': 0.9713850461133071, 'val_auc': 1.0},\n",
       " 8: {'train_auc': 0.9783432147562583, 'val_auc': 1.0},\n",
       " 9: {'train_auc': 0.9783843873517788, 'val_auc': 1.0},\n",
       " 10: {'train_auc': 0.9778491436100131, 'val_auc': 1.0},\n",
       " 11: {'train_auc': 0.9665266798418972, 'val_auc': 1.0},\n",
       " 12: {'train_auc': 0.9749258893280633, 'val_auc': 1.0},\n",
       " 13: {'train_auc': 0.9724143610013175, 'val_auc': 1.0},\n",
       " 14: {'train_auc': 0.9739789196310936, 'val_auc': 1.0},\n",
       " 15: {'train_auc': 0.9762434123847168, 'val_auc': 1.0},\n",
       " 16: {'train_auc': 0.9762845849802372, 'val_auc': 1.0},\n",
       " 17: {'train_auc': 0.9747200263504611, 'val_auc': 1.0},\n",
       " 18: {'train_auc': 0.973731884057971, 'val_auc': 1.0},\n",
       " 19: {'train_auc': 0.9752552700922266, 'val_auc': 1.0},\n",
       " 20: {'train_auc': 0.977643280632411, 'val_auc': 1.0},\n",
       " 21: {'train_auc': 0.9754611330698288, 'val_auc': 1.0},\n",
       " 22: {'train_auc': 0.976078722002635, 'val_auc': 1.0},\n",
       " 23: {'train_auc': 0.9756669960474309, 'val_auc': 1.0},\n",
       " 24: {'train_auc': 0.9708086297760211, 'val_auc': 1.0},\n",
       " 25: {'train_auc': 0.9755434782608696, 'val_auc': 1.0},\n",
       " 26: {'train_auc': 0.9665266798418972, 'val_auc': 1.0},\n",
       " 27: {'train_auc': 0.9742671277997366, 'val_auc': 1.0},\n",
       " 28: {'train_auc': 0.9663619894598157, 'val_auc': 1.0},\n",
       " 29: {'train_auc': 0.9681324110671937, 'val_auc': 1.0},\n",
       " 30: {'train_auc': 0.9674324769433466, 'val_auc': 1.0},\n",
       " 31: {'train_auc': 0.969820487483531, 'val_auc': 1.0},\n",
       " 32: {'train_auc': 0.9657444005270092, 'val_auc': 1.0},\n",
       " 33: {'train_auc': 0.9747200263504612, 'val_auc': 1.0},\n",
       " 34: {'train_auc': 0.9704380764163373, 'val_auc': 1.0},\n",
       " 35: {'train_auc': 0.9728260869565217, 'val_auc': 1.0},\n",
       " 36: {'train_auc': 0.9650032938076416, 'val_auc': 1.0},\n",
       " 37: {'train_auc': 0.9693675889328063, 'val_auc': 1.0},\n",
       " 38: {'train_auc': 0.9810194334650856, 'val_auc': 1.0},\n",
       " 39: {'train_auc': 0.9737318840579711, 'val_auc': 1.0},\n",
       " 40: {'train_auc': 0.972167325428195, 'val_auc': 1.0},\n",
       " 41: {'train_auc': 0.966609025032938, 'val_auc': 1.0},\n",
       " 42: {'train_auc': 0.9776021080368905, 'val_auc': 1.0},\n",
       " 43: {'train_auc': 0.979001976284585, 'val_auc': 1.0},\n",
       " 44: {'train_auc': 0.9768610013175231, 'val_auc': 1.0},\n",
       " 45: {'train_auc': 0.9644680500658761, 'val_auc': 1.0},\n",
       " 46: {'train_auc': 0.9700263504611331, 'val_auc': 1.0},\n",
       " 47: {'train_auc': 0.9694499341238473, 'val_auc': 1.0},\n",
       " 48: {'train_auc': 0.975296442687747, 'val_auc': 1.0},\n",
       " 49: {'train_auc': 0.9688323451910409, 'val_auc': 1.0},\n",
       " 50: {'train_auc': 0.9706027667984191, 'val_auc': 1.0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17839de7",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The Pushshift Reddit Dataset.\n",
    "\n",
    "Tong Zhao, Bo Ni, Wenhao Yu, Zhichun Guo, Neil Shah, and Meng Jiang, 2021. Action Sequence Augmentation for Early Graph-based Anomaly Detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c66d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-eland-venv",
   "language": "python",
   "name": "kedro-eland-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

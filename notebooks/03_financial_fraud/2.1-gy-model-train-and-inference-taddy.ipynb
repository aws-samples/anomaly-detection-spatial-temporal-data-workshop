{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cdb6b31",
   "metadata": {},
   "source": [
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b14d7",
   "metadata": {},
   "source": [
    "# Set up Financial Fraud dataloader, model training and inference \n",
    "\n",
    "## Table of Contents\n",
    "1. Load processed graph data (in notebook 1.1) into a data dict for the data loader for model training \n",
    "* The data dictionary is defined in the referenced TADDY modeling framework to easily fetch relevant data during training \n",
    "2. load the model training and data sampling configurations \n",
    "* Eigenvalue decomposition based on the adjacency matrix is used for node sampling. Nodes are sampled across multiple snapshots for the edge of interest based on a defined time window. \n",
    "3. pass the data dict in step (1) to the model \n",
    "4. train the model \n",
    "5. apply model inference on the specific snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46391add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cb1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1fd5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/external-repos/anomaly-detection-spatial-temporal-data-workshop/src/kedro-taddy-venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from anomaly_detection_spatial_temporal_data.model.model_config import TaddyConfig\n",
    "from anomaly_detection_spatial_temporal_data.utils import ensure_directory\n",
    "from anomaly_detection_spatial_temporal_data.model.dynamic_graph import Taddy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be9d74",
   "metadata": {},
   "source": [
    "# Load processed graph data in notebook 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0ed8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/03_primary/financial_fraud/training_data.pkl\", 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4199e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, labels, weights, headtail, train_size, test_size, nb_nodes, nb_edges = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22388dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.array([len(x) for x in headtail])\n",
    "num_snap = test_size + train_size\n",
    "labels = [torch.LongTensor(label) for label in labels]\n",
    "\n",
    "snap_train = list(range(num_snap))[:train_size]\n",
    "snap_test = list(range(num_snap))[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7069a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(nb_nodes))\n",
    "index_id_map = {i:i for i in idx}\n",
    "idx = np.array(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521dcae",
   "metadata": {},
   "source": [
    "# Set model training and data sampling configuration and create data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5e130",
   "metadata": {},
   "source": [
    "### load the model training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8610fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_load_options': {'c': 0.15, 'eps': 0.001, 'random_state': 3, 'batch_size': 256, 'load_all_tag': False, 'neighbor_num': 5, 'window_size': 2, 'compute_s': True, 'eigen_file_name': 'data/05_model_input/eigen.pkl'}, 'model_options': {'neighbor_num': 5, 'window_size': 2, 'batch_size': 256, 'embedding_dim': 32, 'num_hidden_layers': 2, 'num_attention_heads': 2, 'seed': 1, 'print_feq': 10, 'lr': 0.001, 'weight_decay': '5e-4', 'max_epoch': 10, 'spy_tag': True, 'max_hop_dis_index': 100, 'max_inti_pos_index': 100, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.5, 'attention_probs_dropout_prob': 0.3, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'is_decoder': False, 'save_directory': 'data/07_model_output/'}, 'infer_options': {'snap_num': 6}}\n"
     ]
    }
   ],
   "source": [
    "train_config_file = '../../conf/base/parameters/taddy.yml'\n",
    "\n",
    "with open(train_config_file, \"r\") as stream:\n",
    "    try:\n",
    "        train_config=yaml.safe_load(stream)\n",
    "        print(train_config)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9211e2",
   "metadata": {},
   "source": [
    "### load the data sampling parameters\n",
    "   * window size is the number of snapshots looked back during node sampling \n",
    "   * neighbor number is the number of neighbors to sample close to the source and target nodes of the edge of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4ecafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 0.15,\n",
       " 'eps': 0.001,\n",
       " 'random_state': 3,\n",
       " 'batch_size': 256,\n",
       " 'load_all_tag': False,\n",
       " 'neighbor_num': 5,\n",
       " 'window_size': 2,\n",
       " 'compute_s': True,\n",
       " 'eigen_file_name': 'data/05_model_input/eigen.pkl'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_file_name = \"../../data/05_model_input/financial_fraud/eigen_tmp.pkl\"\n",
    "data_loader_config = train_config['data_load_options']\n",
    "\n",
    "data_loader_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99268760",
   "metadata": {},
   "source": [
    "### Define relevant functions to node sampling, the purpose of eah function is explained in brief docstring.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb3367c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix. (0226)\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def adj_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx).dot(r_mat_inv)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation. (0226)\"\"\"\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "    # adj_np = np.array(adj.todense())\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    adj_normalized = sparse_mx_to_torch_sparse_tensor(adj_normalized)\n",
    "    return adj_normalized\n",
    "\n",
    "def get_adjs(rows, cols, weights, nb_nodes, eigen_file_name, data_loader_config):\n",
    "    \"\"\"Generate adjacency matrix and conduct eigenvalue decomposition for node sampling\"\"\"\n",
    "    if not os.path.exists(eigen_file_name):\n",
    "        generate_eigen = True\n",
    "        print('Generating eigen as: ' + eigen_file_name)\n",
    "    else:\n",
    "        generate_eigen = False\n",
    "        print('Loading eigen from: ' + eigen_file_name)\n",
    "        with open(eigen_file_name, 'rb') as f:\n",
    "            eigen_adjs_sparse = pickle.load(f)\n",
    "        eigen_adjs = []\n",
    "        for eigen_adj_sparse in eigen_adjs_sparse:\n",
    "            eigen_adjs.append(np.array(eigen_adj_sparse.todense()))\n",
    "\n",
    "    adjs = []\n",
    "    if generate_eigen:\n",
    "        eigen_adjs = []\n",
    "        eigen_adjs_sparse = []\n",
    "\n",
    "    for i in range(len(rows)):\n",
    "        adj = sp.csr_matrix((weights[i], (rows[i], cols[i])), shape=(nb_nodes, nb_nodes), dtype=np.float32)\n",
    "        adjs.append(preprocess_adj(adj))\n",
    "        if data_loader_config['compute_s']:\n",
    "            if generate_eigen:\n",
    "                eigen_adj = data_loader_config['c'] * inv((sp.eye(adj.shape[0]) - (1 - data_loader_config['c']) * adj_normalize(adj)).toarray())\n",
    "                for p in range(adj.shape[0]):\n",
    "                    eigen_adj[p,p] = 0.\n",
    "                eigen_adj = normalize(eigen_adj)\n",
    "                eigen_adjs.append(eigen_adj)\n",
    "                eigen_adjs_sparse.append(sp.csr_matrix(eigen_adj))\n",
    "\n",
    "        else:\n",
    "            eigen_adjs.append(None)\n",
    "\n",
    "    if generate_eigen:\n",
    "        with open(eigen_file_name, 'wb') as f:\n",
    "            pickle.dump(eigen_adjs_sparse, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return adjs, eigen_adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9305095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eigen from: ../../data/05_model_input/financial_fraud/eigen_tmp.pkl\n"
     ]
    }
   ],
   "source": [
    "ensure_directory(eigen_file_name)\n",
    "edges = [np.vstack((rows[i], cols[i])).T for i in range(num_snap)]\n",
    "adjs, eigen_adjs = get_adjs(rows, cols, weights, nb_nodes, eigen_file_name, data_loader_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611fe1c",
   "metadata": {},
   "source": [
    "### The data dictionary defined in TADDY modeling framework \n",
    "  * X is the node feature matrix (We did not generate node feature for this use case. Hence we are aiming to learn from the graph structural information and its evloving pattern) \n",
    "  * A is the adjacency matrix (a popular way to represent a graph)\n",
    "  * S is the eigen decomposition result of A \n",
    "  * degrees stores all the node degrees\n",
    "  * other keys are self-explanatory: edges store edge list, y is the edge label, snap_train are snapshots for training and snap_test are snapshots for testing. num_snap is the total number of snapshots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ca03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'X': None, \n",
    "    'A': adjs, \n",
    "    'S': eigen_adjs, \n",
    "    'index_id_map': index_id_map, \n",
    "    'edges': edges,\n",
    "    'y': labels, \n",
    "    'idx': idx, \n",
    "    'snap_train': snap_train, \n",
    "    'degrees': degrees,\n",
    "    'snap_test': snap_test, \n",
    "    'num_snap': num_snap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8307e331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_load_options': {'c': 0.15,\n",
       "  'eps': 0.001,\n",
       "  'random_state': 3,\n",
       "  'batch_size': 256,\n",
       "  'load_all_tag': False,\n",
       "  'neighbor_num': 5,\n",
       "  'window_size': 2,\n",
       "  'compute_s': True,\n",
       "  'eigen_file_name': 'data/05_model_input/eigen.pkl'},\n",
       " 'model_options': {'neighbor_num': 5,\n",
       "  'window_size': 2,\n",
       "  'batch_size': 256,\n",
       "  'embedding_dim': 32,\n",
       "  'num_hidden_layers': 2,\n",
       "  'num_attention_heads': 2,\n",
       "  'seed': 1,\n",
       "  'print_feq': 10,\n",
       "  'lr': 0.001,\n",
       "  'weight_decay': '5e-4',\n",
       "  'max_epoch': 10,\n",
       "  'spy_tag': True,\n",
       "  'max_hop_dis_index': 100,\n",
       "  'max_inti_pos_index': 100,\n",
       "  'hidden_act': 'gelu',\n",
       "  'hidden_dropout_prob': 0.5,\n",
       "  'attention_probs_dropout_prob': 0.3,\n",
       "  'initializer_range': 0.02,\n",
       "  'layer_norm_eps': 1e-12,\n",
       "  'is_decoder': False,\n",
       "  'save_directory': 'data/07_model_output/'},\n",
       " 'infer_options': {'snap_num': 6}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70dc58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change save path for notebook\n",
    "train_config['model_options']['save_directory'] = '../../data/07_model_output/financial_fraud' \n",
    "\n",
    "if not os.path.exists(train_config['model_options']['save_directory']):\n",
    "    os.makedirs(train_config['model_options']['save_directory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46ccf2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/07_model_output/financial_fraud'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = TaddyConfig(config=train_config['model_options'])\n",
    "model_obj = Taddy(data_dict, model_config)\n",
    "\n",
    "model_config.save_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ed56f",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9407fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss:0.6880, Time: 25.7884s\n",
      "Epoch: 2, loss:0.6785, Time: 26.1162s\n",
      "Epoch: 3, loss:0.6700, Time: 26.8577s\n",
      "Epoch: 4, loss:0.6623, Time: 27.2026s\n",
      "Epoch: 5, loss:0.6460, Time: 25.9393s\n",
      "Epoch: 6, loss:0.6000, Time: 27.8746s\n",
      "Epoch: 7, loss:0.5732, Time: 26.4647s\n",
      "Epoch: 8, loss:0.5414, Time: 25.7860s\n",
      "Epoch: 9, loss:0.4494, Time: 27.2630s\n",
      "Epoch: 10, loss:0.4204, Time: 25.6965s\n",
      "Snap: 05 | AUC: 0.6745\n",
      "Snap: 06 | AUC: 0.6489\n",
      "Snap: 07 | AUC: 0.6310\n",
      "Snap: 08 | AUC: 0.6447\n",
      "Snap: 09 | AUC: 0.8219\n",
      "TOTAL AUC:0.6386\n"
     ]
    }
   ],
   "source": [
    "learned_result,save_model_path = model_obj.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447a782",
   "metadata": {},
   "source": [
    "# Model training result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb2dbc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'train_loss': 0.6880353391170502},\n",
       " 2: {'train_loss': 0.678493544459343},\n",
       " 3: {'train_loss': 0.6700391620397568},\n",
       " 4: {'train_loss': 0.6623023152351379},\n",
       " 5: {'train_loss': 0.6460123509168625},\n",
       " 6: {'train_loss': 0.6000367254018784},\n",
       " 7: {'train_loss': 0.5731981694698334},\n",
       " 8: {'train_loss': 0.5413680970668793},\n",
       " 9: {'train_loss': 0.4493589624762535},\n",
       " 10: {'train_loss': 0.42043692618608475, 'test_auc': 0.6385655578120397}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e83a417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/07_model_output/financial_fraud/taddy_model_9.pth'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f929fb",
   "metadata": {},
   "source": [
    "# Run inference on the specific snapshot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2be81",
   "metadata": {},
   "source": [
    "### load trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "108d82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39ac6107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce2ab140",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_num = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe55f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Embeddings created!\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(snap_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aa4ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "171018b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "060d52c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8218944980147476"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "auc = metrics.roc_auc_score(labels[snap_num],pred)\n",
    "\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647e5d5",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Edgar Alonso Lopez-Rojas and Stefan Axelsson. 2014. BANKSIM: A BANK PAYMENTS SIMULATOR FOR FRAUD DETECTION RESEARCH.\n",
    "\n",
    "Yixin Liu, Shirui Pan, Yu Guang Wang, Fei Xiong, Liang Wang, Qingfeng Chen, and Vincent CS Lee. 2015. Anomaly Detection in Dynamic Graphs via Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688a17b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro-taddy-venv",
   "language": "python",
   "name": "kedro-taddy-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
